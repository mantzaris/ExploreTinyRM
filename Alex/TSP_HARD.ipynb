{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If your repo uses src/ layout (as in your notebook)\n",
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "from exploretinyrm.trm import TRM, TRMConfig, RMSNorm\n",
    "from tsp_data_gen import write_dataset, pairwise_euclidean, tour_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 123) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed maximum number of cities for padding (TRM seq_len must be constant)\n",
    "MAX_CITIES = 12\n",
    "\n",
    "# Dataset path\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "DATA_PATH = str(DATA_DIR / \"tsp_moves_mixed.jsonl\")\n",
    "\n",
    "# If you already generated a JSONL, set this False\n",
    "GENERATE_DATASET_IF_MISSING = True\n",
    "\n",
    "# Dataset generation settings (match your generator)\n",
    "NUM_PROBLEMS = 2500\n",
    "N_MIN = 6\n",
    "N_MAX = MAX_CITIES\n",
    "SEED_DATA = 1234\n",
    "\n",
    "P_CONSTRUCTIVE = 0.7\n",
    "INSERT_STEPS_PER_PROBLEM = 6\n",
    "TWO_OPT_STEPS_PER_PROBLEM = 6\n",
    "STEP_SAMPLING = \"mid\"          # \"uniform\" | \"early\" | \"mid\" | \"late\" | \"all\"\n",
    "NOISE_MIN = 2\n",
    "NOISE_MAX = 4\n",
    "INCLUDE_STOP_PROB = 0.20\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128 if device.type == \"cuda\" else 64\n",
    "NUM_WORKERS = 0\n",
    "USE_AMP = (device.type == \"cuda\")\n",
    "\n",
    "# TRM backbone size\n",
    "D_MODEL = 192\n",
    "N_HEADS = 6                    # must divide D_MODEL\n",
    "N_LAYERS = 2\n",
    "\n",
    "# TRM recursion settings\n",
    "INNER_Z_UPDATES = 6            # cfg.n\n",
    "DEEP_RECURSIONS = 3            # cfg.T\n",
    "K_LAST_OPS = None              # truncate last ops inside final recursion (optional)\n",
    "\n",
    "# Outer iterative refinement steps (the \"N_SUP\" idea)\n",
    "OUTER_STEPS = 6                # number of supervised outer steps per example\n",
    "\n",
    "# Optim\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.0\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "\n",
    "# Epochs\n",
    "EPOCHS_INSERT = 15\n",
    "EPOCHS_TWO_OPT = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing dataset: data/tsp_moves_mixed.jsonl\n"
     ]
    }
   ],
   "source": [
    "if (not os.path.exists(DATA_PATH)) and GENERATE_DATASET_IF_MISSING:\n",
    "    print(\"Generating dataset:\", DATA_PATH)\n",
    "    write_dataset(\n",
    "        out_path=DATA_PATH,\n",
    "        num_problems=NUM_PROBLEMS,\n",
    "        n_min=N_MIN,\n",
    "        n_max=N_MAX,\n",
    "        seed=SEED_DATA,\n",
    "        p_constructive=P_CONSTRUCTIVE,\n",
    "        ins_per=INSERT_STEPS_PER_PROBLEM,\n",
    "        opt_per=TWO_OPT_STEPS_PER_PROBLEM,\n",
    "        step_sample=STEP_SAMPLING,\n",
    "        noise_min=NOISE_MIN,\n",
    "        noise_max=NOISE_MAX,\n",
    "        include_stop_prob=INCLUDE_STOP_PROB,\n",
    "    )\n",
    "    print(\"Wrote:\", DATA_PATH)\n",
    "else:\n",
    "    print(\"Using existing dataset:\", DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded items:\n",
      "  insert: 3020\n",
      "  two_opt: 3209\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl_split_with_costs(path: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    insert_items: List[Dict[str, Any]] = []\n",
    "    two_opt_items: List[Dict[str, Any]] = []\n",
    "\n",
    "    with open(path, \"r\") as file_handle:\n",
    "        for line in file_handle:\n",
    "            example = json.loads(line)\n",
    "\n",
    "            coords = np.array(example[\"coords\"], dtype=np.float32)\n",
    "            n_nodes = int(coords.shape[0])\n",
    "\n",
    "            if example[\"mode\"] == \"insert\":\n",
    "                tour_partial = np.array(example[\"tour_partial\"], dtype=np.int64)\n",
    "                m = int(tour_partial.shape[0])\n",
    "\n",
    "                # JSON stores \"position\" in 1..m (insert at that position), we convert to edge index 0..m-1\n",
    "                pos_raw = int(example[\"action\"][\"position\"])\n",
    "                target_edge_index = (pos_raw - 1) % m\n",
    "\n",
    "                insert_items.append({\n",
    "                    \"coords\": coords,                          # [n,2]\n",
    "                    \"n_nodes\": n_nodes,\n",
    "                    \"tour_partial\": tour_partial,              # [m]\n",
    "                    \"partial_length\": m,\n",
    "                    \"target_city\": int(example[\"action\"][\"city\"]),\n",
    "                    \"target_edge_index\": int(target_edge_index),\n",
    "                    \"cost_before\": float(example.get(\"cost_before\", np.nan)),\n",
    "                    \"cost_after\": float(example.get(\"cost_after\", np.nan)),\n",
    "                })\n",
    "\n",
    "            elif example[\"mode\"] == \"two_opt\":\n",
    "                tour_full = np.array(example[\"tour_full\"], dtype=np.int64)\n",
    "                action = example[\"action\"]\n",
    "                stop = bool(action.get(\"stop\", False))\n",
    "                target_i = -1 if stop else int(action[\"i\"])\n",
    "                target_j = -1 if stop else int(action[\"j\"])\n",
    "\n",
    "                two_opt_items.append({\n",
    "                    \"coords\": coords,                         # [n,2]\n",
    "                    \"n_nodes\": n_nodes,\n",
    "                    \"tour_full\": tour_full,                   # [n]\n",
    "                    \"stop\": stop,\n",
    "                    \"target_i\": target_i,\n",
    "                    \"target_j\": target_j,\n",
    "                    \"cost_before\": float(example.get(\"cost_before\", np.nan)),\n",
    "                    \"cost_after\": float(example.get(\"cost_after\", np.nan)),\n",
    "                })\n",
    "\n",
    "    return insert_items, two_opt_items\n",
    "\n",
    "\n",
    "insert_items, two_opt_items = load_jsonl_split_with_costs(DATA_PATH)\n",
    "print(\"Loaded items:\")\n",
    "print(\"  insert:\", len(insert_items))\n",
    "print(\"  two_opt:\", len(two_opt_items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-opt loaders rebuilt with batch size: 32\n",
      "Loader sizes:\n",
      "  insert train: 22 val: 3\n",
      "  two-opt train: 91 val: 11\n"
     ]
    }
   ],
   "source": [
    "def split_list(items: List[Any], val_fraction: float = 0.1, seed: int = 0) -> Tuple[List[Any], List[Any]]:\n",
    "    rng = np.random.RandomState(seed)\n",
    "    indices = np.arange(len(items))\n",
    "    rng.shuffle(indices)\n",
    "    cut = int((1.0 - val_fraction) * len(items))\n",
    "    train_indices = indices[:cut].tolist()\n",
    "    val_indices = indices[cut:].tolist()\n",
    "    train_items = [items[i] for i in train_indices]\n",
    "    val_items = [items[i] for i in val_indices]\n",
    "    return train_items, val_items\n",
    "\n",
    "\n",
    "class InsertDataset(Dataset):\n",
    "    def __init__(self, items: List[Dict[str, Any]]):\n",
    "        self.items = items\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
    "        return self.items[index]\n",
    "\n",
    "\n",
    "class TwoOptDataset(Dataset):\n",
    "    def __init__(self, items: List[Dict[str, Any]]):\n",
    "        self.items = items\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
    "        return self.items[index]\n",
    "\n",
    "\n",
    "def collate_insert_fixed(batch: List[Dict[str, Any]], max_cities: int) -> Dict[str, torch.Tensor]:\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    coords = torch.zeros(batch_size, max_cities, 2, dtype=torch.float32)\n",
    "    node_valid_mask = torch.zeros(batch_size, max_cities, dtype=torch.bool)\n",
    "\n",
    "    visited_mask = torch.zeros(batch_size, max_cities, dtype=torch.float32)\n",
    "    tour_partial = torch.full((batch_size, max_cities), -1, dtype=torch.long)\n",
    "    partial_length = torch.zeros(batch_size, dtype=torch.long)\n",
    "\n",
    "    target_city = torch.zeros(batch_size, dtype=torch.long)\n",
    "    target_edge_index = torch.zeros(batch_size, dtype=torch.long)\n",
    "\n",
    "    cost_before = torch.full((batch_size,), float(\"nan\"), dtype=torch.float32)\n",
    "    cost_after = torch.full((batch_size,), float(\"nan\"), dtype=torch.float32)\n",
    "\n",
    "    for batch_index, item in enumerate(batch):\n",
    "        n_nodes = int(item[\"n_nodes\"])\n",
    "        m = int(item[\"partial_length\"])\n",
    "        assert n_nodes <= max_cities, \"Increase MAX_CITIES or restrict data.\"\n",
    "\n",
    "        coords[batch_index, :n_nodes] = torch.from_numpy(item[\"coords\"])\n",
    "        node_valid_mask[batch_index, :n_nodes] = True\n",
    "\n",
    "        tour_partial[batch_index, :m] = torch.from_numpy(item[\"tour_partial\"])\n",
    "        partial_length[batch_index] = m\n",
    "\n",
    "        # visited from partial tour\n",
    "        for v in item[\"tour_partial\"]:\n",
    "            visited_mask[batch_index, int(v)] = 1.0\n",
    "\n",
    "        target_city[batch_index] = int(item[\"target_city\"])\n",
    "        target_edge_index[batch_index] = int(item[\"target_edge_index\"])\n",
    "\n",
    "        cost_before[batch_index] = float(item[\"cost_before\"])\n",
    "        cost_after[batch_index] = float(item[\"cost_after\"])\n",
    "\n",
    "    return {\n",
    "        \"coords\": coords,\n",
    "        \"node_valid_mask\": node_valid_mask,\n",
    "        \"visited_mask\": visited_mask,\n",
    "        \"tour_partial\": tour_partial,\n",
    "        \"partial_length\": partial_length,\n",
    "        \"target_city\": target_city,\n",
    "        \"target_edge_index\": target_edge_index,\n",
    "        \"cost_before\": cost_before,\n",
    "        \"cost_after\": cost_after,\n",
    "    }\n",
    "\n",
    "\n",
    "def enumerate_two_opt_pairs(tour: List[int]) -> List[Tuple[int, int]]:\n",
    "    n = len(tour)\n",
    "    pairs: List[Tuple[int, int]] = []\n",
    "    for i in range(1, n - 2):\n",
    "        a, b = tour[i - 1], tour[i]\n",
    "        for j in range(i + 1, n - 1):\n",
    "            c, d = tour[j], tour[(j + 1) % n]\n",
    "            if b == c or a == d:\n",
    "                continue\n",
    "            pairs.append((i, j))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def collate_two_opt_fixed(batch: List[Dict[str, Any]], max_cities: int) -> Dict[str, Any]:\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    coords = torch.zeros(batch_size, max_cities, 2, dtype=torch.float32)\n",
    "    node_valid_mask = torch.zeros(batch_size, max_cities, dtype=torch.bool)\n",
    "    tour_full = torch.full((batch_size, max_cities), -1, dtype=torch.long)\n",
    "\n",
    "    n_nodes_tensor = torch.zeros(batch_size, dtype=torch.long)\n",
    "\n",
    "    candidate_lists: List[List[Tuple[int, int]]] = []\n",
    "    target_pair_index_list: List[int] = []\n",
    "\n",
    "    cost_before = torch.full((batch_size,), float(\"nan\"), dtype=torch.float32)\n",
    "    cost_after = torch.full((batch_size,), float(\"nan\"), dtype=torch.float32)\n",
    "\n",
    "    for batch_index, item in enumerate(batch):\n",
    "        n_nodes = int(item[\"n_nodes\"])\n",
    "        assert n_nodes <= max_cities, \"Increase MAX_CITIES or restrict data.\"\n",
    "\n",
    "        coords[batch_index, :n_nodes] = torch.from_numpy(item[\"coords\"])\n",
    "        node_valid_mask[batch_index, :n_nodes] = True\n",
    "        n_nodes_tensor[batch_index] = n_nodes\n",
    "\n",
    "        tour = item[\"tour_full\"].tolist()\n",
    "        tour_full[batch_index, :n_nodes] = torch.from_numpy(item[\"tour_full\"])\n",
    "\n",
    "        pairs = enumerate_two_opt_pairs(tour)\n",
    "        candidate_lists.append(pairs)\n",
    "\n",
    "        if item[\"stop\"]:\n",
    "            # STOP is at index len(pairs)\n",
    "            target_pair_index_list.append(len(pairs))\n",
    "        else:\n",
    "            pair_to_index = {p: idx for idx, p in enumerate(pairs)}\n",
    "            target_pair_index_list.append(pair_to_index[(int(item[\"target_i\"]), int(item[\"target_j\"]))])\n",
    "\n",
    "        cost_before[batch_index] = float(item[\"cost_before\"])\n",
    "        cost_after[batch_index] = float(item[\"cost_after\"])\n",
    "\n",
    "    # Pad candidate pairs to the maximum in the batch (+1 STOP)\n",
    "    max_pairs_plus_stop = max(len(pairs) + 1 for pairs in candidate_lists)\n",
    "    pair_indices = torch.full((batch_size, max_pairs_plus_stop, 2), -1, dtype=torch.long)\n",
    "    pair_mask = torch.zeros(batch_size, max_pairs_plus_stop, dtype=torch.bool)\n",
    "\n",
    "    for batch_index, pairs in enumerate(candidate_lists):\n",
    "        k = len(pairs)\n",
    "        if k > 0:\n",
    "            pair_indices[batch_index, :k] = torch.tensor(pairs, dtype=torch.long)\n",
    "        pair_mask[batch_index, :k + 1] = True  # pairs + STOP slot\n",
    "\n",
    "    target_pair_index = torch.tensor(target_pair_index_list, dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"coords\": coords,\n",
    "        \"node_valid_mask\": node_valid_mask,\n",
    "        \"tour_full\": tour_full,\n",
    "        \"n_nodes\": n_nodes_tensor,\n",
    "        \"pair_indices\": pair_indices,\n",
    "        \"pair_mask\": pair_mask,\n",
    "        \"target_pair_index\": target_pair_index,\n",
    "        \"cost_before\": cost_before,\n",
    "        \"cost_after\": cost_after,\n",
    "    }\n",
    "\n",
    "\n",
    "# Build loaders\n",
    "insert_train, insert_val = split_list(insert_items, val_fraction=0.1, seed=0)\n",
    "two_opt_train, two_opt_val = split_list(two_opt_items, val_fraction=0.1, seed=1)\n",
    "\n",
    "insert_train_loader = DataLoader(\n",
    "    InsertDataset(insert_train),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    "    collate_fn=lambda batch: collate_insert_fixed(batch, MAX_CITIES),\n",
    ")\n",
    "\n",
    "insert_val_loader = DataLoader(\n",
    "    InsertDataset(insert_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    "    collate_fn=lambda batch: collate_insert_fixed(batch, MAX_CITIES),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE_TWO_OPT = 32\n",
    "\n",
    "two_opt_train_loader = DataLoader(\n",
    "    TwoOptDataset(two_opt_train),\n",
    "    batch_size=BATCH_SIZE_TWO_OPT,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    "    collate_fn=lambda batch: collate_two_opt_fixed(batch, MAX_CITIES),\n",
    ")\n",
    "\n",
    "two_opt_val_loader = DataLoader(\n",
    "    TwoOptDataset(two_opt_val),\n",
    "    batch_size=BATCH_SIZE_TWO_OPT,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    "    collate_fn=lambda batch: collate_two_opt_fixed(batch, MAX_CITIES),\n",
    ")\n",
    "\n",
    "print(\"Two-opt loaders rebuilt with batch size:\", BATCH_SIZE_TWO_OPT)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loader sizes:\")\n",
    "print(\"  insert train:\", len(insert_train_loader), \"val:\", len(insert_val_loader))\n",
    "print(\"  two-opt train:\", len(two_opt_train_loader), \"val:\", len(two_opt_val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert batch keys: ['coords', 'node_valid_mask', 'visited_mask', 'tour_partial', 'partial_length', 'target_city', 'target_edge_index', 'cost_before', 'cost_after']\n",
      "  coords (128, 12, 2) torch.float32\n",
      "  node_valid_mask (128, 12) torch.bool\n",
      "  visited_mask (128, 12) torch.float32\n",
      "  tour_partial (128, 12) torch.int64\n",
      "  partial_length (128,) torch.int64\n",
      "  target_city (128,) torch.int64\n",
      "  target_edge_index (128,) torch.int64\n",
      "  cost_before (128,) torch.float32\n",
      "  cost_after (128,) torch.float32\n",
      "\n",
      "Two-opt batch keys: ['coords', 'node_valid_mask', 'tour_full', 'n_nodes', 'pair_indices', 'pair_mask', 'target_pair_index', 'cost_before', 'cost_after']\n",
      "  coords (32, 12, 2) torch.float32\n",
      "  node_valid_mask (32, 12) torch.bool\n",
      "  tour_full (32, 12) torch.int64\n",
      "  n_nodes (32,) torch.int64\n",
      "  pair_indices (32, 46, 2) torch.int64\n",
      "  pair_mask (32, 46) torch.bool\n",
      "  target_pair_index (32,) torch.int64\n",
      "  cost_before (32,) torch.float32\n",
      "  cost_after (32,) torch.float32\n",
      "\n",
      "Sanity checks passed.\n"
     ]
    }
   ],
   "source": [
    "insert_batch = next(iter(insert_train_loader))\n",
    "two_opt_batch = next(iter(two_opt_train_loader))\n",
    "\n",
    "print(\"Insert batch keys:\", list(insert_batch.keys()))\n",
    "for key, value in insert_batch.items():\n",
    "    if torch.is_tensor(value):\n",
    "        print(\" \", key, tuple(value.shape), value.dtype)\n",
    "\n",
    "print(\"\\nTwo-opt batch keys:\", list(two_opt_batch.keys()))\n",
    "for key, value in two_opt_batch.items():\n",
    "    if torch.is_tensor(value):\n",
    "        print(\" \", key, tuple(value.shape), value.dtype)\n",
    "\n",
    "# Assertions\n",
    "assert insert_batch[\"coords\"].shape[1] == MAX_CITIES\n",
    "assert two_opt_batch[\"coords\"].shape[1] == MAX_CITIES\n",
    "assert (insert_batch[\"target_city\"] >= 0).all()\n",
    "assert (insert_batch[\"target_city\"] < MAX_CITIES).all()\n",
    "print(\"\\nSanity checks passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch.amp import autocast as amp_autocast, GradScaler as AmpGradScaler\n",
    "    AMP_STYLE_NEW = True\n",
    "except ImportError:\n",
    "    from torch.cuda.amp import autocast as amp_autocast, GradScaler as AmpGradScaler\n",
    "    AMP_STYLE_NEW = False\n",
    "\n",
    "\n",
    "def make_grad_scaler(is_cuda: bool) -> AmpGradScaler:\n",
    "    if AMP_STYLE_NEW:\n",
    "        try:\n",
    "            return AmpGradScaler(\"cuda\", enabled=is_cuda)\n",
    "        except TypeError:\n",
    "            return AmpGradScaler(enabled=is_cuda)\n",
    "    return AmpGradScaler(enabled=is_cuda)\n",
    "\n",
    "\n",
    "def autocast_context(is_cuda: bool, use_amp: bool):\n",
    "    if AMP_STYLE_NEW:\n",
    "        try:\n",
    "            return amp_autocast(device_type=\"cuda\", enabled=(is_cuda and use_amp))\n",
    "        except TypeError:\n",
    "            return amp_autocast(enabled=(is_cuda and use_amp))\n",
    "    return amp_autocast(enabled=(is_cuda and use_amp))\n",
    "\n",
    "\n",
    "grad_scaler = make_grad_scaler(device.type == \"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_coords(coords: torch.Tensor, node_valid_mask: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    coords: [batch_size, max_cities, 2]\n",
    "    node_valid_mask: [batch_size, max_cities] bool\n",
    "    Returns normalized coords (translation + scale normalized) with pads zeroed.\n",
    "    \"\"\"\n",
    "    mask = node_valid_mask.float().unsqueeze(-1)  # [B, N, 1]\n",
    "    count = mask.sum(dim=1, keepdim=True).clamp(min=1.0)\n",
    "\n",
    "    mean = (coords * mask).sum(dim=1, keepdim=True) / count\n",
    "    centered = (coords - mean) * mask\n",
    "\n",
    "    var = (centered * centered).sum(dim=1, keepdim=True) / count\n",
    "    std = torch.sqrt(var + eps)\n",
    "    normalized = centered / std\n",
    "    return normalized * mask\n",
    "\n",
    "\n",
    "def apply_insert_action(tour_partial: List[int], city: int, edge_index: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    edge_index in 0..m-1 refers to edge (tour[edge_index] -> tour[(edge_index+1)%m]).\n",
    "    Insert city between them, which corresponds to list insertion at position edge_index+1.\n",
    "    \"\"\"\n",
    "    m = len(tour_partial)\n",
    "    insert_position = (edge_index + 1) % m\n",
    "    return tour_partial[:insert_position] + [city] + tour_partial[insert_position:]\n",
    "\n",
    "\n",
    "def apply_two_opt_action(tour: List[int], i: int, j: int) -> List[int]:\n",
    "    return tour[:i] + list(reversed(tour[i:j+1])) + tour[j+1:]\n",
    "\n",
    "\n",
    "def tour_cost(coords: np.ndarray, tour: List[int]) -> float:\n",
    "    dist = pairwise_euclidean(coords)\n",
    "    return float(tour_length(dist, tour))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Builds x_h for TRM from:\n",
    "      - normalized coordinates (continuous)\n",
    "      - visited flag (0/1)\n",
    "      - tour position per city (0..MAX_CITIES-1) plus special indices\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_cities: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_cities = max_cities\n",
    "\n",
    "        self.coord_proj = nn.Linear(2, d_model, bias=False)\n",
    "\n",
    "        self.visited_embed = nn.Embedding(2, d_model)\n",
    "        # position indices:\n",
    "        #   0..max_cities-1 actual positions\n",
    "        #   max_cities = NOT_IN_TOUR\n",
    "        #   max_cities+1 = PAD\n",
    "        self.NOT_IN_TOUR_INDEX = max_cities\n",
    "        self.PAD_INDEX = max_cities + 1\n",
    "        self.position_embed = nn.Embedding(max_cities + 2, d_model)\n",
    "\n",
    "        self.norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        coords: torch.Tensor,               # [B, N, 2]\n",
    "        node_valid_mask: torch.Tensor,      # [B, N] bool\n",
    "        visited_mask: torch.Tensor,         # [B, N] float 0/1\n",
    "        city_position_index: torch.Tensor,  # [B, N] long in {0..max-1, NOT_IN_TOUR, PAD}\n",
    "    ) -> torch.Tensor:\n",
    "        coords_norm = normalize_coords(coords, node_valid_mask)        # [B,N,2]\n",
    "        coord_features = self.coord_proj(coords_norm)                  # [B,N,D]\n",
    "\n",
    "        visited_index = visited_mask.long().clamp(min=0, max=1)\n",
    "        visited_features = self.visited_embed(visited_index)           # [B,N,D]\n",
    "\n",
    "        position_features = self.position_embed(city_position_index)   # [B,N,D]\n",
    "\n",
    "        x_h = coord_features + visited_features + position_features\n",
    "        x_h = self.norm(x_h)\n",
    "        return x_h\n",
    "\n",
    "\n",
    "class TRMBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Owns a TRM instance but we do not use its token embedding or output heads.\n",
    "    We call its latent_recursion in a TRM-style deep recursion loop.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_cities: int,\n",
    "        d_model: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        inner_z_updates: int,\n",
    "        deep_recursions: int,\n",
    "        k_last_ops: int | None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        cfg = TRMConfig(\n",
    "            input_vocab_size=1,\n",
    "            output_vocab_size=1,\n",
    "            seq_len=max_cities,\n",
    "            d_model=d_model,\n",
    "            n_layers=n_layers,\n",
    "            use_attention=True,\n",
    "            n_heads=n_heads,\n",
    "            dropout=0.0,\n",
    "            mlp_ratio=4.0,\n",
    "            token_mlp_ratio=2.0,\n",
    "            n=inner_z_updates,\n",
    "            T=deep_recursions,\n",
    "            k_last_ops=k_last_ops,\n",
    "            stabilize_input_sums=True,\n",
    "            use_pointer_output=False,\n",
    "            use_order_assignment=False,\n",
    "        )\n",
    "        self.trm = TRM(cfg)\n",
    "\n",
    "    def init_state(self, batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.trm.init_state(batch_size=batch_size, device=device)\n",
    "\n",
    "    def one_outer_step(\n",
    "        self,\n",
    "        x_h: torch.Tensor,\n",
    "        y_state: torch.Tensor,\n",
    "        z_state: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Do:\n",
    "          (T-1) deep recursion passes without grad\n",
    "          1 deep recursion pass with grad\n",
    "        Return:\n",
    "          y_grad, z_grad (with grad),\n",
    "          y_next, z_next (detached for next outer step)\n",
    "        \"\"\"\n",
    "        n = self.trm.cfg.n\n",
    "        T = self.trm.cfg.T\n",
    "        k_last_ops = self.trm.cfg.k_last_ops\n",
    "\n",
    "        # improve state cheaply\n",
    "        for _ in range(max(0, T - 1)):\n",
    "            with torch.no_grad():\n",
    "                y_state, z_state = self.trm.latent_recursion(\n",
    "                    x_h=x_h, y=y_state, z=z_state,\n",
    "                    n=n, k_last_ops=None, track_grads=False\n",
    "                )\n",
    "\n",
    "        # final recursion with grad\n",
    "        y_grad, z_grad = self.trm.latent_recursion(\n",
    "            x_h=x_h, y=y_state, z=z_state,\n",
    "            n=n, k_last_ops=k_last_ops, track_grads=True\n",
    "        )\n",
    "\n",
    "        y_next = y_grad.detach()\n",
    "        z_next = z_grad.detach()\n",
    "        return y_grad, z_grad, y_next, z_next\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Autocast-off context manager\n",
    "try:\n",
    "    from torch.amp import autocast as torch_autocast\n",
    "    def autocast_disabled(device: torch.device):\n",
    "        if device.type == \"cuda\":\n",
    "            return torch_autocast(device_type=\"cuda\", enabled=False)\n",
    "        return nullcontext()\n",
    "except Exception:\n",
    "    from torch.cuda.amp import autocast as cuda_autocast\n",
    "    def autocast_disabled(device: torch.device):\n",
    "        if device.type == \"cuda\":\n",
    "            return cuda_autocast(enabled=False)\n",
    "        return nullcontext()\n",
    "\n",
    "\n",
    "class InsertPolicyV4(nn.Module):\n",
    "    \"\"\"\n",
    "    Fixes:\n",
    "      - normalize y before edge features (prevents huge edge logits and million-scale CE)\n",
    "      - allow outer_grad_steps: only last K outer steps build graphs\n",
    "    \"\"\"\n",
    "    def __init__(self, max_cities: int, d_model: int, backbone: TRMBackbone):\n",
    "        super().__init__()\n",
    "        self.max_cities = max_cities\n",
    "        self.d_model = d_model\n",
    "        self.backbone = backbone\n",
    "\n",
    "        self.encoder = NodeEncoder(d_model=d_model, max_cities=max_cities)\n",
    "\n",
    "        # One shared node norm for all heads (stable scale)\n",
    "        self.node_norm = RMSNorm(d_model)\n",
    "\n",
    "        # City head\n",
    "        self.city_score = nn.Linear(d_model, 1, bias=False)\n",
    "\n",
    "        # Edge head\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(4 * d_model, d_model, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def build_city_positions_from_partial_tour(\n",
    "        self,\n",
    "        tour_partial: torch.Tensor,      # [B, N] with -1 pad\n",
    "        partial_length: torch.Tensor,    # [B]\n",
    "        node_valid_mask: torch.Tensor,   # [B, N]\n",
    "    ) -> torch.Tensor:\n",
    "        B, N = tour_partial.shape\n",
    "        not_in_tour = self.encoder.NOT_IN_TOUR_INDEX\n",
    "        pad_index = self.encoder.PAD_INDEX\n",
    "        city_position = torch.full((B, N), not_in_tour, dtype=torch.long, device=tour_partial.device)\n",
    "\n",
    "        for b in range(B):\n",
    "            m = int(partial_length[b].item())\n",
    "            for pos in range(m):\n",
    "                city = int(tour_partial[b, pos].item())\n",
    "                city_position[b, city] = pos\n",
    "\n",
    "        city_position = city_position.masked_fill(~node_valid_mask, pad_index)\n",
    "        return city_position\n",
    "\n",
    "    def build_edge_endpoints(\n",
    "        self,\n",
    "        tour_partial: torch.Tensor,     # [B, N]\n",
    "        partial_length: torch.Tensor,   # [B]\n",
    "    ):\n",
    "        B, N = tour_partial.shape\n",
    "        edge_u = torch.zeros(B, N, dtype=torch.long, device=tour_partial.device)\n",
    "        edge_v = torch.zeros(B, N, dtype=torch.long, device=tour_partial.device)\n",
    "        edge_valid_mask = torch.zeros(B, N, dtype=torch.bool, device=tour_partial.device)\n",
    "\n",
    "        for b in range(B):\n",
    "            m = int(partial_length[b].item())\n",
    "            if m <= 0:\n",
    "                continue\n",
    "            for k in range(m):\n",
    "                u = int(tour_partial[b, k].item())\n",
    "                v = int(tour_partial[b, (k + 1) % m].item())\n",
    "                edge_u[b, k] = u\n",
    "                edge_v[b, k] = v\n",
    "                edge_valid_mask[b, k] = True\n",
    "\n",
    "        return edge_u, edge_v, edge_valid_mask\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _outer_step_no_grad(self, x_h, y_state, z_state):\n",
    "        n = self.backbone.trm.cfg.n\n",
    "        T = self.backbone.trm.cfg.T\n",
    "        for _ in range(T):\n",
    "            y_state, z_state = self.backbone.trm.latent_recursion(\n",
    "                x_h=x_h, y=y_state, z=z_state,\n",
    "                n=n, k_last_ops=None, track_grads=False\n",
    "            )\n",
    "        return y_state, z_state\n",
    "\n",
    "    def forward_with_losses(\n",
    "        self,\n",
    "        batch,\n",
    "        outer_steps: int,\n",
    "        step_weighting: str = \"linear\",\n",
    "        outer_grad_steps: int | None = None,\n",
    "        loss_city_weight: float = 1.0,\n",
    "        loss_edge_weight: float = 1.0,\n",
    "    ):\n",
    "        coords = batch[\"coords\"].to(device)\n",
    "        node_valid_mask = batch[\"node_valid_mask\"].to(device)\n",
    "        visited_mask = batch[\"visited_mask\"].to(device)\n",
    "        tour_partial = batch[\"tour_partial\"].to(device)\n",
    "        partial_length = batch[\"partial_length\"].to(device)\n",
    "\n",
    "        target_city = batch[\"target_city\"].to(device)\n",
    "        target_edge_index = batch[\"target_edge_index\"].to(device)\n",
    "\n",
    "        city_position = self.build_city_positions_from_partial_tour(tour_partial, partial_length, node_valid_mask)\n",
    "        x_h = self.encoder(coords, node_valid_mask, visited_mask, city_position)\n",
    "\n",
    "        B = coords.shape[0]\n",
    "        y_state, z_state = self.backbone.init_state(batch_size=B, device=coords.device)\n",
    "\n",
    "        available_city_mask = (node_valid_mask & (visited_mask < 0.5))\n",
    "        edge_u, edge_v, edge_valid_mask = self.build_edge_endpoints(tour_partial, partial_length)\n",
    "\n",
    "        if outer_grad_steps is None:\n",
    "            outer_grad_steps = outer_steps\n",
    "        outer_grad_steps = int(max(1, min(outer_steps, outer_grad_steps)))\n",
    "        pre_steps = outer_steps - outer_grad_steps\n",
    "\n",
    "        # Pre-refine state without grad\n",
    "        for _ in range(pre_steps):\n",
    "            y_state, z_state = self._outer_step_no_grad(x_h, y_state, z_state)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        loss_city_sum = 0.0\n",
    "        loss_edge_sum = 0.0\n",
    "\n",
    "        for local_step in range(outer_grad_steps):\n",
    "            global_step = pre_steps + local_step\n",
    "\n",
    "            y_grad, z_grad, y_state, z_state = self.backbone.one_outer_step(x_h, y_state, z_state)\n",
    "\n",
    "            # Head logits in fp32 + normalized y\n",
    "            with autocast_disabled(y_grad.device):\n",
    "                y32 = y_grad.float()\n",
    "                yN = self.node_norm(y32)  # crucial: stabilize magnitude\n",
    "\n",
    "                big_neg = -1e4\n",
    "\n",
    "                city_logits = self.city_score(yN).squeeze(-1)           # [B,N]\n",
    "                city_logits = city_logits.masked_fill(~available_city_mask, big_neg)\n",
    "\n",
    "                D = yN.shape[-1]\n",
    "                u = edge_u.clamp(min=0)\n",
    "                v = edge_v.clamp(min=0)\n",
    "                y_u = yN.gather(1, u.unsqueeze(-1).expand(-1, -1, D))\n",
    "                y_v = yN.gather(1, v.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "                edge_features = torch.cat([y_u, y_v, (y_u - y_v), (y_u * y_v)], dim=-1)  # [B,N,4D]\n",
    "                edge_logits = self.edge_mlp(edge_features).squeeze(-1)                   # [B,N]\n",
    "                edge_logits = edge_logits.masked_fill(~edge_valid_mask, big_neg)\n",
    "\n",
    "                loss_city = F.cross_entropy(city_logits, target_city)\n",
    "                loss_edge = F.cross_entropy(edge_logits, target_edge_index)\n",
    "\n",
    "            if step_weighting == \"linear\":\n",
    "                weight = float(global_step + 1) / float(outer_steps)\n",
    "            elif step_weighting == \"last_only\":\n",
    "                weight = 1.0 if (global_step == outer_steps - 1) else 0.0\n",
    "            else:\n",
    "                weight = 1.0\n",
    "\n",
    "            total_loss = total_loss + weight * (loss_city_weight * loss_city + loss_edge_weight * loss_edge)\n",
    "            loss_city_sum += float(loss_city.detach().item())\n",
    "            loss_edge_sum += float(loss_edge.detach().item())\n",
    "\n",
    "        metrics = {\n",
    "            \"loss_city\": loss_city_sum / max(1, outer_grad_steps),\n",
    "            \"loss_edge\": loss_edge_sum / max(1, outer_grad_steps),\n",
    "        }\n",
    "        return total_loss, metrics\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, batch, outer_steps: int):\n",
    "        self.eval()\n",
    "        coords = batch[\"coords\"].to(device)\n",
    "        node_valid_mask = batch[\"node_valid_mask\"].to(device)\n",
    "        visited_mask = batch[\"visited_mask\"].to(device)\n",
    "        tour_partial = batch[\"tour_partial\"].to(device)\n",
    "        partial_length = batch[\"partial_length\"].to(device)\n",
    "\n",
    "        city_position = self.build_city_positions_from_partial_tour(tour_partial, partial_length, node_valid_mask)\n",
    "        x_h = self.encoder(coords, node_valid_mask, visited_mask, city_position)\n",
    "\n",
    "        B = coords.shape[0]\n",
    "        y_state, z_state = self.backbone.init_state(batch_size=B, device=coords.device)\n",
    "\n",
    "        available_city_mask = (node_valid_mask & (visited_mask < 0.5))\n",
    "        edge_u, edge_v, edge_valid_mask = self.build_edge_endpoints(tour_partial, partial_length)\n",
    "\n",
    "        last_city_logits = None\n",
    "        last_edge_logits = None\n",
    "\n",
    "        for _ in range(outer_steps):\n",
    "            y_state, z_state = self._outer_step_no_grad(x_h, y_state, z_state)\n",
    "\n",
    "            with autocast_disabled(y_state.device):\n",
    "                y32 = y_state.float()\n",
    "                yN = self.node_norm(y32)\n",
    "                big_neg = -1e4\n",
    "\n",
    "                city_logits = self.city_score(yN).squeeze(-1)\n",
    "                city_logits = city_logits.masked_fill(~available_city_mask, big_neg)\n",
    "\n",
    "                D = yN.shape[-1]\n",
    "                u = edge_u.clamp(min=0)\n",
    "                v = edge_v.clamp(min=0)\n",
    "                y_u = yN.gather(1, u.unsqueeze(-1).expand(-1, -1, D))\n",
    "                y_v = yN.gather(1, v.unsqueeze(-1).expand(-1, -1, D))\n",
    "                edge_features = torch.cat([y_u, y_v, (y_u - y_v), (y_u * y_v)], dim=-1)\n",
    "                edge_logits = self.edge_mlp(edge_features).squeeze(-1)\n",
    "                edge_logits = edge_logits.masked_fill(~edge_valid_mask, big_neg)\n",
    "\n",
    "            last_city_logits = city_logits\n",
    "            last_edge_logits = edge_logits\n",
    "\n",
    "        return last_city_logits.argmax(dim=1), last_edge_logits.argmax(dim=1)\n",
    "\n",
    "\n",
    "class TwoOptPolicyV4(nn.Module):\n",
    "    \"\"\"\n",
    "    Fixes:\n",
    "      - normalize y before pair features (prevents huge pair logits and 700+ CE)\n",
    "      - outer_grad_steps to avoid OOM (train only last K outer steps)\n",
    "    \"\"\"\n",
    "    def __init__(self, max_cities: int, d_model: int, backbone: TRMBackbone):\n",
    "        super().__init__()\n",
    "        self.max_cities = max_cities\n",
    "        self.d_model = d_model\n",
    "        self.backbone = backbone\n",
    "\n",
    "        self.encoder = NodeEncoder(d_model=d_model, max_cities=max_cities)\n",
    "\n",
    "        self.node_norm = RMSNorm(d_model)\n",
    "\n",
    "        self.stop_norm = RMSNorm(d_model)\n",
    "        self.stop_score = nn.Linear(d_model, 1, bias=False)\n",
    "\n",
    "        self.pair_mlp = nn.Sequential(\n",
    "            nn.Linear(4 * d_model, 2 * d_model, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2 * d_model, 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def build_city_positions_from_full_tour(self, tour_full: torch.Tensor, node_valid_mask: torch.Tensor) -> torch.Tensor:\n",
    "        B, N = tour_full.shape\n",
    "        not_in_tour = self.encoder.NOT_IN_TOUR_INDEX\n",
    "        pad_index = self.encoder.PAD_INDEX\n",
    "        city_position = torch.full((B, N), not_in_tour, dtype=torch.long, device=tour_full.device)\n",
    "\n",
    "        for b in range(B):\n",
    "            n_nodes = int(node_valid_mask[b].sum().item())\n",
    "            for pos in range(n_nodes):\n",
    "                city = int(tour_full[b, pos].item())\n",
    "                city_position[b, city] = pos\n",
    "\n",
    "        city_position = city_position.masked_fill(~node_valid_mask, pad_index)\n",
    "        return city_position\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _outer_step_no_grad(self, x_h, y_state, z_state):\n",
    "        n = self.backbone.trm.cfg.n\n",
    "        T = self.backbone.trm.cfg.T\n",
    "        for _ in range(T):\n",
    "            y_state, z_state = self.backbone.trm.latent_recursion(\n",
    "                x_h=x_h, y=y_state, z=z_state,\n",
    "                n=n, k_last_ops=None, track_grads=False\n",
    "            )\n",
    "        return y_state, z_state\n",
    "\n",
    "    def forward_with_losses(\n",
    "        self,\n",
    "        batch,\n",
    "        outer_steps: int,\n",
    "        step_weighting: str = \"linear\",\n",
    "        outer_grad_steps: int | None = None,\n",
    "    ):\n",
    "        coords = batch[\"coords\"].to(device)\n",
    "        node_valid_mask = batch[\"node_valid_mask\"].to(device)\n",
    "        tour_full = batch[\"tour_full\"].to(device)\n",
    "\n",
    "        pair_indices = batch[\"pair_indices\"].to(device)   # [B, L, 2]\n",
    "        pair_mask = batch[\"pair_mask\"].to(device)         # [B, L]\n",
    "        target_pair_index = batch[\"target_pair_index\"].to(device)\n",
    "\n",
    "        visited_mask = node_valid_mask.float()\n",
    "        city_position = self.build_city_positions_from_full_tour(tour_full, node_valid_mask)\n",
    "        x_h = self.encoder(coords, node_valid_mask, visited_mask, city_position)\n",
    "\n",
    "        B = coords.shape[0]\n",
    "        y_state, z_state = self.backbone.init_state(batch_size=B, device=coords.device)\n",
    "\n",
    "        if outer_grad_steps is None:\n",
    "            outer_grad_steps = outer_steps\n",
    "        outer_grad_steps = int(max(1, min(outer_steps, outer_grad_steps)))\n",
    "        pre_steps = outer_steps - outer_grad_steps\n",
    "\n",
    "        for _ in range(pre_steps):\n",
    "            y_state, z_state = self._outer_step_no_grad(x_h, y_state, z_state)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        loss_sum = 0.0\n",
    "\n",
    "        for local_step in range(outer_grad_steps):\n",
    "            global_step = pre_steps + local_step\n",
    "            y_grad, z_grad, y_state, z_state = self.backbone.one_outer_step(x_h, y_state, z_state)\n",
    "\n",
    "            with autocast_disabled(y_grad.device):\n",
    "                y32 = y_grad.float()\n",
    "                yN = self.node_norm(y32)\n",
    "                big_neg = -1e4\n",
    "\n",
    "                # STOP logit\n",
    "                mask_float = node_valid_mask.float().unsqueeze(-1)\n",
    "                denom = mask_float.sum(dim=1).clamp(min=1.0)\n",
    "                pooled = (yN * mask_float).sum(dim=1) / denom\n",
    "                stop_logit = self.stop_score(self.stop_norm(pooled)).squeeze(-1)  # [B] fp32\n",
    "\n",
    "                B_local, num_slots, _ = pair_indices.shape\n",
    "                logits = torch.full((B_local, num_slots), big_neg, device=yN.device, dtype=torch.float32)\n",
    "\n",
    "                pair_i = pair_indices[:, :, 0]\n",
    "                pair_j = pair_indices[:, :, 1]\n",
    "                valid_pair_slot = (pair_i >= 0) & (pair_j >= 0)\n",
    "\n",
    "                pos_a = (pair_i - 1).clamp(min=0)\n",
    "                pos_b = pair_i.clamp(min=0)\n",
    "                pos_c = pair_j.clamp(min=0)\n",
    "                pos_d = (pair_j + 1).clamp(min=0)\n",
    "\n",
    "                city_a = tour_full.gather(1, pos_a)\n",
    "                city_b = tour_full.gather(1, pos_b)\n",
    "                city_c = tour_full.gather(1, pos_c)\n",
    "                city_d = tour_full.gather(1, pos_d)\n",
    "\n",
    "                D = yN.shape[-1]\n",
    "                def gather_city(city_index: torch.Tensor) -> torch.Tensor:\n",
    "                    idx = city_index.clamp(min=0)\n",
    "                    return yN.gather(1, idx.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "                y_a = gather_city(city_a)\n",
    "                y_b = gather_city(city_b)\n",
    "                y_c = gather_city(city_c)\n",
    "                y_d = gather_city(city_d)\n",
    "\n",
    "                pair_features = torch.cat([y_a, y_b, y_c, y_d], dim=-1)  # [B,L,4D]\n",
    "                pair_logits = self.pair_mlp(pair_features).squeeze(-1)   # [B,L] fp32\n",
    "\n",
    "                logits = torch.where(valid_pair_slot, pair_logits, logits)\n",
    "\n",
    "                # STOP slot index: K = sum(mask)-1\n",
    "                k_pairs = (pair_mask.sum(dim=1) - 1).clamp(min=0)\n",
    "                logits.scatter_(1, k_pairs.unsqueeze(1), stop_logit.unsqueeze(1))\n",
    "\n",
    "                logits = logits.masked_fill(~pair_mask, big_neg)\n",
    "\n",
    "                loss_step = F.cross_entropy(logits, target_pair_index)\n",
    "\n",
    "            if step_weighting == \"linear\":\n",
    "                weight = float(global_step + 1) / float(outer_steps)\n",
    "            elif step_weighting == \"last_only\":\n",
    "                weight = 1.0 if (global_step == outer_steps - 1) else 0.0\n",
    "            else:\n",
    "                weight = 1.0\n",
    "\n",
    "            total_loss = total_loss + weight * loss_step\n",
    "            loss_sum += float(loss_step.detach().item())\n",
    "\n",
    "        return total_loss, {\"loss_pair\": loss_sum / max(1, outer_grad_steps)}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, batch, outer_steps: int):\n",
    "        self.eval()\n",
    "        coords = batch[\"coords\"].to(device)\n",
    "        node_valid_mask = batch[\"node_valid_mask\"].to(device)\n",
    "        tour_full = batch[\"tour_full\"].to(device)\n",
    "\n",
    "        pair_indices = batch[\"pair_indices\"].to(device)\n",
    "        pair_mask = batch[\"pair_mask\"].to(device)\n",
    "\n",
    "        visited_mask = node_valid_mask.float()\n",
    "        city_position = self.build_city_positions_from_full_tour(tour_full, node_valid_mask)\n",
    "        x_h = self.encoder(coords, node_valid_mask, visited_mask, city_position)\n",
    "\n",
    "        B = coords.shape[0]\n",
    "        y_state, z_state = self.backbone.init_state(batch_size=B, device=coords.device)\n",
    "\n",
    "        last_logits = None\n",
    "\n",
    "        for _ in range(outer_steps):\n",
    "            y_state, z_state = self._outer_step_no_grad(x_h, y_state, z_state)\n",
    "\n",
    "            with autocast_disabled(y_state.device):\n",
    "                y32 = y_state.float()\n",
    "                yN = self.node_norm(y32)\n",
    "                big_neg = -1e4\n",
    "\n",
    "                mask_float = node_valid_mask.float().unsqueeze(-1)\n",
    "                denom = mask_float.sum(dim=1).clamp(min=1.0)\n",
    "                pooled = (yN * mask_float).sum(dim=1) / denom\n",
    "                stop_logit = self.stop_score(self.stop_norm(pooled)).squeeze(-1)\n",
    "\n",
    "                B_local, num_slots, _ = pair_indices.shape\n",
    "                logits = torch.full((B_local, num_slots), big_neg, device=yN.device, dtype=torch.float32)\n",
    "\n",
    "                pair_i = pair_indices[:, :, 0]\n",
    "                pair_j = pair_indices[:, :, 1]\n",
    "                valid_pair_slot = (pair_i >= 0) & (pair_j >= 0)\n",
    "\n",
    "                pos_a = (pair_i - 1).clamp(min=0)\n",
    "                pos_b = pair_i.clamp(min=0)\n",
    "                pos_c = pair_j.clamp(min=0)\n",
    "                pos_d = (pair_j + 1).clamp(min=0)\n",
    "\n",
    "                city_a = tour_full.gather(1, pos_a)\n",
    "                city_b = tour_full.gather(1, pos_b)\n",
    "                city_c = tour_full.gather(1, pos_c)\n",
    "                city_d = tour_full.gather(1, pos_d)\n",
    "\n",
    "                D = yN.shape[-1]\n",
    "                def gather_city(city_index: torch.Tensor) -> torch.Tensor:\n",
    "                    idx = city_index.clamp(min=0)\n",
    "                    return yN.gather(1, idx.unsqueeze(-1).expand(-1, -1, D))\n",
    "\n",
    "                y_a = gather_city(city_a)\n",
    "                y_b = gather_city(city_b)\n",
    "                y_c = gather_city(city_c)\n",
    "                y_d = gather_city(city_d)\n",
    "\n",
    "                pair_features = torch.cat([y_a, y_b, y_c, y_d], dim=-1)\n",
    "                pair_logits = self.pair_mlp(pair_features).squeeze(-1)\n",
    "                logits = torch.where(valid_pair_slot, pair_logits, logits)\n",
    "\n",
    "                k_pairs = (pair_mask.sum(dim=1) - 1).clamp(min=0)\n",
    "                logits.scatter_(1, k_pairs.unsqueeze(1), stop_logit.unsqueeze(1))\n",
    "                logits = logits.masked_fill(~pair_mask, big_neg)\n",
    "\n",
    "            last_logits = logits\n",
    "\n",
    "        return last_logits.argmax(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InsertPolicyV4 in globals: True\n",
      "TwoOptPolicyV4 in globals: True\n",
      "Aliased InsertPolicy -> InsertPolicyV4 and TwoOptPolicy -> TwoOptPolicyV4\n"
     ]
    }
   ],
   "source": [
    "# Make sure the V4 patched classes are in scope\n",
    "print(\"InsertPolicyV4 in globals:\", \"InsertPolicyV4\" in globals())\n",
    "print(\"TwoOptPolicyV4 in globals:\", \"TwoOptPolicyV4\" in globals())\n",
    "\n",
    "assert \"InsertPolicyV4\" in globals(), \"You need to run the patch cell that defines InsertPolicyV4 first.\"\n",
    "assert \"TwoOptPolicyV4\" in globals(), \"You need to run the patch cell that defines TwoOptPolicyV4 first.\"\n",
    "\n",
    "# Aliases so your existing build code can keep using InsertPolicy / TwoOptPolicy\n",
    "InsertPolicy = InsertPolicyV4\n",
    "TwoOptPolicy = TwoOptPolicyV4\n",
    "\n",
    "print(\"Aliased InsertPolicy -> InsertPolicyV4 and TwoOptPolicy -> TwoOptPolicyV4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert policy params (M): 1.333056\n",
      "Two-opt policy params (M): 1.480896\n",
      "OUTER_GRAD_STEPS_INSERT = 2 OUTER_GRAD_STEPS_TWO_OPT = 1\n"
     ]
    }
   ],
   "source": [
    "# Strongly recommended: free anything old before rebuilding\n",
    "for name in [\"insert_policy\", \"two_opt_policy\", \"backbone_insert\", \"backbone_two_opt\",\n",
    "             \"optimizer_insert\", \"optimizer_two_opt\"]:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Outer refinement still runs OUTER_STEPS times,\n",
    "# but we only backprop through the last K steps to save memory.\n",
    "OUTER_GRAD_STEPS_INSERT = 2\n",
    "OUTER_GRAD_STEPS_TWO_OPT = 1\n",
    "\n",
    "backbone_insert = TRMBackbone(\n",
    "    max_cities=MAX_CITIES,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    inner_z_updates=INNER_Z_UPDATES,\n",
    "    deep_recursions=DEEP_RECURSIONS,\n",
    "    k_last_ops=K_LAST_OPS,\n",
    ").to(device)\n",
    "\n",
    "insert_policy = InsertPolicy(max_cities=MAX_CITIES, d_model=D_MODEL, backbone=backbone_insert).to(device)\n",
    "\n",
    "optimizer_insert = torch.optim.AdamW(\n",
    "    insert_policy.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "backbone_two_opt = TRMBackbone(\n",
    "    max_cities=MAX_CITIES,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    inner_z_updates=INNER_Z_UPDATES,\n",
    "    deep_recursions=DEEP_RECURSIONS,\n",
    "    k_last_ops=K_LAST_OPS,\n",
    ").to(device)\n",
    "\n",
    "two_opt_policy = TwoOptPolicy(max_cities=MAX_CITIES, d_model=D_MODEL, backbone=backbone_two_opt).to(device)\n",
    "\n",
    "optimizer_two_opt = torch.optim.AdamW(\n",
    "    two_opt_policy.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "print(\"Insert policy params (M):\", sum(p.numel() for p in insert_policy.parameters()) / 1e6)\n",
    "print(\"Two-opt policy params (M):\", sum(p.numel() for p in two_opt_policy.parameters()) / 1e6)\n",
    "print(\"OUTER_GRAD_STEPS_INSERT =\", OUTER_GRAD_STEPS_INSERT, \"OUTER_GRAD_STEPS_TWO_OPT =\", OUTER_GRAD_STEPS_TWO_OPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert policy params (M): 1.333056\n",
      "Two-opt policy params (M): 1.480896\n"
     ]
    }
   ],
   "source": [
    "backbone_insert = TRMBackbone(\n",
    "    max_cities=MAX_CITIES,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    inner_z_updates=INNER_Z_UPDATES,\n",
    "    deep_recursions=DEEP_RECURSIONS,\n",
    "    k_last_ops=K_LAST_OPS,\n",
    ").to(device)\n",
    "\n",
    "insert_policy = InsertPolicy(max_cities=MAX_CITIES, d_model=D_MODEL, backbone=backbone_insert).to(device)\n",
    "\n",
    "optimizer_insert = torch.optim.AdamW(\n",
    "    insert_policy.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "backbone_two_opt = TRMBackbone(\n",
    "    max_cities=MAX_CITIES,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    inner_z_updates=INNER_Z_UPDATES,\n",
    "    deep_recursions=DEEP_RECURSIONS,\n",
    "    k_last_ops=K_LAST_OPS,\n",
    ").to(device)\n",
    "\n",
    "two_opt_policy = TwoOptPolicy(max_cities=MAX_CITIES, d_model=D_MODEL, backbone=backbone_two_opt).to(device)\n",
    "\n",
    "optimizer_two_opt = torch.optim.AdamW(\n",
    "    two_opt_policy.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "print(\"Insert policy params (M):\", sum(p.numel() for p in insert_policy.parameters()) / 1e6)\n",
    "print(\"Two-opt policy params (M):\", sum(p.numel() for p in two_opt_policy.parameters()) / 1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_insert(policy: InsertPolicy, loader: DataLoader, outer_steps: int) -> Dict[str, float]:\n",
    "    policy.eval()\n",
    "    correct_city = 0\n",
    "    correct_edge = 0\n",
    "    correct_both = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        pred_city, pred_edge = policy.predict(batch, outer_steps=outer_steps)\n",
    "\n",
    "        target_city = batch[\"target_city\"].to(device)\n",
    "        target_edge = batch[\"target_edge_index\"].to(device)\n",
    "\n",
    "        match_city = (pred_city == target_city)\n",
    "        match_edge = (pred_edge == target_edge)\n",
    "        match_both = (match_city & match_edge)\n",
    "\n",
    "        correct_city += int(match_city.sum().item())\n",
    "        correct_edge += int(match_edge.sum().item())\n",
    "        correct_both += int(match_both.sum().item())\n",
    "        total += int(target_city.numel())\n",
    "\n",
    "    return {\n",
    "        \"acc_city\": correct_city / max(1, total),\n",
    "        \"acc_edge\": correct_edge / max(1, total),\n",
    "        \"acc_both\": correct_both / max(1, total),\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_two_opt(policy: TwoOptPolicy, loader: DataLoader, outer_steps: int) -> Dict[str, float]:\n",
    "    policy.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_stop = 0\n",
    "    total_stop = 0\n",
    "    correct_nonstop = 0\n",
    "    total_nonstop = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        pred_index = policy.predict(batch, outer_steps=outer_steps)\n",
    "        target_index = batch[\"target_pair_index\"].to(device)\n",
    "\n",
    "        match = (pred_index == target_index)\n",
    "        correct += int(match.sum().item())\n",
    "        total += int(target_index.numel())\n",
    "\n",
    "        # Determine which targets are STOP by reconstructing K = num_pairs = sum(mask)-1\n",
    "        pair_mask = batch[\"pair_mask\"].to(device)\n",
    "        k_pairs = (pair_mask.sum(dim=1) - 1).clamp(min=0)\n",
    "        is_stop = (target_index == k_pairs)\n",
    "\n",
    "        if is_stop.any():\n",
    "            correct_stop += int((match & is_stop).sum().item())\n",
    "            total_stop += int(is_stop.sum().item())\n",
    "        if (~is_stop).any():\n",
    "            correct_nonstop += int((match & (~is_stop)).sum().item())\n",
    "            total_nonstop += int((~is_stop).sum().item())\n",
    "\n",
    "    return {\n",
    "        \"acc_all\": correct / max(1, total),\n",
    "        \"acc_stop\": correct_stop / max(1, total_stop) if total_stop > 0 else float(\"nan\"),\n",
    "        \"acc_nonstop\": correct_nonstop / max(1, total_nonstop) if total_nonstop > 0 else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch_insert(\n",
    "    policy,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    grad_scaler,\n",
    "    epoch: int,\n",
    "    outer_steps: int,\n",
    "    outer_grad_steps: int,\n",
    "    use_amp: bool,\n",
    "):\n",
    "    policy.train()\n",
    "    is_cuda = (device.type == \"cuda\")\n",
    "\n",
    "    loss_total = 0.0\n",
    "    loss_city_total = 0.0\n",
    "    loss_edge_total = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast_context(is_cuda=is_cuda, use_amp=use_amp):\n",
    "            loss, metrics = policy.forward_with_losses(\n",
    "                batch,\n",
    "                outer_steps=outer_steps,\n",
    "                step_weighting=\"linear\",\n",
    "                outer_grad_steps=outer_grad_steps,\n",
    "                loss_city_weight=1.0,\n",
    "                loss_edge_weight=1.0,\n",
    "            )\n",
    "\n",
    "        if use_amp:\n",
    "            grad_scaler.scale(loss).backward()\n",
    "            grad_scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(policy.parameters(), GRAD_CLIP_NORM)\n",
    "            grad_scaler.step(optimizer)\n",
    "            grad_scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(policy.parameters(), GRAD_CLIP_NORM)\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_total += float(loss.detach().item())\n",
    "        loss_city_total += float(metrics[\"loss_city\"])\n",
    "        loss_edge_total += float(metrics[\"loss_edge\"])\n",
    "        steps += 1\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss_total / max(1, steps),\n",
    "        \"loss_city\": loss_city_total / max(1, steps),\n",
    "        \"loss_edge\": loss_edge_total / max(1, steps),\n",
    "    }\n",
    "\n",
    "\n",
    "def train_one_epoch_two_opt(\n",
    "    policy,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    grad_scaler,\n",
    "    epoch: int,\n",
    "    outer_steps: int,\n",
    "    outer_grad_steps: int,\n",
    "    use_amp: bool,\n",
    "):\n",
    "    policy.train()\n",
    "    is_cuda = (device.type == \"cuda\")\n",
    "\n",
    "    loss_total = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast_context(is_cuda=is_cuda, use_amp=use_amp):\n",
    "            loss, metrics = policy.forward_with_losses(\n",
    "                batch,\n",
    "                outer_steps=outer_steps,\n",
    "                step_weighting=\"linear\",\n",
    "                outer_grad_steps=outer_grad_steps,\n",
    "            )\n",
    "\n",
    "        if use_amp:\n",
    "            grad_scaler.scale(loss).backward()\n",
    "            grad_scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(policy.parameters(), GRAD_CLIP_NORM)\n",
    "            grad_scaler.step(optimizer)\n",
    "            grad_scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(policy.parameters(), GRAD_CLIP_NORM)\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_total += float(loss.detach().item())\n",
    "        steps += 1\n",
    "\n",
    "    return {\"loss\": loss_total / max(1, steps)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert loss finite: True loss: 5.670562744140625\n",
      "metrics: {'loss_city': 1.3932865262031555, 'loss_edge': 1.6997475624084473}\n",
      "Two-opt loss finite: True loss: 3.0527396202087402\n",
      "metrics: {'loss_pair': 3.0527396202087402}\n"
     ]
    }
   ],
   "source": [
    "# Insert finiteness and scale check\n",
    "insert_policy.train()\n",
    "batch = next(iter(insert_train_loader))\n",
    "optimizer_insert.zero_grad(set_to_none=True)\n",
    "\n",
    "with autocast_context(is_cuda=(device.type == \"cuda\"), use_amp=USE_AMP):\n",
    "    loss, metrics = insert_policy.forward_with_losses(\n",
    "        batch,\n",
    "        outer_steps=OUTER_STEPS,\n",
    "        outer_grad_steps=OUTER_GRAD_STEPS_INSERT,\n",
    "        step_weighting=\"linear\",\n",
    "    )\n",
    "\n",
    "print(\"Insert loss finite:\", torch.isfinite(loss).item(), \"loss:\", float(loss.detach().cpu()))\n",
    "print(\"metrics:\", metrics)\n",
    "\n",
    "# Two-opt finiteness and scale check\n",
    "two_opt_policy.train()\n",
    "batch2 = next(iter(two_opt_train_loader))\n",
    "optimizer_two_opt.zero_grad(set_to_none=True)\n",
    "\n",
    "with autocast_context(is_cuda=(device.type == \"cuda\"), use_amp=USE_AMP):\n",
    "    loss2, metrics2 = two_opt_policy.forward_with_losses(\n",
    "        batch2,\n",
    "        outer_steps=OUTER_STEPS,\n",
    "        outer_grad_steps=OUTER_GRAD_STEPS_TWO_OPT,\n",
    "        step_weighting=\"linear\",\n",
    "    )\n",
    "\n",
    "print(\"Two-opt loss finite:\", torch.isfinite(loss2).item(), \"loss:\", float(loss2.detach().cpu()))\n",
    "print(\"metrics:\", metrics2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Insert] Epoch 01 | Loss 5.4686 | CityLoss 1.3942 | EdgeLoss 1.5886 | Val acc_city 0.338 acc_edge 0.387 acc_both 0.119\n",
      "[Insert] Epoch 02 | Loss 5.1340 | CityLoss 1.3920 | EdgeLoss 1.4083 | Val acc_city 0.348 acc_edge 0.427 acc_both 0.156\n",
      "[Insert] Epoch 03 | Loss 4.9459 | CityLoss 1.3871 | EdgeLoss 1.3107 | Val acc_city 0.351 acc_edge 0.437 acc_both 0.166\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS_INSERT + 1):\n",
    "    train_metrics = train_one_epoch_insert(\n",
    "        policy=insert_policy,\n",
    "        loader=insert_train_loader,\n",
    "        optimizer=optimizer_insert,\n",
    "        grad_scaler=grad_scaler,\n",
    "        epoch=epoch,\n",
    "        outer_steps=OUTER_STEPS,\n",
    "        outer_grad_steps=OUTER_GRAD_STEPS_INSERT,\n",
    "        use_amp=USE_AMP,\n",
    "    )\n",
    "    val_metrics = evaluate_insert(insert_policy, insert_val_loader, outer_steps=OUTER_STEPS)\n",
    "\n",
    "    print(\n",
    "        f\"[Insert] Epoch {epoch:02d} | \"\n",
    "        f\"Loss {train_metrics['loss']:.4f} | \"\n",
    "        f\"CityLoss {train_metrics['loss_city']:.4f} | \"\n",
    "        f\"EdgeLoss {train_metrics['loss_edge']:.4f} | \"\n",
    "        f\"Val acc_city {val_metrics['acc_city']:.3f} acc_edge {val_metrics['acc_edge']:.3f} acc_both {val_metrics['acc_both']:.3f}\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS_TWO_OPT + 1):\n",
    "    train_metrics = train_one_epoch_two_opt(\n",
    "        policy=two_opt_policy,\n",
    "        loader=two_opt_train_loader,\n",
    "        optimizer=optimizer_two_opt,\n",
    "        grad_scaler=grad_scaler,\n",
    "        epoch=epoch,\n",
    "        outer_steps=OUTER_STEPS,\n",
    "        outer_grad_steps=OUTER_GRAD_STEPS_TWO_OPT,\n",
    "        use_amp=USE_AMP,\n",
    "    )\n",
    "    val_metrics = evaluate_two_opt(two_opt_policy, two_opt_val_loader, outer_steps=OUTER_STEPS)\n",
    "\n",
    "    print(\n",
    "        f\"[TwoOpt] Epoch {epoch:02d} | \"\n",
    "        f\"Loss {train_metrics['loss']:.4f} | \"\n",
    "        f\"Val acc_all {val_metrics['acc_all']:.3f} acc_stop {val_metrics['acc_stop']:.3f} acc_nonstop {val_metrics['acc_nonstop']:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 2.25 MiB is free. Including non-PyTorch memory, this process has 3.55 GiB memory in use. Of the allocated memory 3.44 GiB is allocated by PyTorch, and 21.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS_TWO_OPT + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     train_metrics = \u001b[43mtrain_one_epoch_two_opt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtwo_opt_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtwo_opt_train_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_two_opt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scaler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mouter_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTER_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_AMP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     val_metrics = evaluate_two_opt(two_opt_policy, two_opt_val_loader, outer_steps=OUTER_STEPS)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     14\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[TwoOpt] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_metrics[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVal acc_all \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[33m'\u001b[39m\u001b[33macc_all\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc_stop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[33m'\u001b[39m\u001b[33macc_stop\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc_nonstop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[33m'\u001b[39m\u001b[33macc_nonstop\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mtrain_one_epoch_two_opt\u001b[39m\u001b[34m(policy, loader, optimizer, grad_scaler, epoch, outer_steps, use_amp)\u001b[39m\n\u001b[32m    130\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast_context(is_cuda=is_cuda, use_amp=use_amp):\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     loss, metrics = \u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_with_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouter_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mouter_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_weighting\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_amp:\n\u001b[32m    136\u001b[39m     grad_scaler.scale(loss).backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 407\u001b[39m, in \u001b[36mTwoOptPolicy.forward_with_losses\u001b[39m\u001b[34m(self, batch, outer_steps, step_weighting)\u001b[39m\n\u001b[32m    404\u001b[39m loss_sum = \u001b[32m0.0\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(outer_steps):\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     y_grad, z_grad, y_state, z_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m.\u001b[49m\u001b[43mone_outer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_disabled(y_grad.device):\n\u001b[32m    410\u001b[39m         y32 = y_grad.float()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mTRMBackbone.one_outer_step\u001b[39m\u001b[34m(self, x_h, y_state, z_state)\u001b[39m\n\u001b[32m    106\u001b[39m         y_state, z_state = \u001b[38;5;28mself\u001b[39m.trm.latent_recursion(\n\u001b[32m    107\u001b[39m             x_h=x_h, y=y_state, z=z_state,\n\u001b[32m    108\u001b[39m             n=n, k_last_ops=\u001b[38;5;28;01mNone\u001b[39;00m, track_grads=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    109\u001b[39m         )\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# final recursion with grad\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m y_grad, z_grad = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlatent_recursion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_grads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    115\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m y_next = y_grad.detach()\n\u001b[32m    118\u001b[39m z_next = z_grad.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:375\u001b[39m, in \u001b[36mTRM.latent_recursion\u001b[39m\u001b[34m(self, x_h, y, z, n, k_last_ops, track_grads)\u001b[39m\n\u001b[32m    373\u001b[39m     ctx = nullcontext() \u001b[38;5;28;01mif\u001b[39;00m (track_grads \u001b[38;5;129;01mand\u001b[39;00m op_index >= cutoff) \u001b[38;5;28;01melse\u001b[39;00m torch.no_grad()\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     op_index += \u001b[32m1\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# once: y = net(y + z)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:347\u001b[39m, in \u001b[36mTRM._net\u001b[39m\u001b[34m(self, h)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_net\u001b[39m(\u001b[38;5;28mself\u001b[39m, h: torch.Tensor) -> torch.Tensor:\n\u001b[32m    346\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"pass through the tiny shared network\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshared_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:200\u001b[39m, in \u001b[36mTinySharedNet.forward\u001b[39m\u001b[34m(self, h)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, h: torch.Tensor) -> torch.Tensor:\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m         h = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:155\u001b[39m, in \u001b[36mTRMBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_attention:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmixer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    157\u001b[39m         \u001b[38;5;66;03m# pre-norm + token MLP + residual\u001b[39;00m\n\u001b[32m    158\u001b[39m         h = \u001b[38;5;28mself\u001b[39m.pre(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:122\u001b[39m, in \u001b[36mSelfAttention.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    120\u001b[39m attn = attn_scores.softmax(dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    121\u001b[39m attn = \u001b[38;5;28mself\u001b[39m.drop(attn)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m context = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m               \u001b[38;5;66;03m# [B, H, L, Hd]\u001b[39;00m\n\u001b[32m    123\u001b[39m context = context.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous().view(b, l, d)  \u001b[38;5;66;03m# [B, L, D]\u001b[39;00m\n\u001b[32m    124\u001b[39m out = \u001b[38;5;28mself\u001b[39m.drop(\u001b[38;5;28mself\u001b[39m.proj(context))\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 2.25 MiB is free. Including non-PyTorch memory, this process has 3.55 GiB memory in use. Of the allocated memory 3.44 GiB is allocated by PyTorch, and 21.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS_TWO_OPT + 1):\n",
    "    train_metrics = train_one_epoch_two_opt(\n",
    "        policy=two_opt_policy,\n",
    "        loader=two_opt_train_loader,\n",
    "        optimizer=optimizer_two_opt,\n",
    "        grad_scaler=grad_scaler,\n",
    "        epoch=epoch,\n",
    "        outer_steps=OUTER_STEPS,\n",
    "        outer_grad_steps=OUTER_GRAD_STEPS_TWO_OPT,\n",
    "        use_amp=USE_AMP,\n",
    "    )\n",
    "    val_metrics = evaluate_two_opt(two_opt_policy, two_opt_val_loader, outer_steps=OUTER_STEPS)\n",
    "\n",
    "    print(\n",
    "        f\"[TwoOpt] Epoch {epoch:02d} | \"\n",
    "        f\"Loss {train_metrics['loss']:.4f} | \"\n",
    "        f\"Val acc_all {val_metrics['acc_all']:.3f} acc_stop {val_metrics['acc_stop']:.3f} acc_nonstop {val_metrics['acc_nonstop']:.3f}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_insert_batch(\n",
    "    coords: np.ndarray,\n",
    "    tour_partial: List[int],\n",
    "    max_cities: int,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    n_nodes = coords.shape[0]\n",
    "    coords_pad = np.zeros((max_cities, 2), dtype=np.float32)\n",
    "    coords_pad[:n_nodes] = coords\n",
    "\n",
    "    node_valid_mask = np.zeros((max_cities,), dtype=np.bool_)\n",
    "    node_valid_mask[:n_nodes] = True\n",
    "\n",
    "    visited_mask = np.zeros((max_cities,), dtype=np.float32)\n",
    "    for v in tour_partial:\n",
    "        visited_mask[int(v)] = 1.0\n",
    "\n",
    "    tour_pad = np.full((max_cities,), -1, dtype=np.int64)\n",
    "    tour_pad[:len(tour_partial)] = np.array(tour_partial, dtype=np.int64)\n",
    "\n",
    "    batch = {\n",
    "        \"coords\": torch.from_numpy(coords_pad).unsqueeze(0),\n",
    "        \"node_valid_mask\": torch.from_numpy(node_valid_mask).unsqueeze(0),\n",
    "        \"visited_mask\": torch.from_numpy(visited_mask).unsqueeze(0),\n",
    "        \"tour_partial\": torch.from_numpy(tour_pad).unsqueeze(0),\n",
    "        \"partial_length\": torch.tensor([len(tour_partial)], dtype=torch.long),\n",
    "        # dummy targets\n",
    "        \"target_city\": torch.tensor([0], dtype=torch.long),\n",
    "        \"target_edge_index\": torch.tensor([0], dtype=torch.long),\n",
    "        \"cost_before\": torch.tensor([float(\"nan\")]),\n",
    "        \"cost_after\": torch.tensor([float(\"nan\")]),\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "\n",
    "def make_single_two_opt_batch(\n",
    "    coords: np.ndarray,\n",
    "    tour_full: List[int],\n",
    "    max_cities: int,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    n_nodes = coords.shape[0]\n",
    "    coords_pad = np.zeros((max_cities, 2), dtype=np.float32)\n",
    "    coords_pad[:n_nodes] = coords\n",
    "\n",
    "    node_valid_mask = np.zeros((max_cities,), dtype=np.bool_)\n",
    "    node_valid_mask[:n_nodes] = True\n",
    "\n",
    "    tour_pad = np.full((max_cities,), -1, dtype=np.int64)\n",
    "    tour_pad[:n_nodes] = np.array(tour_full, dtype=np.int64)\n",
    "\n",
    "    pairs = enumerate_two_opt_pairs(tour_full)\n",
    "    num_slots = len(pairs) + 1  # + STOP\n",
    "    pair_indices = np.full((num_slots, 2), -1, dtype=np.int64)\n",
    "    if len(pairs) > 0:\n",
    "        pair_indices[:len(pairs)] = np.array(pairs, dtype=np.int64)\n",
    "\n",
    "    pair_mask = np.zeros((num_slots,), dtype=np.bool_)\n",
    "    pair_mask[:num_slots] = True\n",
    "\n",
    "    batch = {\n",
    "        \"coords\": torch.from_numpy(coords_pad).unsqueeze(0),\n",
    "        \"node_valid_mask\": torch.from_numpy(node_valid_mask).unsqueeze(0),\n",
    "        \"tour_full\": torch.from_numpy(tour_pad).unsqueeze(0),\n",
    "        \"n_nodes\": torch.tensor([n_nodes], dtype=torch.long),\n",
    "        \"pair_indices\": torch.from_numpy(pair_indices).unsqueeze(0),\n",
    "        \"pair_mask\": torch.from_numpy(pair_mask).unsqueeze(0),\n",
    "        # dummy target\n",
    "        \"target_pair_index\": torch.tensor([0], dtype=torch.long),\n",
    "        \"cost_before\": torch.tensor([float(\"nan\")]),\n",
    "        \"cost_after\": torch.tensor([float(\"nan\")]),\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "\n",
    "def solve_tsp_with_policies(\n",
    "    coords: np.ndarray,\n",
    "    insert_policy: InsertPolicy,\n",
    "    two_opt_policy: TwoOptPolicy,\n",
    "    max_cities: int,\n",
    "    outer_steps: int,\n",
    "    start_size: int = 3,\n",
    "    max_two_opt_steps: int = 50,\n",
    ") -> List[int]:\n",
    "    n_nodes = coords.shape[0]\n",
    "    assert n_nodes <= max_cities\n",
    "\n",
    "    # Start partial tour with first start_size nodes\n",
    "    start_size = min(start_size, n_nodes)\n",
    "    tour_partial = list(range(start_size))\n",
    "\n",
    "    # Insert remaining nodes\n",
    "    while len(tour_partial) < n_nodes:\n",
    "        batch = make_single_insert_batch(coords, tour_partial, max_cities)\n",
    "        pred_city, pred_edge = insert_policy.predict(batch, outer_steps=outer_steps)\n",
    "        city = int(pred_city.item())\n",
    "        edge = int(pred_edge.item())\n",
    "\n",
    "        # If model outputs a visited city (should be rare), pick first unvisited as fallback\n",
    "        if city in set(tour_partial):\n",
    "            for candidate in range(n_nodes):\n",
    "                if candidate not in set(tour_partial):\n",
    "                    city = candidate\n",
    "                    break\n",
    "\n",
    "        tour_partial = apply_insert_action(tour_partial, city=city, edge_index=edge)\n",
    "\n",
    "    tour_full = tour_partial\n",
    "\n",
    "    # Improve with 2-opt until STOP\n",
    "    for _ in range(max_two_opt_steps):\n",
    "        batch = make_single_two_opt_batch(coords, tour_full, max_cities)\n",
    "        pred_index = int(two_opt_policy.predict(batch, outer_steps=outer_steps).item())\n",
    "\n",
    "        pairs = enumerate_two_opt_pairs(tour_full)\n",
    "        stop_index = len(pairs)\n",
    "        if pred_index == stop_index:\n",
    "            break\n",
    "\n",
    "        i, j = pairs[pred_index]\n",
    "        tour_full = apply_two_opt_action(tour_full, i=i, j=j)\n",
    "\n",
    "    return tour_full\n",
    "\n",
    "\n",
    "# Demo: pick one random instance from the two_opt_val split for a full tour baseline\n",
    "random_index = np.random.randint(0, len(two_opt_val))\n",
    "demo_item = two_opt_val[random_index]\n",
    "coords_demo = demo_item[\"coords\"]\n",
    "n_nodes_demo = coords_demo.shape[0]\n",
    "\n",
    "# Create a naive baseline tour (0..n-1)\n",
    "baseline_tour = list(range(n_nodes_demo))\n",
    "\n",
    "pred_tour = solve_tsp_with_policies(\n",
    "    coords=coords_demo,\n",
    "    insert_policy=insert_policy,\n",
    "    two_opt_policy=two_opt_policy,\n",
    "    max_cities=MAX_CITIES,\n",
    "    outer_steps=OUTER_STEPS,\n",
    "    start_size=3,\n",
    "    max_two_opt_steps=50,\n",
    ")\n",
    "\n",
    "print(\"n_nodes:\", n_nodes_demo)\n",
    "print(\"Baseline tour:\", baseline_tour)\n",
    "print(\"Predicted tour:\", pred_tour)\n",
    "\n",
    "baseline_cost = tour_cost(coords_demo, baseline_tour)\n",
    "pred_cost = tour_cost(coords_demo, pred_tour)\n",
    "print(\"Baseline cost:\", baseline_cost)\n",
    "print(\"Predicted cost:\", pred_cost)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(coords_demo[:,0], coords_demo[:,1], s=40)\n",
    "for city_index, (x, y) in enumerate(coords_demo):\n",
    "    plt.text(x, y, str(city_index), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "for k in range(len(pred_tour)):\n",
    "    a = pred_tour[k]\n",
    "    b = pred_tour[(k + 1) % len(pred_tour)]\n",
    "    plt.plot([coords_demo[a,0], coords_demo[b,0]], [coords_demo[a,1], coords_demo[b,1]])\n",
    "\n",
    "plt.title(f\"Predicted tour (cost={pred_cost:.3f})\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity OK: x in [0, INPUT_TOKENS-1], y in [0, n_cities-1]\n",
      "Example tour length: 46.3702322388002\n",
      "Successor mapping: [1, 2, 5, 4, 7, 3, 0, 6]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt; \n",
    "\n",
    "import importlib, tsp_data\n",
    "importlib.reload(tsp_data)\n",
    "\n",
    "from tsp_data import (\n",
    "    TSPConfig, TSPDataset, get_tsp_loaders, \n",
    "    decode_coords, distance_matrix, tour_length_from_successors,\n",
    "    plot_tsp_instance_from_tokens\n",
    ")\n",
    "\n",
    "# Configure a small, exact TSP to start\n",
    "cfg = TSPConfig(\n",
    "    n_cities=8,           # keep small for exact Held-Karp in Python\n",
    "    grid_size=16,         # coordinates on a 16x16 integer grid\n",
    "    solver=\"auto\",        # exact up to max_exact_cities, else 2-opt\n",
    "    max_exact_cities=12,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Build loaders\n",
    "train_loader, val_loader = get_tsp_loaders(\n",
    "    n_train=2048,         # pick what you need\n",
    "    n_val=512,\n",
    "    batch_size=32,\n",
    "    cfg=cfg\n",
    ")\n",
    "\n",
    "# Derive TRM dimensions\n",
    "SEQ_LEN = cfg.n_cities\n",
    "INPUT_TOKENS = cfg.grid_size * cfg.grid_size\n",
    "OUTPUT_TOKENS = cfg.n_cities\n",
    "\n",
    "# Quick sanity like Sudoku\n",
    "bx, by = next(iter(train_loader))\n",
    "assert bx.shape[1] == SEQ_LEN and by.shape[1] == SEQ_LEN\n",
    "assert bx.min().item() >= 0 and bx.max().item() < INPUT_TOKENS\n",
    "assert by.min().item() >= 0 and by.max().item() < OUTPUT_TOKENS\n",
    "print(\"Sanity OK: x in [0, INPUT_TOKENS-1], y in [0, n_cities-1]\")\n",
    "\n",
    "# Inspect one validation example: decode, recompute tour length\n",
    "x_tokens, y_tokens = next(iter(val_loader))\n",
    "coords = decode_coords(x_tokens[0].numpy(), cfg.grid_size)  # [L, 2] integer coordinates\n",
    "dist = distance_matrix(coords)                              # [L, L] float64 distances\n",
    "length = tour_length_from_successors(dist, y_tokens[0].numpy())\n",
    "print(\"Example tour length:\", length)\n",
    "\n",
    "# If you want to visualize the permutation:\n",
    "print(\"Successor mapping:\", y_tokens[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Held-Karp length: 46.3702322388002 Brute-force length: 46.3702322388002\n"
     ]
    }
   ],
   "source": [
    "# Verify the successor mapping really is a single cycle\n",
    "from tsp_data import _is_single_cycle, decode_coords, distance_matrix, tour_length_from_successors\n",
    "x_tokens, y_tokens = next(iter(val_loader))\n",
    "coords = decode_coords(x_tokens[0].numpy(), cfg.grid_size)\n",
    "assert _is_single_cycle(y_tokens[0].numpy())\n",
    "\n",
    "# For very small n (<=10), optionally brute-force to confirm optimality\n",
    "import itertools, numpy as np\n",
    "if cfg.n_cities <= 10:\n",
    "    dist = distance_matrix(coords)\n",
    "    best = np.inf\n",
    "    for perm in itertools.permutations(range(1, cfg.n_cities)):  # start fixed at 0\n",
    "        cyc = (0,) + perm + (0,)\n",
    "        cost = sum(dist[cyc[k], cyc[k+1]] for k in range(cfg.n_cities))\n",
    "        if cost < best: best = cost\n",
    "    hk_len = tour_length_from_successors(dist, y_tokens[0].numpy())\n",
    "    print(\"Held-Karp length:\", hk_len, \"Brute-force length:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploretinyrm: 0.1.0\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import exploretinyrm as m\n",
    "print(\"exploretinyrm:\", m.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, math, random, sys\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Make sure TRM is on path if your repo uses a local src/ layout\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "from exploretinyrm.trm import TRM, TRMConfig\n",
    "\n",
    "# TSP data utilities you created\n",
    "from tsp_data import (\n",
    "    TSPConfig, get_tsp_loaders,\n",
    "    decode_coords, distance_matrix, tour_length_from_successors,\n",
    "    path_from_successors\n",
    ")\n",
    "\n",
    "def set_seed(seed: int = 123):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed precision helpers (forward-compatible)\n",
    "try:\n",
    "    from torch.amp import autocast as _autocast, GradScaler as _GradScaler\n",
    "    _USE_TORCH_AMP = True\n",
    "except ImportError:\n",
    "    from torch.cuda.amp import autocast as _autocast, GradScaler as _GradScaler\n",
    "    _USE_TORCH_AMP = False\n",
    "\n",
    "def make_grad_scaler(is_cuda: bool):\n",
    "    if _USE_TORCH_AMP:\n",
    "        try:\n",
    "            return _GradScaler(\"cuda\", enabled=is_cuda)\n",
    "        except TypeError:\n",
    "            return _GradScaler(enabled=is_cuda)\n",
    "    else:\n",
    "        return _GradScaler(enabled=is_cuda)\n",
    "\n",
    "def amp_autocast(is_cuda: bool, use_amp: bool):\n",
    "    if _USE_TORCH_AMP:\n",
    "        try:\n",
    "            return _autocast(device_type=\"cuda\", enabled=(is_cuda and use_amp))\n",
    "        except TypeError:\n",
    "            return _autocast(enabled=(is_cuda and use_amp))\n",
    "    else:\n",
    "        return _autocast(enabled=(is_cuda and use_amp))\n",
    "\n",
    "# EMA utility\n",
    "class EMA:\n",
    "    def __init__(self, model: torch.nn.Module, decay: float = 0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {\n",
    "            name: param.detach().clone()\n",
    "            for name, param in model.named_parameters()\n",
    "            if param.requires_grad\n",
    "        }\n",
    "\n",
    "    def update(self, model: torch.nn.Module) -> None:\n",
    "        d = self.decay\n",
    "        with torch.no_grad():\n",
    "            for name, p in model.named_parameters():\n",
    "                if not p.requires_grad:\n",
    "                    continue\n",
    "                self.shadow[name].mul_(d).add_(p.detach(), alpha=1.0 - d)\n",
    "\n",
    "    def copy_to(self, model: torch.nn.Module) -> None:\n",
    "        with torch.no_grad():\n",
    "            for name, p in model.named_parameters():\n",
    "                if name in self.shadow:\n",
    "                    p.copy_(self.shadow[name])\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def use_ema_weights(model: torch.nn.Module, ema: EMA):\n",
    "    backup = {\n",
    "        name: p.detach().clone()\n",
    "        for name, p in model.named_parameters()\n",
    "        if p.requires_grad\n",
    "    }\n",
    "    ema.copy_to(model)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        with torch.no_grad():\n",
    "            for name, p in model.named_parameters():\n",
    "                if name in backup:\n",
    "                    p.copy_(backup[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params (M): 0.575489\n"
     ]
    }
   ],
   "source": [
    "# TRM hyperparameters sized for a 4 GB GPU\n",
    "D_MODEL = 128\n",
    "N_SUP   = 8      # number of supervision recursions per batch step\n",
    "N       = 4      # number of z-updates per recursion\n",
    "T       = 3      # number of deep recursions inside each supervision pass\n",
    "K_LAST_OPS = None   # backprop only through the last K net calls inside final recursion\n",
    "\n",
    "USE_ATT = True   # attention is helpful for TSP\n",
    "\n",
    "cfg_trm = TRMConfig(\n",
    "    input_vocab_size=INPUT_TOKENS,\n",
    "    output_vocab_size=OUTPUT_TOKENS,\n",
    "    seq_len=SEQ_LEN,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=2,\n",
    "    use_attention=USE_ATT,\n",
    "    n_heads=4,               # 128 % 4 == 0\n",
    "    dropout=0.0,\n",
    "    mlp_ratio=4.0,\n",
    "    token_mlp_ratio=2.0,\n",
    "    n=N,\n",
    "    T=T,\n",
    "    k_last_ops=K_LAST_OPS,\n",
    "    stabilize_input_sums=True,\n",
    "    use_order_assignment=True\n",
    ")\n",
    "\n",
    "model = TRM(cfg_trm).to(device)\n",
    "print(\"Params (M):\", sum(p.numel() for p in model.parameters())/1e6)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.0, betas=(0.9, 0.95))\n",
    "scaler = make_grad_scaler(device.type == \"cuda\")\n",
    "ema = EMA(model, decay=0.999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Path-based evaluation with coverage and cycle validity (5 metrics) ---\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def is_single_cycle(successor: np.ndarray) -> bool:\n",
    "    n = len(successor)\n",
    "    seen = np.zeros(n, dtype=bool)\n",
    "    cur = 0\n",
    "    for _ in range(n):\n",
    "        if seen[cur]:\n",
    "            return False\n",
    "        seen[cur] = True\n",
    "        cur = int(successor[cur])\n",
    "    return cur == 0 and seen.all()\n",
    "\n",
    "def tour_length_from_path(dist: np.ndarray, path: list[int]) -> float:\n",
    "    return float(sum(dist[path[i], path[i+1]] for i in range(len(path) - 1)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity OK: x in [0, INPUT_TOKENS-1], y in [0, n_cities-1], seq_len = 8\n"
     ]
    }
   ],
   "source": [
    "# Small, exact labels for a stable proof of concept\n",
    "cfg_data = TSPConfig(\n",
    "    n_cities=8,          # sequence length L\n",
    "    grid_size=16,        # input vocab = 256\n",
    "    solver=\"auto\",       # exact up to max_exact_cities\n",
    "    max_exact_cities=12,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "train_loader, val_loader = get_tsp_loaders(\n",
    "    n_train=2048,\n",
    "    n_val=512,\n",
    "    batch_size=32,       # bump up or down based on memory\n",
    "    cfg=cfg_data\n",
    ")\n",
    "\n",
    "# Derive TRM dimensions\n",
    "SEQ_LEN = cfg_data.n_cities\n",
    "INPUT_TOKENS = cfg_data.grid_size * cfg_data.grid_size\n",
    "OUTPUT_TOKENS = cfg_data.n_cities\n",
    "\n",
    "# Sanity on one batch\n",
    "bx, by = next(iter(train_loader))\n",
    "assert bx.shape[1] == SEQ_LEN and by.shape[1] == SEQ_LEN\n",
    "assert bx.min().item() >= 0 and bx.max().item() < INPUT_TOKENS\n",
    "assert by.min().item() >= 0 and by.max().item() < OUTPUT_TOKENS\n",
    "print(\"Sanity OK: x in [0, INPUT_TOKENS-1], y in [0, n_cities-1], seq_len =\", SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized input embedding with coordinate features.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def init_coord_embedding_(model: TRM, grid_size: int, d_model: int, seed: int = 0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    V = grid_size * grid_size\n",
    "\n",
    "    W = np.zeros((V, d_model), dtype=np.float32)\n",
    "    for t in range(V):\n",
    "        x = t % grid_size\n",
    "        y = t // grid_size\n",
    "        # normalize to [-1, 1]\n",
    "        xf = (x / (grid_size - 1)) * 2.0 - 1.0\n",
    "        yf = (y / (grid_size - 1)) * 2.0 - 1.0\n",
    "        r  = math.sqrt(xf*xf + yf*yf)\n",
    "        th = math.atan2(yf, xf) / math.pi  # ~[-1, 1]\n",
    "\n",
    "        # coordinate features (6 dims): x, y, xy, x^2, y^2, r, (optionally angle)\n",
    "        feats = [xf, yf, xf*yf, xf*xf, yf*yf, r, th]\n",
    "        k = len(feats)\n",
    "        W[t, :k] = np.array(feats, dtype=np.float32)\n",
    "\n",
    "    # small random for remaining channels\n",
    "    if d_model > k:\n",
    "        W[:, k:] = 0.02 * rng.standard_normal(size=(V, d_model - k)).astype(np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.input_emb.weight.copy_(torch.from_numpy(W))\n",
    "\n",
    "# call once after model creation\n",
    "init_coord_embedding_(model, grid_size=cfg_data.grid_size, d_model=cfg_trm.d_model, seed=123)\n",
    "print(\"Initialized input embedding with coordinate features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSP data TRM: quick start and reference\n",
    "\n",
    "The dataset provides fixedlength `(x_tokens, y_tokens)` pairs suitable for training the Tiny Recursion Model (TRM) on Traveling Salesman Problem (TSP) instances with a discrete 2D grid.\n",
    "\n",
    "- Sequence length `L = n_cities`.\n",
    "- Input vocabulary: integer coordinates on a `grid_size x grid_size` grid, flattened to a single token per city. `input_vocab_size = grid_size * grid_size`.\n",
    "- Output vocabulary: successor city indices in the tour. `output_vocab_size = n_cities`.\n",
    "- Label semantics: `y_tokens[j]` is the index of the next city after city `j` on a canonical optimal tour starting at city 0. Reverse duplicates are tiebroken deterministically.\n",
    "- TRM expects fixed [B, L] inputs and [B, L] outputs trained with tokenlevel cross entropy and a halting head supervised by exactmatch. This format follows that setup (see Section 2.1 of the TRM paper for the [B, L] task framing). :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Install and import\n",
    "\n",
    "Place `tsp_data.py` next to your notebook or in `src/`. Then:\n",
    "\n",
    "```python\n",
    "from tsp_data import (\n",
    "    TSPConfig, TSPDataset, get_tsp_loaders,\n",
    "    decode_coords, distance_matrix, tour_length_from_successors\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Configure a run\n",
    "\n",
    "Pick the number of cities and grid size. The solver is chosen automatically:\n",
    "- Exact HeldKarp for small `n_cities` (up to `max_exact_cities`, default 12).\n",
    "- 2opt local search otherwise.\n",
    "\n",
    "```python\n",
    "cfg = TSPConfig(\n",
    "    n_cities=10,        # sequence length L\n",
    "    grid_size=16,       # tokens in {0..grid_size^2-1}\n",
    "    solver=\"auto\",      # \"auto\" | \"held_karp\" | \"two_opt\"\n",
    "    max_exact_cities=12,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "train_loader, val_loader = get_tsp_loaders(\n",
    "    n_train=2048,\n",
    "    n_val=512,\n",
    "    batch_size=32,\n",
    "    cfg=cfg\n",
    ")\n",
    "\n",
    "# Derive TRM dimensions\n",
    "SEQ_LEN = cfg.n_cities\n",
    "INPUT_TOKENS = cfg.grid_size * cfg.grid_size\n",
    "output_vocab_size = cfg.n_cities\n",
    "```\n",
    "\n",
    "Notes:\n",
    "- For proofofconcept training, try `n_cities` in `[6, 12]` so you get exact optimal tours cheaply.\n",
    "- For larger `n_cities`, switch to `\"two_opt\"` explicitly if you want speed.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) What a batch looks like\n",
    "\n",
    "Each batch yields integer tensors shaped `[B, L]`.\n",
    "\n",
    "```python\n",
    "x_tokens, y_tokens = next(iter(train_loader))\n",
    "print(x_tokens.shape, y_tokens.shape)   # torch.Size([B, L]) torch.Size([B, L])\n",
    "\n",
    "# Contracts\n",
    "assert x_tokens.min().item() >= 0 and x_tokens.max().item() < INPUT_TOKENS\n",
    "assert y_tokens.min().item() >= 0 and y_tokens.max().item() < output_vocab_size\n",
    "```\n",
    "\n",
    "Semantics:\n",
    "- `x_tokens[j]` is the flattened coordinate token of city `j`.\n",
    "- `y_tokens[j]` is the successor city index after `j` in the canonical tour.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Accessing coordinates and tour quality\n",
    "\n",
    "You can decode integer coordinate tokens and compute distances and tour length to verify data or evaluate predictions.\n",
    "\n",
    "```python\n",
    "# Decode integer coordinates for one item\n",
    "coords = decode_coords(x_tokens[0].cpu().numpy(), cfg.grid_size)  # shape [L, 2]\n",
    "\n",
    "# Build Euclidean distance matrix\n",
    "dist = distance_matrix(coords)  # shape [L, L], float64\n",
    "\n",
    "# Ground truth tour length from the successor mapping\n",
    "gt_len = tour_length_from_successors(dist, y_tokens[0].cpu().numpy())\n",
    "print(\"Ground truth tour length:\", gt_len)\n",
    "```\n",
    "\n",
    "To reconstruct the explicit path `[0, v1, ..., v_{L-1}, 0]` from successors:\n",
    "\n",
    "```python\n",
    "def path_from_successors(succ, start=0):\n",
    "    n = len(succ)\n",
    "    path = [start]\n",
    "    cur = start\n",
    "    for _ in range(n - 1):\n",
    "        cur = int(succ[cur])\n",
    "        path.append(cur)\n",
    "    path.append(start)\n",
    "    return path\n",
    "\n",
    "print(\"Canonical path:\", path_from_successors(y_tokens[0].cpu().numpy(), start=0))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Plugging into TRM\n",
    "\n",
    "Your TRM expects:\n",
    "- `seq_len = L = n_cities`\n",
    "- `input_vocab_size = grid_size * grid_size`\n",
    "- `output_vocab_size = n_cities`\n",
    "\n",
    "Example config sketch:\n",
    "\n",
    "```python\n",
    "from exploretinyrm.trm import TRM, TRMConfig\n",
    "\n",
    "cfg_trm = TRMConfig(\n",
    "    input_vocab_size=INPUT_TOKENS,\n",
    "    output_vocab_size=output_vocab_size,\n",
    "    seq_len=SEQ_LEN,\n",
    "    d_model=128,\n",
    "    n_layers=2,\n",
    "    use_attention=True,     # attention often helps for TSP; Mixer also works when L <= D\n",
    "    n_heads=8,\n",
    "    dropout=0.0,\n",
    "    mlp_ratio=4.0,\n",
    "    token_mlp_ratio=2.0,\n",
    "    n=6,\n",
    "    T=3,\n",
    "    k_last_ops=None,\n",
    "    stabilize_input_sums=True\n",
    ")\n",
    "\n",
    "model = TRM(cfg_trm).to(device)\n",
    "```\n",
    "\n",
    "Training loop\n",
    "- Use the same training loop you used for Sudoku:\n",
    "  - Inputs: `x_tokens` as produced by the loader.\n",
    "  - Targets: `y_tokens`.\n",
    "  - Loss: pertoken crossentropy on logits vs `y_tokens`, plus the halting BCE target on exactmatch.\n",
    "- The output head produces `[B, L, V]` logits where `V = n_cities`. Compare with `y_tokens` directly. This follows the TRM papers pertoken CE plus halting objective. :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Common configurations\n",
    "\n",
    "- Small exact TSP for correctness:\n",
    "  - `n_cities=8..12`, `grid_size=16`, `solver=\"held_karp\"` or `\"auto\"`.\n",
    "- Medium approximate TSP for throughput:\n",
    "  - `n_cities=20..50`, `grid_size=32`, `solver=\"two_opt\"`.\n",
    "- Reproducibility:\n",
    "  - Set `seed` in `TSPConfig`. Traintime randomness is controlled separately by your global PyTorch and NumPy seeds.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Sanity and evaluation snippets\n",
    "\n",
    "Sanity on one validation item:\n",
    "\n",
    "```python\n",
    "x_val, y_val = next(iter(val_loader))\n",
    "coords = decode_coords(x_val[0].cpu().numpy(), cfg.grid_size)\n",
    "dist = distance_matrix(coords)\n",
    "gt_len = tour_length_from_successors(dist, y_val[0].cpu().numpy())\n",
    "print(\"Validation tour length:\", gt_len)\n",
    "```\n",
    "\n",
    "If model predicts `pred_succ` of shape `[L]` with values in `[0, L-1]`:\n",
    "\n",
    "```python\n",
    "pred_path = path_from_successors(pred_succ)\n",
    "pred_len = tour_length_from_successors(dist, pred_succ)\n",
    "print(\"Pred tour length:\", pred_len)\n",
    "```\n",
    "\n",
    "Exactmatch at sequence level:\n",
    "\n",
    "```python\n",
    "# logits: [B, L, V], y_val: [B, L]\n",
    "preds = logits.argmax(dim=-1)          # [B, L]\n",
    "exact_match = (preds == y_val).all(dim=1).float().mean().item()\n",
    "print(\"Exactmatch:\", exact_match)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8) API summary\n",
    "\n",
    "- `TSPConfig(n_cities, grid_size, solver=\"auto\", max_exact_cities=12, seed)`\n",
    "- `TSPDataset(n_samples, cfg) -> yields (x_tokens, y_tokens) with shape [L]`\n",
    "- `get_tsp_loaders(n_train, n_val, batch_size, cfg)`\n",
    "- `decode_coords(x_tokens, grid_size) -> [L, 2] int`\n",
    "- `distance_matrix(coords) -> [L, L] float`\n",
    "- `tour_length_from_successors(dist, succ) -> float`\n",
    "- Successor labels always form a single Hamiltonian cycle and are canonicalized with start city 0 and deterministic tiebreak.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Notes\n",
    "\n",
    "- Exact solver is used for small `n_cities` so training labels are optimal. For larger `n_cities`, 2opt produces strong approximate labels quickly.\n",
    "- If you change `n_cities`, remember to update `TRMConfig.output_vocab_size` and `seq_len` accordingly.\n",
    "- If you change `grid_size`, only `input_vocab_size` changes. The sequence length stays equal to `n_cities`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_val, y_val = next(iter(val_loader))\n",
    "_ = plot_tsp_instance_from_tokens(\n",
    "    x_val[0], y_val[0], cfg.grid_size,\n",
    "    annotate=True,\n",
    "    title=f\"TSP instance (L={cfg.n_cities})\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ring_shift_matrix(L: int, device: torch.device, dtype: torch.dtype) -> torch.Tensor:\n",
    "    R = torch.zeros(L, L, device=device, dtype=dtype)\n",
    "    ar = torch.arange(L, device=device)\n",
    "    R[ar, (ar + 1) % L] = 1.0\n",
    "    return R\n",
    "\n",
    "\n",
    "def sinkhorn_logits_to_Q(logits: torch.Tensor, iters: int, tau: float, eps: float = 1e-9) -> torch.Tensor:\n",
    "    # logits: [B, L, L] row=position, col=city\n",
    "    P = logits / tau\n",
    "    P = P - P.amax(dim=-1, keepdim=True)\n",
    "    P = P.exp()\n",
    "    for _ in range(iters):\n",
    "        P = P / (P.sum(dim=-1, keepdim=True) + eps)  # rows -> 1\n",
    "        P = P / (P.sum(dim=-2, keepdim=True) + eps)  # cols -> 1\n",
    "    return P\n",
    "\n",
    "\n",
    "def rotate_and_optionally_reverse_path(y_true: torch.Tensor, rng: torch.Generator | None = None) -> torch.Tensor:\n",
    "    # y_true: [B, L] successor mapping; return path [B, L] starting at city 0, then rotate and reverse randomly\n",
    "    B, L = y_true.shape\n",
    "    path = successors_to_path(y_true)  # your helper; [B, L], starts at 0\n",
    "    if rng is None:\n",
    "        rng = torch.Generator(device=y_true.device)\n",
    "    # random rotation in [0..L-1] and random direction\n",
    "    rot = torch.randint(low=0, high=L, size=(B,), generator=rng, device=y_true.device)\n",
    "    dir_flip = torch.randint(low=0, high=2, size=(B,), generator=rng, device=y_true.device)  # 0 or 1\n",
    "    out = torch.empty_like(path)\n",
    "    for b in range(B):\n",
    "        p = path[b]\n",
    "        if dir_flip[b].item() == 1:\n",
    "            p = torch.flip(p, dims=[0])\n",
    "        r = rot[b].item()\n",
    "        out[b] = torch.roll(p, shifts=-r, dims=0)\n",
    "    return out  # [B, L]\n",
    "\n",
    "\n",
    "def soft_successor_from_Q(Q: torch.Tensor, R: torch.Tensor) -> torch.Tensor:\n",
    "    # Q: [B, L, L] (pos->city), R: [L, L] cyclic shift\n",
    "    return Q.transpose(1, 2) @ R.unsqueeze(0) @ Q  # [B, L, L]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part B helpers ---\n",
    "def sinkhorn(M: torch.Tensor, n_iters: int = 10, tau: float = 0.75, eps: float = 1e-9) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Project row-logits M to an approximately doubly-stochastic matrix with temperature tau.\n",
    "    M: [B, L, L] logits\n",
    "    Returns P: [B, L, L] with rows and columns approximately summing to 1.\n",
    "    \"\"\"\n",
    "    P = (M / tau)\n",
    "    P = P - P.amax(dim=-1, keepdim=True)  # log-sum-exp stabilizer on each row\n",
    "    P = P.exp()\n",
    "    for _ in range(n_iters):\n",
    "        P = P / (P.sum(dim=-1, keepdim=True) + eps)  # rows -> 1\n",
    "        P = P / (P.sum(dim=-2, keepdim=True) + eps)  # cols -> 1\n",
    "    return P\n",
    "\n",
    "def pairwise_dist_from_tokens(x_tokens: torch.Tensor, grid_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return pairwise Euclidean distances D for integer-grid coordinates encoded in x_tokens.\n",
    "    x_tokens: [B, L], grid_size: int\n",
    "    Returns D: [B, L, L]\n",
    "    \"\"\"\n",
    "    gx = (x_tokens % grid_size).float()\n",
    "    gy = (x_tokens // grid_size).float()\n",
    "    dx = gx.unsqueeze(2) - gx.unsqueeze(1)\n",
    "    dy = gy.unsqueeze(2) - gy.unsqueeze(1)\n",
    "    return torch.sqrt(dx * dx + dy * dy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment  # already present\n",
    "\n",
    "# --- Part C helper (robust to non-sorted row indices) ---\n",
    "@torch.no_grad()\n",
    "def hungarian_assign_indices(logits_masked: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    logits_masked: [B, L, L] (diagonal already penalized)\n",
    "    return: [B, L], where out[b, i] = assigned column for row i\n",
    "    \"\"\"\n",
    "    B, L, _ = logits_masked.shape\n",
    "    out = []\n",
    "    for b in range(B):\n",
    "        M = logits_masked[b].detach().cpu().numpy()\n",
    "        r, c = linear_sum_assignment(-M)          # maximize logits\n",
    "        assign = np.empty(L, dtype=np.int64)      # map rows -> cols explicitly\n",
    "        assign[r] = c\n",
    "        out.append(torch.from_numpy(assign))\n",
    "    return torch.stack(out, dim=0).to(logits_masked.device)\n",
    "\n",
    "\n",
    "def cycle_trace_penalty(P: torch.Tensor, k_max: int | None = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Penalize short cycles by summing traces of P^k for k=2..k_max.\n",
    "    P: [B, L, L] approximately doubly-stochastic (Sinkhorn output).\n",
    "    Returns a scalar penalty (mean over batch).\n",
    "    \"\"\"\n",
    "    B, L, _ = P.shape\n",
    "    if k_max is None:\n",
    "        k_max = max(3, L // 2)  # enough to catch short/mid cycles\n",
    "    M = P\n",
    "    penalty = P.new_zeros(B)\n",
    "    for _ in range(2, k_max + 1):\n",
    "        M = M @ P\n",
    "        tr = torch.diagonal(M, dim1=1, dim2=2).sum(dim=1)\n",
    "        penalty = penalty + tr\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "def build_path_rowcol_targets(y_true: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    y_true[b, i] = successor of i (0..L-1). We build teacher-forced path supervision.\n",
    "    Returns:\n",
    "      rows[b, t] = the current city index at step t (starting from 0)\n",
    "      cols[b, t] = the correct next city (successor) at step t\n",
    "    Shapes: rows, cols are [B, L] with ints in [0, L-1].\n",
    "    Assumes your dataset canonicalizes start=0 for every example (it does).\n",
    "    \"\"\"\n",
    "    B, L = y_true.shape\n",
    "    rows = torch.empty_like(y_true)\n",
    "    cols = torch.empty_like(y_true)\n",
    "    ar = torch.arange(B, device=y_true.device)\n",
    "    cur = torch.zeros(B, dtype=torch.long, device=y_true.device)  # start city 0\n",
    "    for t in range(L):\n",
    "        rows[:, t] = cur\n",
    "        nxt = y_true[ar, cur]  # successor of current city\n",
    "        cols[:, t] = nxt\n",
    "        cur = nxt\n",
    "    return rows, cols\n",
    "\n",
    "\n",
    "def canonicalize_order_to_start0(order: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    order: [B, L], position -> city.\n",
    "    Rotate each sequence so that city 0 is at position 0.\n",
    "    \"\"\"\n",
    "    B, L = order.shape\n",
    "    out = torch.empty_like(order)\n",
    "    for b in range(B):\n",
    "        p = order[b]\n",
    "        idx0 = (p == 0).nonzero(as_tuple=True)[0].item()\n",
    "        out[b] = torch.roll(p, shifts=-idx0, dims=0)\n",
    "    return out  # [B, L]\n",
    "\n",
    "def reverse_order_keep_start0(order: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    order: [B, L] canonicalized with city 0 at position 0.\n",
    "    Return the reversed tour, still anchored at position 0 = city 0.\n",
    "    If order = [0, c1, c2, ..., c_{L-1}],\n",
    "    reversed anchored version is [0, c_{L-1}, c_{L-2}, ..., c1].\n",
    "    \"\"\"\n",
    "    return torch.cat([order[:, :1], torch.flip(order[:, 1:], dims=[1])], dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# AFTER  add this helper\n",
    "def masked_path_ce(logits_eff: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Teacher-forced path CE with 'visited' mask.\n",
    "    Allows return to start (city 0) only at the final step.\n",
    "    logits_eff: [B, L, L] (row i = logits for successor of city i)\n",
    "    y_true:     [B, L]    (successor labels; start=0 canonicalized)\n",
    "    \"\"\"\n",
    "    B, L, _ = logits_eff.shape\n",
    "    rows, cols = build_path_rowcol_targets(y_true)  # [B, L], [B, L]\n",
    "\n",
    "    # Score the row of the current city at each step t\n",
    "    logits_rows = logits_eff.gather(1, rows.unsqueeze(-1).expand(-1, -1, L))  # [B, L, L]\n",
    "\n",
    "    # Build mask of previously visited cities at each step t\n",
    "    one_hot_rows = F.one_hot(rows, num_classes=L).float()   # [B, L, L]\n",
    "    cum_visited  = torch.cumsum(one_hot_rows, dim=1).clamp(max=1.0)\n",
    "    mask_prev    = cum_visited - one_hot_rows               # visited strictly before t\n",
    "\n",
    "    # Permit return to start only at the final step\n",
    "    mask_prev[:, -1, 0] = 0.0\n",
    "\n",
    "    BIG = -1e9 if logits_rows.dtype not in (torch.float16, torch.bfloat16) else -1e4\n",
    "    logits_rows = logits_rows.masked_fill(mask_prev.bool(), BIG)\n",
    "\n",
    "    return F.cross_entropy(logits_rows.reshape(B * L, L), cols.reshape(B * L))\n",
    "\n",
    "\n",
    "\n",
    "def successors_to_path(succ: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"succ[b,i] = successor of i; return canonical path starting at 0.\"\"\"\n",
    "    B, L = succ.shape\n",
    "    arB = torch.arange(B, device=succ.device)\n",
    "    cur = torch.zeros(B, dtype=torch.long, device=succ.device)\n",
    "    path = torch.empty(B, L, dtype=torch.long, device=succ.device)\n",
    "    for t in range(L):\n",
    "        path[:, t] = cur\n",
    "        cur = succ[arB, cur]\n",
    "    return path  # [B, L], path[:,0] = 0\n",
    "\n",
    "\n",
    "def path_to_successors(path: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    path[b] is a permutation 0..L-1; create successors via:\n",
    "    succ[b, path[b, t]] = path[b, (t+1) % L]\n",
    "    \"\"\"\n",
    "    path = path.long()                                     # ensure index dtype\n",
    "    succ = torch.empty_like(path)\n",
    "    succ.scatter_(1, path, path.roll(shifts=-1, dims=1))   # one-shot assignment\n",
    "    return succ\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def exact_match_from_logits(logits: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    preds = logits.argmax(dim=-1)              # [B, L]\n",
    "    return (preds == y_true).all(dim=1).float()\n",
    "\n",
    "def token_ce_loss(logits: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, V = logits.shape\n",
    "    return F.cross_entropy(logits.reshape(B * L, V), y_true.reshape(B * L))\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters for structural regularization\n",
    "ENFORCE_NO_SELF = True        # forbid s[i] = i\n",
    "LAMBDA_PERM = 0.2             # column-sum (permutation) penalty weight\n",
    "LAMBDA_2CYCLE = 0.05          # 2-cycle penalty weight (keep small)\n",
    "LAMBDA_PREVCE = 0.2             # NEW: predecessor cross-entropy weight\n",
    "\n",
    "LAMBDA_LEN = 0.05      # soft tour-length weight during warmup (epoch schedule applied below)\n",
    "SINKHORN_ITERS = 10\n",
    "SINKHORN_TAU = 0.75\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "def build_predecessor_targets(y_true: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    y_true[b, i] = successor of i. Return prev[b, j] = predecessor of j.\n",
    "    Shapes: y_true and return are [B, L] with ints in [0, L-1].\n",
    "    \"\"\"\n",
    "    B, L = y_true.shape\n",
    "    ar = torch.arange(L, device=y_true.device).unsqueeze(0).expand(B, L)  # indices i\n",
    "    prev = torch.empty_like(y_true)\n",
    "    prev.scatter_(1, y_true, ar)  # for each i, set prev[ successor(i) ] = i\n",
    "    return prev\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: TRM,\n",
    "    loader: DataLoader,\n",
    "    optimizer,\n",
    "    scaler,\n",
    "    epoch: int,\n",
    "    use_amp: bool = True,\n",
    "    ema: \"EMA | None\" = None,\n",
    "    grid_size: int | None = None,\n",
    "):\n",
    "    assert grid_size is not None\n",
    "\n",
    "    # schedules\n",
    "    if epoch <= 20:\n",
    "        tau, iters = 0.55, 16\n",
    "        w_pos, w_perm, w_ent, w_len, w_match = 1.0, 0.3, 0.15, 0.15, 0.20\n",
    "    elif epoch <= 80:\n",
    "        tau, iters = 0.65, 12\n",
    "        w_pos, w_perm, w_ent, w_len, w_match = 1.0, 0.2, 0.05, 0.10, 0.20\n",
    "    elif epoch <= 120:\n",
    "        tau, iters = 0.75, 10\n",
    "        w_pos, w_perm, w_ent, w_len, w_match = 0.8, 0.1, 0.02, 0.06, 0.15\n",
    "    else:\n",
    "        tau, iters = 0.80, 8\n",
    "        w_pos, w_perm, w_ent, w_len, w_match = 0.6, 0.05, 0.00, 0.00, 0.10\n",
    "\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    is_cuda = (device.type == \"cuda\")\n",
    "    model.train()\n",
    "\n",
    "    # logging accumulators\n",
    "    tot_pos, tot_perm, tot_ent, tot_len, tot_match, tot_em = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for x_tokens, y_true in loader:\n",
    "        x_tokens = x_tokens.to(device, non_blocking=True)\n",
    "        y_true   = y_true.to(device,   non_blocking=True)\n",
    "\n",
    "        # geometry\n",
    "        gx = (x_tokens % grid_size).float()\n",
    "        gy = (x_tokens // grid_size).float()\n",
    "        dx = gx.unsqueeze(2) - gx.unsqueeze(1)\n",
    "        dy = gy.unsqueeze(2) - gy.unsqueeze(1)\n",
    "        D  = torch.sqrt(dx * dx + dy * dy)  # [B, L, L]\n",
    "        mean_offdiag = D.sum(dim=(1,2)) / (D.shape[1] * D.shape[2] - D.shape[1])\n",
    "        D  = (D / mean_offdiag.view(-1, 1, 1)).to(torch.float32)\n",
    "\n",
    "        y_state, z_state = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with amp_autocast(is_cuda, use_amp):\n",
    "            y_state, z_state, logits_pos, halt_logit = model.forward_step(\n",
    "                x_tokens, y=y_state, z=z_state, n=model.cfg.n, T=model.cfg.T, k_last_ops=model.cfg.k_last_ops\n",
    "            )  # logits_pos: [B, L, L], row=position, col=city\n",
    "\n",
    "\n",
    "\n",
    "            # canonical supervision: start at city 0; optionally allow reversal\n",
    "            path_gt  = successors_to_path(y_true)                 # [B, L], starts at 0\n",
    "            path_rev = torch.cat([path_gt[:, :1], torch.flip(path_gt[:, 1:], dims=[1])], dim=1)\n",
    "\n",
    "            B, L, _ = logits_pos.shape\n",
    "            loss_pos_fwd = F.cross_entropy(logits_pos.reshape(B*L, L), path_gt.reshape(B*L))\n",
    "            loss_pos_rev = F.cross_entropy(logits_pos.reshape(B*L, L), path_rev.reshape(B*L))\n",
    "            loss_pos     = torch.minimum(loss_pos_fwd, loss_pos_rev)\n",
    "\n",
    "\n",
    "\n",
    "            # Sinkhorn projection and permutation regularizers\n",
    "            Q = sinkhorn_logits_to_Q(logits_pos, iters=iters, tau=tau)  # [B, L, L]\n",
    "            row_sum = Q.sum(dim=-1)\n",
    "            col_sum = Q.sum(dim=-2)\n",
    "            loss_perm = F.mse_loss(row_sum, torch.ones_like(row_sum)) + F.mse_loss(col_sum, torch.ones_like(col_sum))\n",
    "\n",
    "            # small entropy penalty on rows to sharpen assignments\n",
    "            Q_safe = Q.clamp_min(1e-8)\n",
    "            row_ent = -(Q_safe * Q_safe.log()).sum(dim=-1)  # [B, L]\n",
    "            loss_ent = row_ent.mean()\n",
    "\n",
    "            # expected length via soft successor S = Q^T R Q\n",
    "            R = ring_shift_matrix(L, device=Q.device, dtype=Q.dtype)\n",
    "            S = soft_successor_from_Q(Q, R)  # [B, L, L]\n",
    "            loss_len = (S.to(D.dtype) * D).sum(dim=(1,2)).mean() / L\n",
    "\n",
    "            # light matching NLL against Hungarian on position->city\n",
    "            assign = hungarian_assign_indices(logits_pos)         # [B, L], already provided in your helpers\n",
    "            logp   = F.log_softmax(logits_pos, dim=-1)\n",
    "            loss_match = -logp.gather(2, assign.unsqueeze(-1)).squeeze(-1).mean()\n",
    "\n",
    "            # total loss\n",
    "            loss = (\n",
    "                w_pos   * loss_pos +\n",
    "                w_perm  * loss_perm +\n",
    "                w_ent   * loss_ent +\n",
    "                w_len   * loss_len +\n",
    "                w_match * loss_match\n",
    "            )\n",
    "\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        if ema is not None:\n",
    "            ema.update(model)\n",
    "\n",
    "        # logging\n",
    "        with torch.no_grad():\n",
    "            preds_hun = assign  # [B, L]\n",
    "            preds_hun = canonicalize_order_to_start0(preds_hun)      # align predicted start\n",
    "            path_gt   = successors_to_path(y_true)                   # [B, L], canonical forward\n",
    "            path_rev  = reverse_order_keep_start0(path_gt)           # [B, L], canonical reverse\n",
    "\n",
    "            match_fwd = (preds_hun == path_gt).all(dim=1)\n",
    "            match_rev = (preds_hun == path_rev).all(dim=1)\n",
    "            em_pos    = torch.maximum(match_fwd, match_rev).float().mean()\n",
    "\n",
    "            tot_em   += em_pos.item()\n",
    "            tot_pos  += loss_pos.detach().item()\n",
    "            tot_perm += loss_perm.detach().item()\n",
    "            tot_ent  += loss_ent.detach().item()\n",
    "            tot_len  += loss_len.detach().item()\n",
    "            tot_match+= loss_match.detach().item()\n",
    "            steps    += 1\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | PosCE {tot_pos/max(1,steps):.4f} | Perm {tot_perm/max(1,steps):.4f} \"\n",
    "        f\"| Ent {tot_ent/max(1,steps):.4f} | Len {tot_len/max(1,steps):.4f} | Match {tot_match/max(1,steps):.4f} \"\n",
    "        f\"| EM(pos, Hungarian) {tot_em/max(1,steps):.3f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_order(model: TRM, loader: DataLoader, n_sup_eval: int = N_SUP):\n",
    "    model.eval()\n",
    "    em_pos_list, cover_list, valid_list, gap_list = [], [], [], []\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for x_tokens, y_true in loader:\n",
    "        x_tokens = x_tokens.to(device)\n",
    "        y_true   = y_true.to(device)\n",
    "\n",
    "        # roll N_SUP times\n",
    "        y_state, z_state = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "        for _ in range(n_sup_eval):\n",
    "            y_state, z_state, logits_pos, _ = model.forward_step(x_tokens, y=y_state, z=z_state)\n",
    "\n",
    "        # 1) decode position->city with Hungarian\n",
    "        assign = hungarian_assign_indices(logits_pos)     # [B, L]\n",
    "\n",
    "        # 2) canonicalize to start at city 0\n",
    "        assign_c = canonicalize_order_to_start0(assign)   # [B, L]\n",
    "\n",
    "        # 3) dihedral-invariant EM (allow reversal)\n",
    "        path_gt  = successors_to_path(y_true)             # [B, L]\n",
    "        path_rev = reverse_order_keep_start0(path_gt)     # [B, L]\n",
    "        match_fwd = (assign_c == path_gt).all(dim=1).float()\n",
    "        match_rev = (assign_c == path_rev).all(dim=1).float()\n",
    "        em_pos    = torch.maximum(match_fwd, match_rev)   # [B]\n",
    "        em_pos_list.append(em_pos)\n",
    "\n",
    "        # 4) successor prediction derived from canonicalized order\n",
    "        succ_pred = path_to_successors(assign_c)          # [B, L]\n",
    "\n",
    "        # 5) Coverage, ValidCycle, AvgGap(valid)\n",
    "        x_np = x_tokens.detach().cpu().numpy()\n",
    "        y_np = y_true.detach().cpu().numpy()\n",
    "        p_np = succ_pred.detach().cpu().numpy()\n",
    "        for i in range(x_np.shape[0]):\n",
    "            coords = decode_coords(x_np[i], cfg_data.grid_size)  # uses your existing cfg_data\n",
    "            dist   = distance_matrix(coords)\n",
    "            gt_path = path_from_successors(y_np[i], start=0)\n",
    "            pr_path = path_from_successors(p_np[i], start=0)\n",
    "            gt_len  = sum(dist[gt_path[k], gt_path[k+1]] for k in range(len(gt_path)-1))\n",
    "            pr_len  = sum(dist[pr_path[k], pr_path[k+1]] for k in range(len(pr_path)-1))\n",
    "            coverage = len(set(pr_path[:-1])) / SEQ_LEN\n",
    "            cover_list.append(coverage)\n",
    "            valid = _is_single_cycle_local(p_np[i])\n",
    "            valid_list.append(1.0 if valid else 0.0)\n",
    "            gap_list.append((pr_len - gt_len) if (coverage == 1.0 and valid) else np.nan)\n",
    "\n",
    "    em_pos    = torch.cat(em_pos_list).mean().item() if em_pos_list else 0.0\n",
    "    coverage  = float(np.nanmean(cover_list)) if cover_list else 0.0\n",
    "    valid_rate= float(np.nanmean(valid_list)) if valid_list else 0.0\n",
    "    gap_valid = float(np.nanmean([g for g in gap_list if not np.isnan(g)])) \\\n",
    "                if any(not np.isnan(g) for g in gap_list) else float('nan')\n",
    "\n",
    "    print(f\"Validation | EM(pos,Hun) {em_pos:.3f} | Coverage {coverage:.3f} | ValidCycle {valid_rate:.3f} | AvgGap(valid) {gap_valid:.3f}\")\n",
    "    return em_pos, coverage, valid_rate, gap_valid\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_order_with_ema(model: TRM, ema: EMA, loader: DataLoader, n_sup_eval: int = N_SUP):\n",
    "    with use_ema_weights(model, ema):\n",
    "        return evaluate_order(model, loader, n_sup_eval=n_sup_eval)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (6) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      8\u001b[39m     y0, z0 = model.init_state(batch_size=x_tokens.size(\u001b[32m0\u001b[39m), device=device)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     y1, z1, logits, halt_logit = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK_LAST_OPS\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mForward-only finiteness:\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33my1\u001b[39m\u001b[33m\"\u001b[39m, torch.isfinite(y1).all().item(),\n\u001b[32m     14\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mz1\u001b[39m\u001b[33m\"\u001b[39m, torch.isfinite(z1).all().item(),\n\u001b[32m     15\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m, torch.isfinite(logits).all().item(),\n\u001b[32m     16\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mhalt_logit\u001b[39m\u001b[33m\"\u001b[39m, torch.isfinite(halt_logit).all().item())\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Single FP32 step to ensure gradients flow\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:458\u001b[39m, in \u001b[36mTRM.forward_step\u001b[39m\u001b[34m(self, x_tokens, y, z, n, T, k_last_ops)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (z \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    457\u001b[39m     y, z = \u001b[38;5;28mself\u001b[39m.init_state(batch_size=x_tokens.size(\u001b[32m0\u001b[39m), device=x_tokens.device)\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeep_recursion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:413\u001b[39m, in \u001b[36mTRM.deep_recursion\u001b[39m\u001b[34m(self, x_h, y, z, n, T, k_last_ops)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, T - \u001b[32m1\u001b[39m)):\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m         y, z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlatent_recursion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_grads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# Final loop with gradients; optionally backprop only through the last k ops\u001b[39;00m\n\u001b[32m    416\u001b[39m y, z = \u001b[38;5;28mself\u001b[39m.latent_recursion(x_h, y, z, n=n, k_last_ops=k_last_ops, track_grads=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:372\u001b[39m, in \u001b[36mTRM.latent_recursion\u001b[39m\u001b[34m(self, x_h, y, z, n, k_last_ops, track_grads)\u001b[39m\n\u001b[32m    370\u001b[39m \u001b[38;5;66;03m# n times: z = net(x + y + z)\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     h_z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sum_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstabilize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstabilize_input_sums\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m     ctx = nullcontext() \u001b[38;5;28;01mif\u001b[39;00m (track_grads \u001b[38;5;129;01mand\u001b[39;00m op_index >= cutoff) \u001b[38;5;28;01melse\u001b[39;00m torch.no_grad()\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:338\u001b[39m, in \u001b[36mTRM._sum_inputs\u001b[39m\u001b[34m(tensors, stabilize)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum_inputs\u001b[39m(tensors: List[torch.Tensor], stabilize: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     h = \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stabilize:\n\u001b[32m    340\u001b[39m         h = h / math.sqrt(\u001b[38;5;28mlen\u001b[39m(tensors))\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (8) must match the size of tensor b (6) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# One forward-only sanity batch\n",
    "x_tokens, y_true = next(iter(train_loader)) #x_tokens, y_true = next(iter(train_loader))\n",
    "x_tokens = x_tokens.to(device)\n",
    "y_true   = y_true.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y0, z0 = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "    y1, z1, logits, halt_logit = model.forward_step(\n",
    "        x_tokens, y=y0, z=z0, n=N, T=T, k_last_ops=K_LAST_OPS\n",
    "    )\n",
    "print(\"Forward-only finiteness:\",\n",
    "      \"y1\", torch.isfinite(y1).all().item(),\n",
    "      \"z1\", torch.isfinite(z1).all().item(),\n",
    "      \"logits\", torch.isfinite(logits).all().item(),\n",
    "      \"halt_logit\", torch.isfinite(halt_logit).all().item())\n",
    "\n",
    "# Single FP32 step to ensure gradients flow\n",
    "model.train()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "opt.zero_grad(set_to_none=True)\n",
    "\n",
    "y0, z0 = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "y1, z1, logits, halt_logit = model.forward_step(\n",
    "    x_tokens, y=y0, z=z0, n=N, T=T, k_last_ops=K_LAST_OPS\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "path_tgt = successors_to_path(y_true)  # [B, L]\n",
    "loss_ce = F.cross_entropy(logits.float().reshape(-1, OUTPUT_TOKENS), path_tgt.reshape(-1))\n",
    "em_pos  = (logits.argmax(dim=-1) == path_tgt).all(dim=1).float()\n",
    "\n",
    "\n",
    "\n",
    "loss_halt = F.binary_cross_entropy_with_logits(halt_logit.float(), em_pos)\n",
    "loss = loss_ce + loss_halt\n",
    "print(\"Pre-backward finiteness:\",\n",
    "      \"loss\", torch.isfinite(loss).item(),\n",
    "      \"loss_ce\", torch.isfinite(loss_ce).item(),\n",
    "      \"loss_halt\", torch.isfinite(loss_halt).item())\n",
    "loss.backward()\n",
    "\n",
    "all_grads_finite = True\n",
    "for n, p in model.named_parameters():\n",
    "    if p.grad is None:\n",
    "        continue\n",
    "    if not torch.isfinite(p.grad).all():\n",
    "        print(\"Non-finite grad in:\", n)\n",
    "        all_grads_finite = False\n",
    "        break\n",
    "print(\"Gradients finite:\", all_grads_finite)\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "opt.step()\n",
    "\n",
    "# Post-step forward\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y0, z0 = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "    y2, z2, logits2, halt2 = model.forward_step(\n",
    "        x_tokens, y=y0, z=z0, n=N, T=T, k_last_ops=K_LAST_OPS\n",
    "    )\n",
    "print(\"Post-step forward finiteness:\",\n",
    "      \"y2\", torch.isfinite(y2).all().item(),\n",
    "      \"z2\", torch.isfinite(z2).all().item(),\n",
    "      \"logits2\", torch.isfinite(logits2).all().item(),\n",
    "      \"halt2\", torch.isfinite(halt2).all().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# FINAL evaluate override: 5 metrics, path-based\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def _is_single_cycle_local(successor: np.ndarray) -> bool:\n",
    "    n = len(successor)\n",
    "    seen = np.zeros(n, dtype=bool)\n",
    "    cur = 0\n",
    "    for _ in range(n):\n",
    "        if seen[cur]:\n",
    "            return False\n",
    "        seen[cur] = True\n",
    "        cur = int(successor[cur])\n",
    "    return cur == 0 and seen.all()\n",
    "\n",
    "def _tour_length_from_path(dist: np.ndarray, path: list[int]) -> float:\n",
    "    return float(sum(dist[path[i], path[i+1]] for i in range(len(path) - 1)))\n",
    "\n",
    "\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_hungarian(logits: torch.Tensor) -> torch.Tensor:\n",
    "    B, L, _ = logits.shape\n",
    "    preds = []\n",
    "    for b in range(B):\n",
    "        M = logits[b].detach().cpu().numpy().copy()\n",
    "        np.fill_diagonal(M, -1e9)               # forbid self loops\n",
    "        r, c = linear_sum_assignment(-M)        # maximize sum of logits\n",
    "        preds.append(torch.from_numpy(c).long())\n",
    "    return torch.stack(preds, dim=0).to(logits.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: TRM, loader: DataLoader, n_sup_eval: int = N_SUP):\n",
    "    model.eval()\n",
    "    em_arg_list, em_hun_list, tok_acc_list = [], [], []\n",
    "    cover_list, valid_list, gap_list = [], [], []\n",
    "\n",
    "    for x_tokens, y_true in loader:\n",
    "        x_tokens = x_tokens.to(device)\n",
    "        y_true   = y_true.to(device)\n",
    "\n",
    "        y_state, z_state = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "        for _ in range(n_sup_eval):\n",
    "            y_state, z_state, logits, halt_logit = model.forward_step(\n",
    "                x_tokens, y=y_state, z=z_state, n=N, T=T, k_last_ops=K_LAST_OPS\n",
    "            )\n",
    "\n",
    "        L = logits.shape[1]\n",
    "        ar = torch.arange(L, device=logits.device)\n",
    "\n",
    "        logits_masked = logits.clone()\n",
    "        mask_val = -1e4 if logits_masked.dtype in (torch.float16, torch.bfloat16) else -1e9\n",
    "        logits_masked[:, ar, ar] = mask_val\n",
    "\n",
    "\n",
    "        # Argmax decode\n",
    "        preds_arg = logits_masked.argmax(dim=-1)           # [B, L]\n",
    "        em_arg = (preds_arg == y_true).all(dim=1).float()\n",
    "        tok_acc = (preds_arg == y_true).float().mean(dim=1)\n",
    "\n",
    "        # Hungarian decode\n",
    "        preds_hun = decode_hungarian(logits_masked)        # [B, L]\n",
    "        em_hun = (preds_hun == y_true).all(dim=1).float()\n",
    "        \n",
    "        p_hun = preds_hun.detach().cpu().numpy()\n",
    "\n",
    "        # Path-based metrics (keep using argmax predictions)\n",
    "        x_np = x_tokens.detach().cpu().numpy()\n",
    "        y_np = y_true.detach().cpu().numpy()\n",
    "        p_np = preds_arg.detach().cpu().numpy()\n",
    "        for i in range(x_np.shape[0]):\n",
    "            coords = decode_coords(x_np[i], cfg_data.grid_size)\n",
    "            dist = distance_matrix(coords)\n",
    "            gt_path = path_from_successors(y_np[i], start=0)\n",
    "            pr_path = path_from_successors(p_np[i], start=0)\n",
    "            gt_len  = sum(dist[gt_path[k], gt_path[k+1]] for k in range(len(gt_path)-1))\n",
    "            pr_len  = sum(dist[pr_path[k], pr_path[k+1]] for k in range(len(pr_path)-1))\n",
    "            coverage = len(set(pr_path[:-1])) / SEQ_LEN\n",
    "            cover_list.append(coverage)\n",
    "            valid = _is_single_cycle_local(p_np[i])\n",
    "            valid_list.append(1.0 if valid else 0.0)\n",
    "            gap_list.append((pr_len - gt_len) if (coverage == 1.0 and valid) else np.nan)\n",
    "\n",
    "        em_arg_list.append(em_arg)\n",
    "        em_hun_list.append(em_hun)\n",
    "        tok_acc_list.append(tok_acc)\n",
    "\n",
    "    em_arg = torch.cat(em_arg_list).mean().item()\n",
    "    em_hun = torch.cat(em_hun_list).mean().item()\n",
    "    tok_acc = torch.cat(tok_acc_list).mean().item()\n",
    "    coverage = float(np.nanmean(cover_list)) if cover_list else 0.0\n",
    "    valid_rate = float(np.nanmean(valid_list)) if valid_list else 0.0\n",
    "    gap_valid = float(np.nanmean([g for g in gap_list if not np.isnan(g)])) if any(not np.isnan(g) for g in gap_list) else float('nan')\n",
    "\n",
    "\n",
    "    print(f\"Validation | EM(arg) {em_arg:.3f} | EM(hung) {em_hun:.3f} | Token {tok_acc:.3f} | Coverage {coverage:.3f} | ValidCycle {valid_rate:.3f} | AvgGap(valid) {gap_valid:.3f}\")\n",
    "    valid_h = np.mean([_is_single_cycle_local(p_hun[i]) for i in range(p_hun.shape[0])])\n",
    "    print(f\"Validity (Hungarian): {valid_h:.3f}\")\n",
    "    # Optional: validity under argmax, for symmetry\n",
    "    p_arg = preds_arg.detach().cpu().numpy()\n",
    "    valid_a = np.mean([_is_single_cycle_local(p_arg[i]) for i in range(p_arg.shape[0])])\n",
    "    print(f\"Validity (Argmax):    {valid_a:.3f}\")\n",
    "\n",
    "    \n",
    "    return em_arg, em_hun, tok_acc, coverage, valid_rate, gap_valid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_ema(model: TRM, ema: EMA, loader: DataLoader, n_sup_eval: int = N_SUP):\n",
    "    with use_ema_weights(model, ema):\n",
    "        return evaluate(model, loader, n_sup_eval=n_sup_eval)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token 0: max_abs_diff = 0.000000\n",
      "token 1: max_abs_diff = 0.000000\n",
      "token 15: max_abs_diff = 0.000000\n",
      "token 240: max_abs_diff = 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Initialize input embeddings with coordinate features\n",
    "init_coord_embedding_(model, grid_size=cfg_data.grid_size, d_model=cfg_trm.d_model, seed=123)\n",
    "\n",
    "# Verification: check a few tokens match the analytic features\n",
    "def expected_feats(token_id: int, grid_size: int):\n",
    "    import math\n",
    "    x = token_id % grid_size\n",
    "    y = token_id // grid_size\n",
    "    xf = (x / (grid_size - 1)) * 2.0 - 1.0\n",
    "    yf = (y / (grid_size - 1)) * 2.0 - 1.0\n",
    "    r  = math.sqrt(xf*xf + yf*yf)\n",
    "    th = math.atan2(yf, xf) / math.pi\n",
    "    return np.array([xf, yf, xf*yf, xf*xf, yf*yf, r, th], dtype=np.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for token_id in [0, 1, cfg_data.grid_size - 1, cfg_data.grid_size*(cfg_data.grid_size - 1)]:\n",
    "        got = model.input_emb.weight[token_id, :7].cpu().numpy()\n",
    "        exp = expected_feats(token_id, cfg_data.grid_size)\n",
    "        print(f\"token {token_id}: max_abs_diff = {np.max(np.abs(got - exp)):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (6) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      6\u001b[39m     y0, z0 = model.init_state(batch_size=x_tokens.size(\u001b[32m0\u001b[39m), device=device)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     y1, z1, logits, halt = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK_LAST_OPS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlogits shape:\u001b[39m\u001b[33m\"\u001b[39m, logits.shape)  \u001b[38;5;66;03m# expect [B, L, L]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:458\u001b[39m, in \u001b[36mTRM.forward_step\u001b[39m\u001b[34m(self, x_tokens, y, z, n, T, k_last_ops)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (z \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    457\u001b[39m     y, z = \u001b[38;5;28mself\u001b[39m.init_state(batch_size=x_tokens.size(\u001b[32m0\u001b[39m), device=x_tokens.device)\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeep_recursion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:413\u001b[39m, in \u001b[36mTRM.deep_recursion\u001b[39m\u001b[34m(self, x_h, y, z, n, T, k_last_ops)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, T - \u001b[32m1\u001b[39m)):\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m         y, z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlatent_recursion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_grads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# Final loop with gradients; optionally backprop only through the last k ops\u001b[39;00m\n\u001b[32m    416\u001b[39m y, z = \u001b[38;5;28mself\u001b[39m.latent_recursion(x_h, y, z, n=n, k_last_ops=k_last_ops, track_grads=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:372\u001b[39m, in \u001b[36mTRM.latent_recursion\u001b[39m\u001b[34m(self, x_h, y, z, n, k_last_ops, track_grads)\u001b[39m\n\u001b[32m    370\u001b[39m \u001b[38;5;66;03m# n times: z = net(x + y + z)\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     h_z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sum_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstabilize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstabilize_input_sums\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m     ctx = nullcontext() \u001b[38;5;28;01mif\u001b[39;00m (track_grads \u001b[38;5;129;01mand\u001b[39;00m op_index >= cutoff) \u001b[38;5;28;01melse\u001b[39;00m torch.no_grad()\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:338\u001b[39m, in \u001b[36mTRM._sum_inputs\u001b[39m\u001b[34m(tensors, stabilize)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum_inputs\u001b[39m(tensors: List[torch.Tensor], stabilize: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     h = \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stabilize:\n\u001b[32m    340\u001b[39m         h = h / math.sqrt(\u001b[38;5;28mlen\u001b[39m(tensors))\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (8) must match the size of tensor b (6) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "x_tokens, y_true = next(iter(train_loader))\n",
    "x_tokens = x_tokens.to(device)\n",
    "y_true = y_true.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y0, z0 = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "    y1, z1, logits, halt = model.forward_step(x_tokens, y=y0, z=z0, n=N, T=T, k_last_ops=K_LAST_OPS)\n",
    "print(\"logits shape:\", logits.shape)  # expect [B, L, L]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | PosCE 1.7945 | Perm 0.0000 | Ent 1.7705 | Len 0.8352 | Match 1.6677 | EM(pos, Hungarian) 0.031\n",
      "Validation | EM(pos,Hun) 0.047 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 11.894\n",
      "Epoch 02 | PosCE 1.7828 | Perm 0.0000 | Ent 1.7628 | Len 0.8353 | Match 1.6486 | EM(pos, Hungarian) 0.047\n",
      "Validation | EM(pos,Hun) 0.031 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 11.790\n",
      "Epoch 03 | PosCE 1.7699 | Perm 0.0000 | Ent 1.7529 | Len 0.8354 | Match 1.6277 | EM(pos, Hungarian) 0.062\n",
      "Validation | EM(pos,Hun) 0.062 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 11.166\n",
      "Epoch 04 | PosCE 1.7660 | Perm 0.0000 | Ent 1.7408 | Len 0.8350 | Match 1.6061 | EM(pos, Hungarian) 0.062\n",
      "Validation | EM(pos,Hun) 0.062 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 11.171\n",
      "Epoch 05 | PosCE 1.7580 | Perm 0.0000 | Ent 1.7270 | Len 0.8344 | Match 1.5836 | EM(pos, Hungarian) 0.078\n",
      "Validation | EM(pos,Hun) 0.078 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 11.224\n",
      "Epoch 06 | PosCE 1.7474 | Perm 0.0000 | Ent 1.7118 | Len 0.8334 | Match 1.5625 | EM(pos, Hungarian) 0.078\n",
      "Validation | EM(pos,Hun) 0.047 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 11.368\n",
      "Epoch 07 | PosCE 1.7409 | Perm 0.0000 | Ent 1.6963 | Len 0.8317 | Match 1.5411 | EM(pos, Hungarian) 0.078\n",
      "Validation | EM(pos,Hun) 0.047 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 10.794\n",
      "Epoch 08 | PosCE 1.7297 | Perm 0.0000 | Ent 1.6808 | Len 0.8296 | Match 1.5202 | EM(pos, Hungarian) 0.094\n",
      "Validation | EM(pos,Hun) 0.062 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 10.784\n",
      "Epoch 09 | PosCE 1.7270 | Perm 0.0000 | Ent 1.6648 | Len 0.8270 | Match 1.4997 | EM(pos, Hungarian) 0.094\n",
      "Validation | EM(pos,Hun) 0.062 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 10.476\n",
      "Epoch 10 | PosCE 1.7124 | Perm 0.0000 | Ent 1.6481 | Len 0.8238 | Match 1.4785 | EM(pos, Hungarian) 0.109\n",
      "Validation | EM(pos,Hun) 0.094 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 9.816\n",
      "Epoch 11 | PosCE 1.7150 | Perm 0.0000 | Ent 1.6314 | Len 0.8204 | Match 1.4567 | EM(pos, Hungarian) 0.109\n",
      "Validation | EM(pos,Hun) 0.094 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 9.299\n",
      "Epoch 12 | PosCE 1.6827 | Perm 0.0000 | Ent 1.6160 | Len 0.8173 | Match 1.4367 | EM(pos, Hungarian) 0.109\n",
      "Validation | EM(pos,Hun) 0.094 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 8.851\n",
      "Epoch 13 | PosCE 1.6885 | Perm 0.0000 | Ent 1.5997 | Len 0.8140 | Match 1.4157 | EM(pos, Hungarian) 0.109\n",
      "Validation | EM(pos,Hun) 0.109 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 7.979\n",
      "Epoch 14 | PosCE 1.6666 | Perm 0.0000 | Ent 1.5833 | Len 0.8107 | Match 1.3944 | EM(pos, Hungarian) 0.125\n",
      "Validation | EM(pos,Hun) 0.125 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 7.729\n",
      "Epoch 15 | PosCE 1.6608 | Perm 0.0000 | Ent 1.5674 | Len 0.8076 | Match 1.3742 | EM(pos, Hungarian) 0.125\n",
      "Validation | EM(pos,Hun) 0.125 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 7.727\n",
      "Epoch 16 | PosCE 1.6547 | Perm 0.0000 | Ent 1.5520 | Len 0.8051 | Match 1.3552 | EM(pos, Hungarian) 0.125\n",
      "Validation | EM(pos,Hun) 0.141 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 7.633\n",
      "Epoch 17 | PosCE 1.6111 | Perm 0.0000 | Ent 1.5365 | Len 0.8029 | Match 1.3368 | EM(pos, Hungarian) 0.141\n",
      "Validation | EM(pos,Hun) 0.141 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 8.013\n",
      "Epoch 18 | PosCE 1.6126 | Perm 0.0000 | Ent 1.5194 | Len 0.8008 | Match 1.3172 | EM(pos, Hungarian) 0.156\n",
      "Validation | EM(pos,Hun) 0.141 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 7.775\n",
      "Epoch 19 | PosCE 1.6054 | Perm 0.0000 | Ent 1.5019 | Len 0.7994 | Match 1.2975 | EM(pos, Hungarian) 0.172\n",
      "Validation | EM(pos,Hun) 0.156 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 7.651\n",
      "Epoch 20 | PosCE 1.5835 | Perm 0.0000 | Ent 1.4866 | Len 0.7988 | Match 1.2796 | EM(pos, Hungarian) 0.172\n",
      "Validation | EM(pos,Hun) 0.156 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 7.521\n",
      "Epoch 21 | PosCE 1.5587 | Perm 0.0000 | Ent 1.5392 | Len 0.8034 | Match 1.2636 | EM(pos, Hungarian) 0.172\n",
      "Validation | EM(pos,Hun) 0.172 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 7.388\n",
      "Epoch 22 | PosCE 1.5190 | Perm 0.0000 | Ent 1.5295 | Len 0.8042 | Match 1.2500 | EM(pos, Hungarian) 0.172\n",
      "Validation | EM(pos,Hun) 0.172 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 7.179\n",
      "Epoch 23 | PosCE 1.5038 | Perm 0.0000 | Ent 1.5187 | Len 0.8051 | Match 1.2352 | EM(pos, Hungarian) 0.219\n",
      "Validation | EM(pos,Hun) 0.219 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 7.016\n",
      "Epoch 24 | PosCE 1.4754 | Perm 0.0000 | Ent 1.5067 | Len 0.8065 | Match 1.2201 | EM(pos, Hungarian) 0.250\n",
      "Validation | EM(pos,Hun) 0.250 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 7.136\n",
      "Epoch 25 | PosCE 1.4460 | Perm 0.0000 | Ent 1.4953 | Len 0.8083 | Match 1.2064 | EM(pos, Hungarian) 0.266\n",
      "Validation | EM(pos,Hun) 0.266 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 7.043\n",
      "Epoch 26 | PosCE 1.4141 | Perm 0.0000 | Ent 1.4819 | Len 0.8104 | Match 1.1918 | EM(pos, Hungarian) 0.297\n",
      "Validation | EM(pos,Hun) 0.266 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 7.056\n",
      "Epoch 27 | PosCE 1.3827 | Perm 0.0000 | Ent 1.4656 | Len 0.8123 | Match 1.1738 | EM(pos, Hungarian) 0.312\n",
      "Validation | EM(pos,Hun) 0.312 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 6.769\n",
      "Epoch 28 | PosCE 1.3513 | Perm 0.0000 | Ent 1.4470 | Len 0.8143 | Match 1.1556 | EM(pos, Hungarian) 0.359\n",
      "Validation | EM(pos,Hun) 0.344 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 5.888\n",
      "Epoch 29 | PosCE 1.3201 | Perm 0.0000 | Ent 1.4257 | Len 0.8160 | Match 1.1356 | EM(pos, Hungarian) 0.375\n",
      "Validation | EM(pos,Hun) 0.344 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 5.900\n",
      "Epoch 30 | PosCE 1.2705 | Perm 0.0000 | Ent 1.4020 | Len 0.8177 | Match 1.1134 | EM(pos, Hungarian) 0.344\n",
      "Validation | EM(pos,Hun) 0.344 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 6.047\n",
      "Epoch 31 | PosCE 1.2583 | Perm 0.0000 | Ent 1.3755 | Len 0.8188 | Match 1.0889 | EM(pos, Hungarian) 0.406\n",
      "Validation | EM(pos,Hun) 0.375 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 5.777\n",
      "Epoch 32 | PosCE 1.2285 | Perm 0.0000 | Ent 1.3472 | Len 0.8200 | Match 1.0618 | EM(pos, Hungarian) 0.391\n",
      "Validation | EM(pos,Hun) 0.438 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 5.205\n",
      "Epoch 33 | PosCE 1.1987 | Perm 0.0000 | Ent 1.3183 | Len 0.8214 | Match 1.0355 | EM(pos, Hungarian) 0.406\n",
      "Validation | EM(pos,Hun) 0.438 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 5.317\n",
      "Epoch 34 | PosCE 1.1684 | Perm 0.0000 | Ent 1.2890 | Len 0.8227 | Match 1.0109 | EM(pos, Hungarian) 0.438\n",
      "Validation | EM(pos,Hun) 0.438 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 5.231\n",
      "Epoch 35 | PosCE 1.1391 | Perm 0.0000 | Ent 1.2599 | Len 0.8238 | Match 0.9866 | EM(pos, Hungarian) 0.469\n",
      "Validation | EM(pos,Hun) 0.500 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 4.583\n",
      "Epoch 36 | PosCE 1.1094 | Perm 0.0000 | Ent 1.2294 | Len 0.8249 | Match 0.9615 | EM(pos, Hungarian) 0.484\n",
      "Validation | EM(pos,Hun) 0.531 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 4.454\n",
      "Epoch 37 | PosCE 1.0808 | Perm 0.0000 | Ent 1.1995 | Len 0.8254 | Match 0.9363 | EM(pos, Hungarian) 0.531\n",
      "Validation | EM(pos,Hun) 0.547 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 4.310\n",
      "Epoch 38 | PosCE 1.0543 | Perm 0.0000 | Ent 1.1688 | Len 0.8263 | Match 0.9111 | EM(pos, Hungarian) 0.531\n",
      "Validation | EM(pos,Hun) 0.547 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 4.484\n",
      "Epoch 39 | PosCE 1.0269 | Perm 0.0000 | Ent 1.1380 | Len 0.8265 | Match 0.8854 | EM(pos, Hungarian) 0.531\n",
      "Validation | EM(pos,Hun) 0.547 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 4.559\n",
      "Epoch 40 | PosCE 1.0004 | Perm 0.0000 | Ent 1.1087 | Len 0.8263 | Match 0.8617 | EM(pos, Hungarian) 0.531\n",
      "Validation | EM(pos,Hun) 0.547 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 4.559\n",
      "Epoch 41 | PosCE 0.9752 | Perm 0.0000 | Ent 1.0798 | Len 0.8262 | Match 0.8391 | EM(pos, Hungarian) 0.547\n",
      "Validation | EM(pos,Hun) 0.547 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 4.559\n",
      "Epoch 42 | PosCE 0.9509 | Perm 0.0000 | Ent 1.0511 | Len 0.8259 | Match 0.8169 | EM(pos, Hungarian) 0.562\n",
      "Validation | EM(pos,Hun) 0.594 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 4.098\n",
      "Epoch 43 | PosCE 0.9273 | Perm 0.0000 | Ent 1.0234 | Len 0.8254 | Match 0.7958 | EM(pos, Hungarian) 0.578\n",
      "Validation | EM(pos,Hun) 0.594 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 4.004\n",
      "Epoch 44 | PosCE 0.9030 | Perm 0.0000 | Ent 0.9954 | Len 0.8243 | Match 0.7741 | EM(pos, Hungarian) 0.594\n",
      "Validation | EM(pos,Hun) 0.594 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.793\n",
      "Epoch 45 | PosCE 0.8807 | Perm 0.0000 | Ent 0.9690 | Len 0.8233 | Match 0.7537 | EM(pos, Hungarian) 0.625\n",
      "Validation | EM(pos,Hun) 0.609 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.612\n",
      "Epoch 46 | PosCE 0.8584 | Perm 0.0000 | Ent 0.9423 | Len 0.8218 | Match 0.7331 | EM(pos, Hungarian) 0.625\n",
      "Validation | EM(pos,Hun) 0.625 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.428\n",
      "Epoch 47 | PosCE 0.8362 | Perm 0.0000 | Ent 0.9170 | Len 0.8204 | Match 0.7143 | EM(pos, Hungarian) 0.656\n",
      "Validation | EM(pos,Hun) 0.641 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.179\n",
      "Epoch 48 | PosCE 0.8146 | Perm 0.0001 | Ent 0.8925 | Len 0.8190 | Match 0.6960 | EM(pos, Hungarian) 0.656\n",
      "Validation | EM(pos,Hun) 0.656 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.144\n",
      "Epoch 49 | PosCE 0.7936 | Perm 0.0001 | Ent 0.8683 | Len 0.8174 | Match 0.6778 | EM(pos, Hungarian) 0.672\n",
      "Validation | EM(pos,Hun) 0.672 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.015\n",
      "Epoch 50 | PosCE 0.7721 | Perm 0.0001 | Ent 0.8438 | Len 0.8157 | Match 0.6597 | EM(pos, Hungarian) 0.672\n",
      "Validation | EM(pos,Hun) 0.672 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.280\n",
      "Epoch 51 | PosCE 0.7517 | Perm 0.0001 | Ent 0.8204 | Len 0.8141 | Match 0.6428 | EM(pos, Hungarian) 0.688\n",
      "Validation | EM(pos,Hun) 0.656 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.302\n",
      "Epoch 52 | PosCE 0.7312 | Perm 0.0001 | Ent 0.7975 | Len 0.8123 | Match 0.6269 | EM(pos, Hungarian) 0.688\n",
      "Validation | EM(pos,Hun) 0.672 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.239\n",
      "Epoch 53 | PosCE 0.7110 | Perm 0.0001 | Ent 0.7739 | Len 0.8104 | Match 0.6097 | EM(pos, Hungarian) 0.703\n",
      "Validation | EM(pos,Hun) 0.672 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.366\n",
      "Epoch 54 | PosCE 0.6909 | Perm 0.0001 | Ent 0.7519 | Len 0.8086 | Match 0.5928 | EM(pos, Hungarian) 0.703\n",
      "Validation | EM(pos,Hun) 0.688 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.098\n",
      "Epoch 55 | PosCE 0.6716 | Perm 0.0001 | Ent 0.7308 | Len 0.8068 | Match 0.5772 | EM(pos, Hungarian) 0.703\n",
      "Validation | EM(pos,Hun) 0.688 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.098\n",
      "Epoch 56 | PosCE 0.6519 | Perm 0.0001 | Ent 0.7091 | Len 0.8050 | Match 0.5612 | EM(pos, Hungarian) 0.719\n",
      "Validation | EM(pos,Hun) 0.688 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.098\n",
      "Epoch 57 | PosCE 0.6325 | Perm 0.0001 | Ent 0.6882 | Len 0.8034 | Match 0.5458 | EM(pos, Hungarian) 0.719\n",
      "Validation | EM(pos,Hun) 0.703 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 3.071\n",
      "Epoch 58 | PosCE 0.6135 | Perm 0.0002 | Ent 0.6671 | Len 0.8014 | Match 0.5310 | EM(pos, Hungarian) 0.703\n",
      "Validation | EM(pos,Hun) 0.734 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 2.747\n",
      "Epoch 59 | PosCE 0.5943 | Perm 0.0002 | Ent 0.6469 | Len 0.7997 | Match 0.5171 | EM(pos, Hungarian) 0.688\n",
      "Validation | EM(pos,Hun) 0.734 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 2.747\n",
      "Epoch 60 | PosCE 0.5758 | Perm 0.0002 | Ent 0.6274 | Len 0.7981 | Match 0.5038 | EM(pos, Hungarian) 0.719\n",
      "Validation | EM(pos,Hun) 0.734 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 2.353\n",
      "Epoch 61 | PosCE 0.5575 | Perm 0.0002 | Ent 0.6077 | Len 0.7967 | Match 0.4900 | EM(pos, Hungarian) 0.734\n",
      "Validation | EM(pos,Hun) 0.734 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 2.353\n",
      "Epoch 62 | PosCE 0.5397 | Perm 0.0002 | Ent 0.5879 | Len 0.7949 | Match 0.4757 | EM(pos, Hungarian) 0.734\n",
      "Validation | EM(pos,Hun) 0.734 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 2.353\n",
      "Epoch 63 | PosCE 0.5216 | Perm 0.0002 | Ent 0.5697 | Len 0.7931 | Match 0.4636 | EM(pos, Hungarian) 0.766\n",
      "Validation | EM(pos,Hun) 0.766 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 2.017\n",
      "Epoch 64 | PosCE 0.5041 | Perm 0.0002 | Ent 0.5515 | Len 0.7914 | Match 0.4506 | EM(pos, Hungarian) 0.781\n",
      "Validation | EM(pos,Hun) 0.766 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 2.017\n",
      "Epoch 65 | PosCE 0.4876 | Perm 0.0002 | Ent 0.5347 | Len 0.7900 | Match 0.4386 | EM(pos, Hungarian) 0.781\n",
      "Validation | EM(pos,Hun) 0.797 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 1.908\n",
      "Epoch 66 | PosCE 0.4700 | Perm 0.0002 | Ent 0.5161 | Len 0.7882 | Match 0.4249 | EM(pos, Hungarian) 0.812\n",
      "Validation | EM(pos,Hun) 0.781 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 1.961\n",
      "Epoch 67 | PosCE 0.4540 | Perm 0.0002 | Ent 0.4995 | Len 0.7867 | Match 0.4128 | EM(pos, Hungarian) 0.812\n",
      "Validation | EM(pos,Hun) 0.812 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 1.740\n",
      "Stopping early: training EM(pos,Hun) reached 0.80\n"
     ]
    }
   ],
   "source": [
    "# Build a tiny train/val on 6 cities with exact labels\n",
    "cfg_overfit = TSPConfig(n_cities=6, grid_size=16, solver=\"held_karp\", seed=42)\n",
    "train_small, val_small = get_tsp_loaders(n_train=64, n_val=64, batch_size=16, cfg=cfg_overfit)\n",
    "\n",
    "# Rebuild dims and model (output vocab and seq_len changed!)\n",
    "SEQ_LEN = cfg_overfit.n_cities\n",
    "INPUT_TOKENS = cfg_overfit.grid_size * cfg_overfit.grid_size\n",
    "OUTPUT_TOKENS = cfg_overfit.n_cities\n",
    "\n",
    "cfg_trm_small = TRMConfig(\n",
    "    input_vocab_size=INPUT_TOKENS,\n",
    "    output_vocab_size=OUTPUT_TOKENS,\n",
    "    seq_len=SEQ_LEN,\n",
    "    d_model=128,\n",
    "    n_layers=2,\n",
    "    use_attention=True,\n",
    "    n_heads=4,\n",
    "    dropout=0.0,\n",
    "    mlp_ratio=4.0,\n",
    "    token_mlp_ratio=2.0,\n",
    "    n=6, T=3, k_last_ops=6,\n",
    "    stabilize_input_sums=True,\n",
    "    use_order_assignment=True   # switched on\n",
    ")\n",
    "\n",
    "model = TRM(cfg_trm_small).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.0, betas=(0.9, 0.95))\n",
    "scaler = make_grad_scaler(device.type == \"cuda\")\n",
    "ema = EMA(model, decay=0.999)\n",
    "\n",
    "init_coord_embedding_(model, grid_size=cfg_overfit.grid_size, d_model=cfg_trm_small.d_model, seed=123)\n",
    "\n",
    "EPOCHS = 100\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_one_epoch(model, train_small, optimizer, scaler, epoch,\n",
    "                    use_amp=True, ema=ema, grid_size=cfg_overfit.grid_size)\n",
    "\n",
    "    em_pos, cov_raw, val_raw, gap_raw = evaluate_order(model, train_small, n_sup_eval=N_SUP)\n",
    "    if em_pos >= 0.80:\n",
    "        print(\"Stopping early: training EM(pos,Hun) reached 0.80\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | PosCE 2.1100 | Perm 0.0000 | Ent 1.9102 | Len 0.8550 | Match 1.7022 | EM(pos, Hungarian) 0.001\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.279\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 25.272\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.279\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 25.272\n",
      "Epoch 02 | PosCE 2.1327 | Perm 0.0000 | Ent 1.7995 | Len 0.8326 | Match 1.5511 | EM(pos, Hungarian) 0.001\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.027\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 24.151\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.027\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 24.151\n",
      "Epoch 03 | PosCE 2.1378 | Perm 0.0000 | Ent 1.7652 | Len 0.8271 | Match 1.5112 | EM(pos, Hungarian) 0.000\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.995\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 24.247\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.995\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 24.247\n",
      "Epoch 04 | PosCE 2.1386 | Perm 0.0000 | Ent 1.7549 | Len 0.8297 | Match 1.4952 | EM(pos, Hungarian) 0.000\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.130\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 23.906\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.130\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 23.906\n",
      "Epoch 05 | PosCE 2.1415 | Perm 0.0000 | Ent 1.7411 | Len 0.8290 | Match 1.4784 | EM(pos, Hungarian) 0.000\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.141\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 23.717\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.141\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 23.717\n",
      "Epoch 06 | PosCE 2.1379 | Perm 0.0000 | Ent 1.7210 | Len 0.8283 | Match 1.4578 | EM(pos, Hungarian) 0.000\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.998\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 22.990\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.998\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 22.990\n",
      "Epoch 07 | PosCE 2.1353 | Perm 0.0000 | Ent 1.7063 | Len 0.8285 | Match 1.4415 | EM(pos, Hungarian) 0.001\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.923\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 22.050\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.923\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 22.050\n",
      "Epoch 08 | PosCE 2.1342 | Perm 0.0000 | Ent 1.6899 | Len 0.8274 | Match 1.4259 | EM(pos, Hungarian) 0.001\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.720\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 21.304\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.720\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 21.304\n",
      "Epoch 09 | PosCE 2.1322 | Perm 0.0000 | Ent 1.6797 | Len 0.8295 | Match 1.4146 | EM(pos, Hungarian) 0.000\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.765\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.533\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.765\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.533\n",
      "Epoch 10 | PosCE 2.1309 | Perm 0.0000 | Ent 1.6725 | Len 0.8296 | Match 1.4074 | EM(pos, Hungarian) 0.002\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.549\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.716\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.549\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.716\n",
      "Epoch 11 | PosCE 2.1342 | Perm 0.0000 | Ent 1.6582 | Len 0.8306 | Match 1.3943 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.776\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.246\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.776\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.246\n",
      "Epoch 12 | PosCE 2.1301 | Perm 0.0000 | Ent 1.6466 | Len 0.8300 | Match 1.3845 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.790\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.508\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.790\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.508\n",
      "Epoch 13 | PosCE 2.1315 | Perm 0.0000 | Ent 1.6376 | Len 0.8291 | Match 1.3769 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.682\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.885\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.682\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.885\n",
      "Epoch 14 | PosCE 2.1314 | Perm 0.0000 | Ent 1.6380 | Len 0.8331 | Match 1.3754 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.564\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.838\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.564\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.838\n",
      "Epoch 15 | PosCE 2.1303 | Perm 0.0000 | Ent 1.6231 | Len 0.8292 | Match 1.3638 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.170\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.773\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.170\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.773\n",
      "Epoch 16 | PosCE 2.1311 | Perm 0.0000 | Ent 1.6209 | Len 0.8282 | Match 1.3617 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.189\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.745\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.189\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.745\n",
      "Epoch 17 | PosCE 2.1281 | Perm 0.0000 | Ent 1.6147 | Len 0.8279 | Match 1.3568 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.911\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.519\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.911\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.519\n",
      "Epoch 18 | PosCE 2.1323 | Perm 0.0000 | Ent 1.6117 | Len 0.8311 | Match 1.3527 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.109\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.354\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.109\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.354\n",
      "Epoch 19 | PosCE 2.1291 | Perm 0.0000 | Ent 1.6040 | Len 0.8309 | Match 1.3461 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.817\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.250\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.817\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.250\n",
      "Epoch 20 | PosCE 2.1303 | Perm 0.0000 | Ent 1.5956 | Len 0.8280 | Match 1.3395 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.097\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.118\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.097\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.118\n",
      "Epoch 21 | PosCE 2.1038 | Perm 0.0000 | Ent 1.7873 | Len 0.8510 | Match 1.4174 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.796\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.212\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.796\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.212\n",
      "Epoch 22 | PosCE 2.0961 | Perm 0.0000 | Ent 1.8095 | Len 0.8483 | Match 1.4413 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.414\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.961\n",
      "Validation (raw) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.414\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.961\n",
      "Epoch 23 | PosCE 2.0909 | Perm 0.0000 | Ent 1.8129 | Len 0.8478 | Match 1.4458 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.909\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.012\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.909\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.012\n",
      "Epoch 24 | PosCE 2.0918 | Perm 0.0000 | Ent 1.8169 | Len 0.8488 | Match 1.4493 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.241\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.204\n",
      "Validation (raw) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.241\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.204\n",
      "Epoch 25 | PosCE 2.0879 | Perm 0.0000 | Ent 1.8067 | Len 0.8465 | Match 1.4383 | EM(pos, Hungarian) 0.002\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.037\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.100\n",
      "Validation (raw) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.037\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.100\n",
      "Epoch 26 | PosCE 2.0915 | Perm 0.0000 | Ent 1.8161 | Len 0.8493 | Match 1.4480 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.889\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.990\n",
      "Validation (raw) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.889\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.990\n",
      "Epoch 27 | PosCE 2.0907 | Perm 0.0000 | Ent 1.8110 | Len 0.8480 | Match 1.4426 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.050\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.028\n",
      "Validation (raw) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.050\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.028\n",
      "Epoch 28 | PosCE 2.0916 | Perm 0.0000 | Ent 1.8143 | Len 0.8513 | Match 1.4445 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.821\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.046\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.821\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.046\n",
      "Epoch 29 | PosCE 2.0893 | Perm 0.0000 | Ent 1.7998 | Len 0.8467 | Match 1.4293 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.048\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.891\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.048\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.891\n",
      "Epoch 30 | PosCE 2.0892 | Perm 0.0000 | Ent 1.8072 | Len 0.8475 | Match 1.4378 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.975\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.976\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.975\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.976\n",
      "Epoch 31 | PosCE 2.0882 | Perm 0.0000 | Ent 1.8045 | Len 0.8477 | Match 1.4343 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.835\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.958\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.835\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.958\n",
      "Epoch 32 | PosCE 2.0897 | Perm 0.0000 | Ent 1.8030 | Len 0.8475 | Match 1.4324 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.008\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.812\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.008\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.812\n",
      "Epoch 33 | PosCE 2.0896 | Perm 0.0000 | Ent 1.8061 | Len 0.8478 | Match 1.4365 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.156\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.780\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.156\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.780\n",
      "Epoch 34 | PosCE 2.0888 | Perm 0.0000 | Ent 1.8084 | Len 0.8478 | Match 1.4398 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.020\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.924\n",
      "Validation (raw) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.020\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.924\n",
      "Epoch 35 | PosCE 2.0898 | Perm 0.0000 | Ent 1.8096 | Len 0.8501 | Match 1.4397 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.994\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.031\n",
      "Validation (raw) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.994\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.031\n",
      "Epoch 36 | PosCE 2.0876 | Perm 0.0000 | Ent 1.8038 | Len 0.8474 | Match 1.4341 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.892\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.771\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.892\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.771\n",
      "Epoch 37 | PosCE 2.0876 | Perm 0.0000 | Ent 1.8023 | Len 0.8487 | Match 1.4313 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.819\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.863\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.819\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.863\n",
      "Epoch 38 | PosCE 2.0855 | Perm 0.0000 | Ent 1.8007 | Len 0.8472 | Match 1.4298 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.010 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.766\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.813\n",
      "Validation (raw) | EM(pos,Hun) 0.010 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.766\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.813\n",
      "Epoch 39 | PosCE 2.0879 | Perm 0.0000 | Ent 1.8032 | Len 0.8491 | Match 1.4316 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.922\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.863\n",
      "Validation (raw) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.922\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.863\n",
      "Epoch 40 | PosCE 2.0897 | Perm 0.0000 | Ent 1.7986 | Len 0.8497 | Match 1.4259 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.135\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.927\n",
      "Validation (raw) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.135\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.927\n",
      "Epoch 41 | PosCE 2.0870 | Perm 0.0000 | Ent 1.8003 | Len 0.8488 | Match 1.4281 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.010 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.961\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.977\n",
      "Validation (raw) | EM(pos,Hun) 0.010 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.961\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.977\n",
      "Epoch 42 | PosCE 2.0889 | Perm 0.0000 | Ent 1.8049 | Len 0.8512 | Match 1.4327 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.786\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.887\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.786\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.887\n",
      "Epoch 43 | PosCE 2.0877 | Perm 0.0000 | Ent 1.8016 | Len 0.8485 | Match 1.4292 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.263\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.792\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.263\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.792\n",
      "Epoch 44 | PosCE 2.0882 | Perm 0.0000 | Ent 1.8042 | Len 0.8514 | Match 1.4320 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.330\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.964\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.330\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.964\n",
      "Epoch 45 | PosCE 2.0885 | Perm 0.0000 | Ent 1.8003 | Len 0.8503 | Match 1.4272 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.315\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.816\n",
      "Validation (raw) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.315\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.816\n",
      "Epoch 46 | PosCE 2.0859 | Perm 0.0000 | Ent 1.8000 | Len 0.8498 | Match 1.4272 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.387\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.770\n",
      "Validation (raw) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.387\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.770\n",
      "Epoch 47 | PosCE 2.0864 | Perm 0.0000 | Ent 1.7984 | Len 0.8495 | Match 1.4246 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.010 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.034\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.836\n",
      "Validation (raw) | EM(pos,Hun) 0.010 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.034\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.836\n",
      "Epoch 48 | PosCE 2.0876 | Perm 0.0000 | Ent 1.7967 | Len 0.8494 | Match 1.4232 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.282\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.691\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.282\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.691\n",
      "Epoch 49 | PosCE 2.0846 | Perm 0.0000 | Ent 1.7941 | Len 0.8478 | Match 1.4214 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.012 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.796\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.002\n",
      "Validation (raw) | EM(pos,Hun) 0.012 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.796\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.002\n",
      "Epoch 50 | PosCE 2.0867 | Perm 0.0000 | Ent 1.7998 | Len 0.8489 | Match 1.4274 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.831\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.951\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.831\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.951\n",
      "Epoch 51 | PosCE 2.0854 | Perm 0.0000 | Ent 1.7998 | Len 0.8502 | Match 1.4265 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.029\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.961\n",
      "Validation (raw) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.029\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.961\n",
      "Epoch 52 | PosCE 2.0871 | Perm 0.0000 | Ent 1.7995 | Len 0.8492 | Match 1.4271 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.265\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.029\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.265\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.029\n",
      "Epoch 53 | PosCE 2.0849 | Perm 0.0000 | Ent 1.7962 | Len 0.8503 | Match 1.4223 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.387\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.076\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.387\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.076\n",
      "Epoch 54 | PosCE 2.0841 | Perm 0.0000 | Ent 1.7993 | Len 0.8509 | Match 1.4252 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.396\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.123\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.396\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.123\n",
      "Epoch 55 | PosCE 2.0863 | Perm 0.0000 | Ent 1.7939 | Len 0.8522 | Match 1.4188 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.451\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.070\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.451\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.070\n",
      "Epoch 56 | PosCE 2.0841 | Perm 0.0000 | Ent 1.7933 | Len 0.8515 | Match 1.4182 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.403\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.116\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.403\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.116\n",
      "Epoch 57 | PosCE 2.0831 | Perm 0.0000 | Ent 1.7923 | Len 0.8506 | Match 1.4167 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.105\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.186\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.105\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.186\n",
      "Epoch 58 | PosCE 2.0838 | Perm 0.0000 | Ent 1.7902 | Len 0.8501 | Match 1.4150 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.435\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.211\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.435\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.211\n",
      "Epoch 59 | PosCE 2.0849 | Perm 0.0000 | Ent 1.7906 | Len 0.8512 | Match 1.4161 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.126\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.247\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.126\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.247\n",
      "Epoch 60 | PosCE 2.0827 | Perm 0.0000 | Ent 1.7896 | Len 0.8494 | Match 1.4155 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.035\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.249\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.035\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.249\n",
      "Epoch 61 | PosCE 2.0807 | Perm 0.0000 | Ent 1.7898 | Len 0.8482 | Match 1.4164 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.175\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.273\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.175\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.273\n",
      "Epoch 62 | PosCE 2.0813 | Perm 0.0000 | Ent 1.7833 | Len 0.8468 | Match 1.4101 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.175\n",
      "Validation | EM(pos,Hun) 0.008 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.050\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.175\n",
      "Validation (EMA) | EM(pos,Hun) 0.008 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.050\n",
      "Epoch 63 | PosCE 2.0812 | Perm 0.0000 | Ent 1.7873 | Len 0.8488 | Match 1.4140 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.105\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.134\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.105\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.134\n",
      "Epoch 64 | PosCE 2.0819 | Perm 0.0000 | Ent 1.7936 | Len 0.8502 | Match 1.4203 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.271\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.088\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.271\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.088\n",
      "Epoch 65 | PosCE 2.0798 | Perm 0.0000 | Ent 1.7914 | Len 0.8511 | Match 1.4185 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.435\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.178\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.435\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.178\n",
      "Epoch 66 | PosCE 2.0807 | Perm 0.0000 | Ent 1.7835 | Len 0.8489 | Match 1.4101 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.163\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.209\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.163\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.209\n",
      "Epoch 67 | PosCE 2.0779 | Perm 0.0000 | Ent 1.7853 | Len 0.8483 | Match 1.4129 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.277\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.203\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.277\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.203\n",
      "Epoch 68 | PosCE 2.0815 | Perm 0.0000 | Ent 1.7853 | Len 0.8501 | Match 1.4118 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.652\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.990\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.652\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.990\n",
      "Epoch 69 | PosCE 2.0778 | Perm 0.0000 | Ent 1.7785 | Len 0.8468 | Match 1.4059 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.439\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.082\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.439\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.082\n",
      "Epoch 70 | PosCE 2.0787 | Perm 0.0000 | Ent 1.7873 | Len 0.8500 | Match 1.4146 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.269\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.108\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.269\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.108\n",
      "Epoch 71 | PosCE 2.0806 | Perm 0.0000 | Ent 1.7899 | Len 0.8511 | Match 1.4172 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.244\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.186\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.244\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.186\n",
      "Epoch 72 | PosCE 2.0772 | Perm 0.0000 | Ent 1.7866 | Len 0.8501 | Match 1.4141 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.566\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.141\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.566\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.141\n",
      "Epoch 73 | PosCE 2.0784 | Perm 0.0000 | Ent 1.7849 | Len 0.8501 | Match 1.4124 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.580\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.185\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.580\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.185\n",
      "Epoch 74 | PosCE 2.0771 | Perm 0.0000 | Ent 1.7861 | Len 0.8491 | Match 1.4147 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.550\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.267\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.550\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.267\n",
      "Epoch 75 | PosCE 2.0775 | Perm 0.0000 | Ent 1.7822 | Len 0.8495 | Match 1.4099 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.266\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.309\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.266\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.309\n",
      "Epoch 76 | PosCE 2.0760 | Perm 0.0000 | Ent 1.7840 | Len 0.8512 | Match 1.4115 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.485\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.306\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.485\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.306\n",
      "Epoch 77 | PosCE 2.0761 | Perm 0.0000 | Ent 1.7807 | Len 0.8510 | Match 1.4083 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.443\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.296\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.443\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.296\n",
      "Epoch 78 | PosCE 2.0766 | Perm 0.0000 | Ent 1.7819 | Len 0.8520 | Match 1.4094 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.332\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.214\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.332\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.214\n",
      "Epoch 79 | PosCE 2.0753 | Perm 0.0000 | Ent 1.7793 | Len 0.8494 | Match 1.4078 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.489\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.273\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.489\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.273\n",
      "Epoch 80 | PosCE 2.0745 | Perm 0.0000 | Ent 1.7772 | Len 0.8489 | Match 1.4064 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.599\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.366\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.599\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.366\n",
      "Epoch 81 | PosCE 2.0674 | Perm 0.0000 | Ent 1.8614 | Len 0.8570 | Match 1.4275 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.449\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.440\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.449\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.440\n",
      "Epoch 82 | PosCE 2.0646 | Perm 0.0000 | Ent 1.8752 | Len 0.8579 | Match 1.4479 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.609\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.512\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.609\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.512\n",
      "Epoch 83 | PosCE 2.0634 | Perm 0.0000 | Ent 1.8730 | Len 0.8581 | Match 1.4443 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.357\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.447\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.357\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.447\n",
      "Epoch 84 | PosCE 2.0636 | Perm 0.0000 | Ent 1.8692 | Len 0.8564 | Match 1.4405 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.808\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.592\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.808\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.592\n",
      "Epoch 85 | PosCE 2.0608 | Perm 0.0000 | Ent 1.8741 | Len 0.8584 | Match 1.4473 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.355\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.550\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.355\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.550\n",
      "Epoch 86 | PosCE 2.0601 | Perm 0.0000 | Ent 1.8713 | Len 0.8572 | Match 1.4444 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.922\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.581\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.922\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.581\n",
      "Epoch 87 | PosCE 2.0598 | Perm 0.0000 | Ent 1.8718 | Len 0.8562 | Match 1.4455 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.373\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.683\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.373\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.683\n",
      "Epoch 88 | PosCE 2.0621 | Perm 0.0000 | Ent 1.8725 | Len 0.8577 | Match 1.4462 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.263\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.734\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.263\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.734\n",
      "Epoch 89 | PosCE 2.0576 | Perm 0.0000 | Ent 1.8655 | Len 0.8558 | Match 1.4375 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.042\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.652\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.042\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.652\n",
      "Epoch 90 | PosCE 2.0613 | Perm 0.0000 | Ent 1.8737 | Len 0.8585 | Match 1.4489 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.588\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.633\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.588\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.633\n",
      "Epoch 91 | PosCE 2.0591 | Perm 0.0000 | Ent 1.8753 | Len 0.8595 | Match 1.4509 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.295\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.490\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.295\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.490\n",
      "Epoch 92 | PosCE 2.0588 | Perm 0.0000 | Ent 1.8695 | Len 0.8579 | Match 1.4439 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.277\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.292\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.277\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.292\n",
      "Epoch 93 | PosCE 2.0601 | Perm 0.0000 | Ent 1.8704 | Len 0.8583 | Match 1.4453 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.275\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.324\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.275\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.324\n",
      "Epoch 94 | PosCE 2.0596 | Perm 0.0000 | Ent 1.8697 | Len 0.8575 | Match 1.4461 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.382\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.195\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.382\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.195\n",
      "Epoch 95 | PosCE 2.0546 | Perm 0.0000 | Ent 1.8725 | Len 0.8584 | Match 1.4503 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.894\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.329\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.894\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.329\n",
      "Epoch 96 | PosCE 2.0572 | Perm 0.0000 | Ent 1.8642 | Len 0.8563 | Match 1.4396 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.376\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.255\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.376\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.255\n",
      "Epoch 97 | PosCE 2.0582 | Perm 0.0000 | Ent 1.8706 | Len 0.8592 | Match 1.4483 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.500\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.360\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.500\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.360\n",
      "Epoch 98 | PosCE 2.0538 | Perm 0.0000 | Ent 1.8667 | Len 0.8569 | Match 1.4440 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.232\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.311\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.232\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.311\n",
      "Epoch 99 | PosCE 2.0537 | Perm 0.0000 | Ent 1.8672 | Len 0.8575 | Match 1.4450 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.546\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.378\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.546\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.378\n",
      "Epoch 100 | PosCE 2.0539 | Perm 0.0000 | Ent 1.8621 | Len 0.8562 | Match 1.4377 | EM(pos, Hungarian) 0.008\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.431\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.288\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.431\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.288\n",
      "Epoch 101 | PosCE 2.0543 | Perm 0.0000 | Ent 1.8693 | Len 0.8586 | Match 1.4488 | EM(pos, Hungarian) 0.008\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.524\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.259\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.524\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.259\n",
      "Epoch 102 | PosCE 2.0518 | Perm 0.0000 | Ent 1.8659 | Len 0.8581 | Match 1.4450 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.560\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.341\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.560\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.341\n",
      "Epoch 103 | PosCE 2.0519 | Perm 0.0000 | Ent 1.8576 | Len 0.8562 | Match 1.4335 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.646\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.333\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.646\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.333\n",
      "Epoch 104 | PosCE 2.0515 | Perm 0.0000 | Ent 1.8703 | Len 0.8602 | Match 1.4503 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.396\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.183\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.396\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.183\n",
      "Epoch 105 | PosCE 2.0511 | Perm 0.0000 | Ent 1.8597 | Len 0.8576 | Match 1.4368 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 16.878\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.183\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 16.878\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.183\n",
      "Epoch 106 | PosCE 2.0518 | Perm 0.0000 | Ent 1.8615 | Len 0.8573 | Match 1.4400 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.523\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.377\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.523\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.377\n",
      "Epoch 107 | PosCE 2.0497 | Perm 0.0000 | Ent 1.8633 | Len 0.8580 | Match 1.4429 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.815\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.423\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.815\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.423\n",
      "Epoch 108 | PosCE 2.0504 | Perm 0.0000 | Ent 1.8619 | Len 0.8588 | Match 1.4408 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.434\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.367\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.434\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.367\n",
      "Epoch 109 | PosCE 2.0489 | Perm 0.0000 | Ent 1.8619 | Len 0.8587 | Match 1.4417 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.805\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.591\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.805\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.591\n",
      "Epoch 110 | PosCE 2.0496 | Perm 0.0000 | Ent 1.8564 | Len 0.8581 | Match 1.4345 | EM(pos, Hungarian) 0.008\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.961\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.514\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.961\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.514\n",
      "Epoch 111 | PosCE 2.0478 | Perm 0.0000 | Ent 1.8616 | Len 0.8591 | Match 1.4416 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.759\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.688\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.759\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.688\n",
      "Epoch 112 | PosCE 2.0473 | Perm 0.0000 | Ent 1.8561 | Len 0.8581 | Match 1.4350 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.591\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.595\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.591\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.595\n",
      "Epoch 113 | PosCE 2.0462 | Perm 0.0000 | Ent 1.8546 | Len 0.8567 | Match 1.4339 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.694\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.518\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.694\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.518\n",
      "Epoch 114 | PosCE 2.0469 | Perm 0.0000 | Ent 1.8555 | Len 0.8579 | Match 1.4346 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.761\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.610\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.761\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.610\n",
      "Epoch 115 | PosCE 2.0430 | Perm 0.0000 | Ent 1.8559 | Len 0.8586 | Match 1.4352 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.175\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.640\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.175\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.640\n",
      "Epoch 116 | PosCE 2.0462 | Perm 0.0000 | Ent 1.8556 | Len 0.8588 | Match 1.4353 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.398\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.682\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.398\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.682\n",
      "Epoch 117 | PosCE 2.0434 | Perm 0.0000 | Ent 1.8481 | Len 0.8566 | Match 1.4264 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.280\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.604\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.280\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.604\n",
      "Epoch 118 | PosCE 2.0449 | Perm 0.0000 | Ent 1.8505 | Len 0.8565 | Match 1.4306 | EM(pos, Hungarian) 0.008\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.812\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.595\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.812\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.595\n",
      "Epoch 119 | PosCE 2.0418 | Perm 0.0000 | Ent 1.8510 | Len 0.8576 | Match 1.4326 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.573\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.714\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.573\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.714\n",
      "Epoch 120 | PosCE 2.0406 | Perm 0.0000 | Ent 1.8492 | Len 0.8557 | Match 1.4307 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.626\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.673\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.626\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.673\n",
      "Epoch 121 | PosCE 2.0372 | Perm 0.0000 | Ent 1.8842 | Len 0.8621 | Match 1.4456 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.033\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.836\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.033\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.836\n",
      "Epoch 122 | PosCE 2.0356 | Perm 0.0000 | Ent 1.8966 | Len 0.8632 | Match 1.4663 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.052\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.958\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.052\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.958\n",
      "Epoch 123 | PosCE 2.0331 | Perm 0.0000 | Ent 1.8933 | Len 0.8628 | Match 1.4613 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.278\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.061\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.278\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.061\n",
      "Epoch 124 | PosCE 2.0338 | Perm 0.0000 | Ent 1.8988 | Len 0.8643 | Match 1.4711 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.186\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.111\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.186\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.111\n",
      "Epoch 125 | PosCE 2.0333 | Perm 0.0000 | Ent 1.8958 | Len 0.8639 | Match 1.4677 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.462\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.018\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.462\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.018\n",
      "Epoch 126 | PosCE 2.0313 | Perm 0.0000 | Ent 1.8934 | Len 0.8635 | Match 1.4633 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.081\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.026\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.081\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.026\n",
      "Epoch 127 | PosCE 2.0297 | Perm 0.0000 | Ent 1.8947 | Len 0.8635 | Match 1.4660 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.138\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.152\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.138\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.152\n",
      "Epoch 128 | PosCE 2.0294 | Perm 0.0000 | Ent 1.8913 | Len 0.8621 | Match 1.4614 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.961\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.196\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.961\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.196\n",
      "Epoch 129 | PosCE 2.0292 | Perm 0.0000 | Ent 1.8912 | Len 0.8622 | Match 1.4621 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.257\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.152\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.257\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.152\n",
      "Epoch 130 | PosCE 2.0275 | Perm 0.0000 | Ent 1.8895 | Len 0.8620 | Match 1.4597 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.538\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.190\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.538\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.190\n",
      "Epoch 131 | PosCE 2.0274 | Perm 0.0000 | Ent 1.8893 | Len 0.8628 | Match 1.4590 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.347\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.371\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.347\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.371\n",
      "Epoch 132 | PosCE 2.0293 | Perm 0.0000 | Ent 1.8918 | Len 0.8637 | Match 1.4646 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.339\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.522\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.339\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.522\n",
      "Epoch 133 | PosCE 2.0288 | Perm 0.0000 | Ent 1.8924 | Len 0.8641 | Match 1.4670 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.575\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.535\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.575\n",
      "Validation (EMA) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.535\n",
      "Epoch 134 | PosCE 2.0273 | Perm 0.0000 | Ent 1.8930 | Len 0.8656 | Match 1.4696 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.167\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.561\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.167\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.561\n",
      "Epoch 135 | PosCE 2.0277 | Perm 0.0000 | Ent 1.8919 | Len 0.8647 | Match 1.4657 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.481\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.522\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.481\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.522\n",
      "Epoch 136 | PosCE 2.0281 | Perm 0.0000 | Ent 1.8876 | Len 0.8637 | Match 1.4597 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.428\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.458\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.428\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.458\n",
      "Epoch 137 | PosCE 2.0251 | Perm 0.0000 | Ent 1.8929 | Len 0.8648 | Match 1.4694 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.485\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.479\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.485\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.479\n",
      "Epoch 138 | PosCE 2.0258 | Perm 0.0000 | Ent 1.8917 | Len 0.8657 | Match 1.4676 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.199\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.617\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.199\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.617\n",
      "Epoch 139 | PosCE 2.0247 | Perm 0.0000 | Ent 1.8902 | Len 0.8659 | Match 1.4645 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.777\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.481\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.777\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.481\n",
      "Epoch 140 | PosCE 2.0226 | Perm 0.0000 | Ent 1.8857 | Len 0.8638 | Match 1.4592 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.411\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.561\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.411\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.561\n",
      "Epoch 141 | PosCE 2.0242 | Perm 0.0000 | Ent 1.8883 | Len 0.8654 | Match 1.4639 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.408\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.614\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.408\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.614\n",
      "Epoch 142 | PosCE 2.0218 | Perm 0.0000 | Ent 1.8818 | Len 0.8620 | Match 1.4533 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.852\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.585\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.852\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.585\n",
      "Epoch 143 | PosCE 2.0215 | Perm 0.0000 | Ent 1.8862 | Len 0.8645 | Match 1.4624 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.361\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.571\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.361\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.571\n",
      "Epoch 144 | PosCE 2.0193 | Perm 0.0000 | Ent 1.8819 | Len 0.8619 | Match 1.4552 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 17.909\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.297\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 17.909\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.297\n",
      "Epoch 145 | PosCE 2.0214 | Perm 0.0000 | Ent 1.8832 | Len 0.8632 | Match 1.4585 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.842\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.280\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.842\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.280\n",
      "Epoch 146 | PosCE 2.0192 | Perm 0.0000 | Ent 1.8849 | Len 0.8642 | Match 1.4622 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.337\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.232\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.337\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.232\n",
      "Epoch 147 | PosCE 2.0197 | Perm 0.0000 | Ent 1.8816 | Len 0.8632 | Match 1.4564 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.858\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.364\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.858\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.364\n",
      "Epoch 148 | PosCE 2.0201 | Perm 0.0000 | Ent 1.8820 | Len 0.8640 | Match 1.4572 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.742\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.338\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.742\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.338\n",
      "Epoch 149 | PosCE 2.0184 | Perm 0.0000 | Ent 1.8826 | Len 0.8640 | Match 1.4591 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.747\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.254\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.747\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.254\n",
      "Epoch 150 | PosCE 2.0185 | Perm 0.0000 | Ent 1.8833 | Len 0.8644 | Match 1.4597 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.484\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.216\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.484\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.216\n",
      "Epoch 151 | PosCE 2.0163 | Perm 0.0000 | Ent 1.8812 | Len 0.8647 | Match 1.4574 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.212\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.299\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.212\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.299\n",
      "Epoch 152 | PosCE 2.0184 | Perm 0.0000 | Ent 1.8800 | Len 0.8641 | Match 1.4550 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.324\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.429\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.324\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.429\n",
      "Epoch 153 | PosCE 2.0168 | Perm 0.0000 | Ent 1.8787 | Len 0.8637 | Match 1.4532 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.697\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.340\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.697\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.340\n",
      "Epoch 154 | PosCE 2.0172 | Perm 0.0000 | Ent 1.8807 | Len 0.8654 | Match 1.4586 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.485\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.385\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.485\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.385\n",
      "Epoch 155 | PosCE 2.0154 | Perm 0.0000 | Ent 1.8778 | Len 0.8638 | Match 1.4522 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.744\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.329\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.744\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.329\n",
      "Epoch 156 | PosCE 2.0150 | Perm 0.0000 | Ent 1.8797 | Len 0.8643 | Match 1.4574 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.656\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.415\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.656\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.415\n",
      "Epoch 157 | PosCE 2.0147 | Perm 0.0000 | Ent 1.8765 | Len 0.8646 | Match 1.4515 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.346\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.515\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.346\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.515\n",
      "Epoch 158 | PosCE 2.0145 | Perm 0.0000 | Ent 1.8791 | Len 0.8652 | Match 1.4566 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.086\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.606\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.086\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.606\n",
      "Epoch 159 | PosCE 2.0140 | Perm 0.0000 | Ent 1.8776 | Len 0.8645 | Match 1.4542 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.694\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.560\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.694\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.560\n",
      "Epoch 160 | PosCE 2.0129 | Perm 0.0000 | Ent 1.8757 | Len 0.8646 | Match 1.4516 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.746\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.718\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.746\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.718\n",
      "Epoch 161 | PosCE 2.0137 | Perm 0.0000 | Ent 1.8744 | Len 0.8645 | Match 1.4495 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.991\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.789\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.991\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.789\n",
      "Epoch 162 | PosCE 2.0108 | Perm 0.0000 | Ent 1.8745 | Len 0.8642 | Match 1.4499 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.652\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.839\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.652\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.839\n",
      "Epoch 163 | PosCE 2.0125 | Perm 0.0000 | Ent 1.8752 | Len 0.8655 | Match 1.4514 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.043\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.863\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.043\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.863\n",
      "Epoch 164 | PosCE 2.0126 | Perm 0.0000 | Ent 1.8747 | Len 0.8654 | Match 1.4502 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.782\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.950\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.782\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.950\n",
      "Epoch 165 | PosCE 2.0130 | Perm 0.0000 | Ent 1.8719 | Len 0.8646 | Match 1.4464 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.650\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.046\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.650\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.046\n",
      "Epoch 166 | PosCE 2.0111 | Perm 0.0000 | Ent 1.8756 | Len 0.8667 | Match 1.4529 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.335\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.092\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.335\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.092\n",
      "Epoch 167 | PosCE 2.0104 | Perm 0.0000 | Ent 1.8744 | Len 0.8662 | Match 1.4506 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.169\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.002\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.169\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.002\n",
      "Epoch 168 | PosCE 2.0115 | Perm 0.0000 | Ent 1.8726 | Len 0.8660 | Match 1.4482 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.259\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 18.980\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.259\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 18.980\n",
      "Epoch 169 | PosCE 2.0114 | Perm 0.0000 | Ent 1.8725 | Len 0.8668 | Match 1.4483 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.664\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.103\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.664\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.103\n",
      "Epoch 170 | PosCE 2.0102 | Perm 0.0000 | Ent 1.8761 | Len 0.8675 | Match 1.4558 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.246\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.312\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.246\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.312\n",
      "Epoch 171 | PosCE 2.0074 | Perm 0.0000 | Ent 1.8748 | Len 0.8673 | Match 1.4527 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.521\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.384\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.521\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.384\n",
      "Epoch 172 | PosCE 2.0074 | Perm 0.0000 | Ent 1.8650 | Len 0.8647 | Match 1.4378 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.520\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.422\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.520\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.422\n",
      "Epoch 173 | PosCE 2.0075 | Perm 0.0000 | Ent 1.8664 | Len 0.8652 | Match 1.4395 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.766\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.352\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.766\n",
      "Validation (EMA) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.352\n",
      "Epoch 174 | PosCE 2.0070 | Perm 0.0000 | Ent 1.8673 | Len 0.8656 | Match 1.4412 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.337\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.456\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.337\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.456\n",
      "Epoch 175 | PosCE 2.0069 | Perm 0.0000 | Ent 1.8683 | Len 0.8662 | Match 1.4439 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.816\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.507\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.816\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.507\n",
      "Epoch 176 | PosCE 2.0074 | Perm 0.0000 | Ent 1.8683 | Len 0.8667 | Match 1.4442 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.734\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.390\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.734\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.390\n",
      "Epoch 177 | PosCE 2.0050 | Perm 0.0000 | Ent 1.8677 | Len 0.8673 | Match 1.4431 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.782\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.409\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.782\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.409\n",
      "Epoch 178 | PosCE 2.0071 | Perm 0.0000 | Ent 1.8690 | Len 0.8680 | Match 1.4452 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.089\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.417\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.089\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.417\n",
      "Epoch 179 | PosCE 2.0034 | Perm 0.0000 | Ent 1.8656 | Len 0.8668 | Match 1.4407 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.478\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.599\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.478\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.599\n",
      "Epoch 180 | PosCE 2.0060 | Perm 0.0000 | Ent 1.8673 | Len 0.8681 | Match 1.4432 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.049\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.842\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.049\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.842\n",
      "Epoch 181 | PosCE 2.0060 | Perm 0.0000 | Ent 1.8681 | Len 0.8690 | Match 1.4457 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.006 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.726\n",
      "Validation (raw) | EM(pos,Hun) 0.006 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.006\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.726\n",
      "Epoch 182 | PosCE 2.0010 | Perm 0.0000 | Ent 1.8640 | Len 0.8677 | Match 1.4387 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.844\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.748\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.844\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.748\n",
      "Epoch 183 | PosCE 2.0026 | Perm 0.0000 | Ent 1.8627 | Len 0.8665 | Match 1.4377 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.002\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 19.990\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.002\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 19.990\n",
      "Epoch 184 | PosCE 2.0017 | Perm 0.0000 | Ent 1.8657 | Len 0.8695 | Match 1.4429 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.248\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.016\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.248\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.016\n",
      "Epoch 185 | PosCE 2.0027 | Perm 0.0000 | Ent 1.8657 | Len 0.8698 | Match 1.4439 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.626\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.399\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.626\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.399\n",
      "Epoch 186 | PosCE 2.0023 | Perm 0.0000 | Ent 1.8606 | Len 0.8687 | Match 1.4356 | EM(pos, Hungarian) 0.007\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.419\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.330\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.419\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.330\n",
      "Epoch 187 | PosCE 2.0018 | Perm 0.0000 | Ent 1.8656 | Len 0.8697 | Match 1.4445 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.947\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.218\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.947\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.218\n",
      "Epoch 188 | PosCE 2.0012 | Perm 0.0000 | Ent 1.8635 | Len 0.8698 | Match 1.4417 | EM(pos, Hungarian) 0.006\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.737\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.224\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.737\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.224\n",
      "Epoch 189 | PosCE 1.9993 | Perm 0.0000 | Ent 1.8619 | Len 0.8691 | Match 1.4389 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.555\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.454\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.555\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.454\n",
      "Epoch 190 | PosCE 1.9995 | Perm 0.0000 | Ent 1.8630 | Len 0.8704 | Match 1.4417 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 21.061\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.649\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 21.061\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.649\n",
      "Epoch 191 | PosCE 1.9988 | Perm 0.0000 | Ent 1.8606 | Len 0.8704 | Match 1.4375 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 21.416\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.647\n",
      "Validation (raw) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 21.416\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.647\n",
      "Epoch 192 | PosCE 1.9980 | Perm 0.0000 | Ent 1.8571 | Len 0.8699 | Match 1.4322 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 21.137\n",
      "Validation | EM(pos,Hun) 0.004 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.648\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 21.137\n",
      "Validation (EMA) | EM(pos,Hun) 0.004 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.648\n",
      "Epoch 193 | PosCE 1.9977 | Perm 0.0000 | Ent 1.8613 | Len 0.8713 | Match 1.4398 | EM(pos, Hungarian) 0.005\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 21.727\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 20.829\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 21.727\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 20.829\n",
      "Epoch 194 | PosCE 1.9975 | Perm 0.0000 | Ent 1.8627 | Len 0.8732 | Match 1.4437 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 22.113\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 21.240\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 22.113\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 21.240\n",
      "Epoch 195 | PosCE 1.9950 | Perm 0.0000 | Ent 1.8611 | Len 0.8723 | Match 1.4416 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 21.751\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 21.238\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 21.751\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 21.238\n",
      "Epoch 196 | PosCE 1.9961 | Perm 0.0000 | Ent 1.8578 | Len 0.8725 | Match 1.4366 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 22.026\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 21.440\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 22.026\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 21.440\n",
      "Epoch 197 | PosCE 1.9944 | Perm 0.0000 | Ent 1.8575 | Len 0.8726 | Match 1.4373 | EM(pos, Hungarian) 0.003\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 22.332\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 21.499\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 22.332\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 21.499\n",
      "Epoch 198 | PosCE 1.9937 | Perm 0.0000 | Ent 1.8567 | Len 0.8736 | Match 1.4363 | EM(pos, Hungarian) 0.004\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 22.025\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 21.758\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 22.025\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 21.758\n",
      "Epoch 199 | PosCE 1.9940 | Perm 0.0000 | Ent 1.8580 | Len 0.8735 | Match 1.4391 | EM(pos, Hungarian) 0.002\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 22.416\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 21.748\n",
      "Validation (raw) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 22.416\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 21.748\n",
      "Epoch 200 | PosCE 1.9927 | Perm 0.0000 | Ent 1.8572 | Len 0.8742 | Match 1.4383 | EM(pos, Hungarian) 0.002\n",
      "Validation | EM(pos,Hun) 0.000 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 22.694\n",
      "Validation | EM(pos,Hun) 0.002 | Coverage 1.000 | ValidCycle 1.000 | AvgGap(valid) 21.950\n",
      "Validation (raw) | EM(pos,Hun) 0.000 | Cov 1.000 | Valid 1.000 | Gap(valid) 22.694\n",
      "Validation (EMA) | EM(pos,Hun) 0.002 | Cov 1.000 | Valid 1.000 | Gap(valid) 21.950\n"
     ]
    }
   ],
   "source": [
    "# 8-city exact data\n",
    "cfg_data = TSPConfig(n_cities=8, grid_size=16, solver=\"held_karp\", seed=123)\n",
    "train_loader, val_loader = get_tsp_loaders(n_train=4096, n_val=512, batch_size=16, cfg=cfg_data)\n",
    "\n",
    "SEQ_LEN = cfg_data.n_cities\n",
    "INPUT_TOKENS = cfg_data.grid_size * cfg_data.grid_size\n",
    "OUTPUT_TOKENS = cfg_data.n_cities\n",
    "\n",
    "D_MODEL = 192\n",
    "N_SUP   = 12\n",
    "N       = 6\n",
    "T       = 3\n",
    "K_LAST_OPS = None\n",
    "\n",
    "cfg_trm = TRMConfig(\n",
    "    input_vocab_size=INPUT_TOKENS,\n",
    "    output_vocab_size=OUTPUT_TOKENS,\n",
    "    seq_len=SEQ_LEN,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=2,\n",
    "    use_attention=True,\n",
    "    n_heads=6,                 # 192 / 6 = 32 per head\n",
    "    dropout=0.0,\n",
    "    mlp_ratio=4.0,\n",
    "    token_mlp_ratio=2.0,\n",
    "    n=N, T=T, k_last_ops=K_LAST_OPS,\n",
    "    stabilize_input_sums=True,  # use_pointer_output=True ,\n",
    "    use_order_assignment=True\n",
    ")\n",
    "model = TRM(cfg_trm).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.0, betas=(0.9, 0.95))\n",
    "scaler = make_grad_scaler(device.type == \"cuda\")\n",
    "ema = EMA(model, decay=0.999)\n",
    "\n",
    "init_coord_embedding_(model, grid_size=cfg_data.grid_size, d_model=cfg_trm.d_model, seed=123)\n",
    "# Optional: freeze for 1 epoch\n",
    "for p in model.input_emb.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "EPOCHS = 200\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    # --- structure schedule ---\n",
    "    if epoch <= 10:\n",
    "        LAMBDA_PERM, LAMBDA_2CYCLE, LAMBDA_PREVCE = 1.0, 0.30, 0.50\n",
    "    elif epoch <= 12:   # add a short plateau to keep pressure on 2-cycles\n",
    "        LAMBDA_PERM, LAMBDA_2CYCLE, LAMBDA_PREVCE = 0.80, 0.20, 0.40\n",
    "    elif epoch <= 20:\n",
    "        LAMBDA_PERM, LAMBDA_2CYCLE, LAMBDA_PREVCE = 0.50, 0.10, 0.30\n",
    "    else:\n",
    "        LAMBDA_PERM, LAMBDA_2CYCLE, LAMBDA_PREVCE = 0.20, 0.05, 0.20\n",
    "\n",
    "\n",
    "    # geometry prior schedule (dp)  keep it on longer, then taper\n",
    "    if epoch <= 5:        dp = 0.25\n",
    "    elif epoch <= 60:     dp = 0.12\n",
    "    elif epoch <= 80:     dp = 0.06\n",
    "    else:                 dp = 0.00\n",
    "\n",
    "    halt_w = 0.0 if epoch <= 3 else 1.0\n",
    "\n",
    "    train_one_epoch(\n",
    "        model, train_loader, optimizer, scaler, epoch,\n",
    "        use_amp=True, ema=ema, grid_size=cfg_data.grid_size,\n",
    "    )\n",
    "\n",
    "\n",
    "    if epoch == 1:\n",
    "        for p in model.input_emb.parameters():\n",
    "            p.requires_grad = True\n",
    "        # optional: keep momentum on other params and give a lower LR to the embedding\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            [\n",
    "                {\"params\": [p for n,p in model.named_parameters() if \"input_emb\" not in n], \"lr\": 3e-4},\n",
    "                {\"params\": model.input_emb.parameters(), \"lr\": 1e-4},\n",
    "            ],\n",
    "            weight_decay=0.0, betas=(0.9, 0.95)\n",
    "        )\n",
    "\n",
    "\n",
    "    em_pos, cov_raw, val_raw, gap_raw = evaluate_order(model, val_loader, n_sup_eval=N_SUP)\n",
    "    em_pos_ema, cov_ema, val_ema, gap_ema = evaluate_order_with_ema(model, ema, val_loader, n_sup_eval=N_SUP)\n",
    "\n",
    "    print(f\"Validation (raw) | EM(pos,Hun) {em_pos:.3f} | Cov {cov_raw:.3f} | Valid {val_raw:.3f} | Gap(valid) {gap_raw:.3f}\")\n",
    "    print(f\"Validation (EMA) | EM(pos,Hun) {em_pos_ema:.3f} | Cov {cov_ema:.3f} | Valid {val_ema:.3f} | Gap(valid) {gap_ema:.3f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | CE 2.1363 | HaltBCE 0.6783 | Exact-match 0.000\n",
      "Validation | EM 0.000 | Token 0.126 | Coverage 0.393 | ValidCycle 0.000 | AvgGap(valid) nan\n",
      "Validation | EM 0.000 | Token 0.121 | Coverage 0.366 | ValidCycle 0.000 | AvgGap(valid) nan\n",
      "Validation (raw) | EM 0.000 | Tok 0.126 | Cov 0.393 | Valid 0.000 | Gap(valid) nan\n",
      "Validation (EMA) | EM 0.000 | Tok 0.121 | Cov 0.366 | Valid 0.000 | Gap(valid) nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m      3\u001b[39m     halt_w = \u001b[32m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch <= \u001b[32m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalt_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhalt_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     em_raw, tok_raw, cover_raw, valid_raw, gap_raw = evaluate(model, val_loader, n_sup_eval=N_SUP)\n\u001b[32m      7\u001b[39m     em_ema, tok_ema, cover_ema, valid_ema, gap_ema = evaluate_with_ema(model, ema, val_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, scaler, epoch, use_amp, ema, halt_weight)\u001b[39m\n\u001b[32m     32\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m amp_autocast(is_cuda, use_amp):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     y_state, z_state, logits, halt_logit = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK_LAST_OPS\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     loss_ce = token_ce_loss(logits.float(), y_true)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:437\u001b[39m, in \u001b[36mTRM.forward_step\u001b[39m\u001b[34m(self, x_tokens, y, z, n, T, k_last_ops)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (z \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    436\u001b[39m     y, z = \u001b[38;5;28mself\u001b[39m.init_state(batch_size=x_tokens.size(\u001b[32m0\u001b[39m), device=x_tokens.device)\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeep_recursion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:397\u001b[39m, in \u001b[36mTRM.deep_recursion\u001b[39m\u001b[34m(self, x_h, y, z, n, T, k_last_ops)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, T - \u001b[32m1\u001b[39m)):\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m         y, z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlatent_recursion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_last_ops\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_grads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;66;03m# Final loop with gradients; optionally backprop only through the last k ops\u001b[39;00m\n\u001b[32m    400\u001b[39m y, z = \u001b[38;5;28mself\u001b[39m.latent_recursion(x_h, y, z, n=n, k_last_ops=k_last_ops, track_grads=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:359\u001b[39m, in \u001b[36mTRM.latent_recursion\u001b[39m\u001b[34m(self, x_h, y, z, n, k_last_ops, track_grads)\u001b[39m\n\u001b[32m    357\u001b[39m     ctx = nullcontext() \u001b[38;5;28;01mif\u001b[39;00m (track_grads \u001b[38;5;129;01mand\u001b[39;00m op_index >= cutoff) \u001b[38;5;28;01melse\u001b[39;00m torch.no_grad()\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     op_index += \u001b[32m1\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# once: y = net(y + z)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:331\u001b[39m, in \u001b[36mTRM._net\u001b[39m\u001b[34m(self, h)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_net\u001b[39m(\u001b[38;5;28mself\u001b[39m, h: torch.Tensor) -> torch.Tensor:\n\u001b[32m    330\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"pass through the tiny shared network\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshared_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:199\u001b[39m, in \u001b[36mTinySharedNet.forward\u001b[39m\u001b[34m(self, h)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, h: torch.Tensor) -> torch.Tensor:\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m         h = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:162\u001b[39m, in \u001b[36mTRMBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    160\u001b[39m     x = x + h\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# channel MLP (pre-norm inside)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchannel_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:64\u001b[39m, in \u001b[36mChannelMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m     63\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.norm(x)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.drop(h)\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x + h\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/src/exploretinyrm/trm.py:50\u001b[39m, in \u001b[36mSwiGLU.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m     49\u001b[39m     u, v = \u001b[38;5;28mself\u001b[39m.w_in(x).chunk(\u001b[32m2\u001b[39m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mw_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/repos/ExploreTinyRM/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1771\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1768\u001b[39m             tracing_state.pop_scope()\n\u001b[32m   1769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m1771\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1773\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    halt_w = 0.0 if epoch <= 3 else 1.0\n",
    "    train_one_epoch(model, train_loader, optimizer, scaler, epoch, use_amp=True, ema=ema, halt_weight=halt_w)\n",
    "\n",
    "    em_raw, tok_raw, cover_raw, valid_raw, gap_raw = evaluate(model, val_loader, n_sup_eval=N_SUP)\n",
    "    em_ema, tok_ema, cover_ema, valid_ema, gap_ema = evaluate_with_ema(model, ema, val_loader)\n",
    "\n",
    "    print(f\"Validation (raw) | EM {em_raw:.3f} | Tok {tok_raw:.3f} | Cov {cover_raw:.3f} | Valid {valid_raw:.3f} | Gap(valid) {gap_raw:.3f}\")\n",
    "    print(f\"Validation (EMA) | EM {em_ema:.3f} | Tok {tok_ema:.3f} | Cov {cover_ema:.3f} | Valid {valid_ema:.3f} | Gap(valid) {gap_ema:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item 0:\n",
      "GT path: [0, 1, 2, 5, 3, 4, 7, 6, 0]  | length=46.370\n",
      "PR path: [0, 6, 0]  | length=24.000\n",
      "\n",
      "Item 1:\n",
      "GT path: [0, 6, 4, 5, 1, 3, 2, 7, 0]  | length=35.749\n",
      "PR path: [0, 5, 0]  | length=83.522\n",
      "\n",
      "Item 2:\n",
      "GT path: [0, 3, 2, 5, 6, 4, 1, 7, 0]  | length=45.539\n",
      "PR path: [0, 7, 4, 2, 0]  | length=85.378\n",
      "\n",
      "Item 3:\n",
      "GT path: [0, 5, 2, 1, 4, 6, 3, 7, 0]  | length=41.068\n",
      "PR path: [0, 5, 0]  | length=4.000\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def show_tsp_solutions(model: TRM, loader: DataLoader, n_batches: int = 1):\n",
    "    model.eval()\n",
    "    shown = 0\n",
    "    for x_tokens, y_true in loader:\n",
    "        x_tokens = x_tokens.to(device)\n",
    "        y_true   = y_true.to(device)\n",
    "        y_state, z_state = model.init_state(batch_size=x_tokens.size(0), device=device)\n",
    "        for _ in range(N_SUP):\n",
    "            y_state, z_state, logits, halt_logit = model.forward_step(\n",
    "                x_tokens, y=y_state, z=z_state, n=N, T=T, k_last_ops=K_LAST_OPS\n",
    "            )\n",
    "        \n",
    "        L = logits.shape[1]\n",
    "        ar = torch.arange(L, device=logits.device)\n",
    "        logits_masked = logits.clone()\n",
    "        logits_masked[:, ar, ar] = logits_masked[:, ar, ar] - 1e9\n",
    "        preds = logits_masked.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "        xs = x_tokens.cpu().numpy()\n",
    "        ys = y_true.cpu().numpy()\n",
    "        for i in range(min(4, xs.shape[0])):\n",
    "            coords = decode_coords(xs[i], cfg_data.grid_size)\n",
    "            dist = distance_matrix(coords)\n",
    "            gt_path = path_from_successors(ys[i], start=0)\n",
    "            pr_path = path_from_successors(preds[i], start=0)\n",
    "            gt_len = tour_length_from_successors(dist, ys[i])\n",
    "            pr_len = tour_length_from_successors(dist, preds[i])\n",
    "            print(f\"\\nItem {shown+i}:\")\n",
    "            print(\"GT path:\", gt_path, f\" | length={gt_len:.3f}\")\n",
    "            print(\"PR path:\", pr_path, f\" | length={pr_len:.3f}\")\n",
    "        shown += 1\n",
    "        if shown >= n_batches:\n",
    "            break\n",
    "\n",
    "show_tsp_solutions(model, val_loader, n_batches=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 550x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tsp_data import plot_tsp_instance_from_tokens, plot_tsp_compare_from_tokens\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pick one batch from validation\n",
    "x_val, y_val = next(iter(val_loader))\n",
    "\n",
    "# plot ground truth\n",
    "_ = plot_tsp_instance_from_tokens(x_val[0], y_val[0], cfg_data.grid_size, annotate=True, title=\"Ground truth tour\")\n",
    "plt.show()\n",
    "\n",
    "# if you have logits from evaluation above, overlay prediction vs GT\n",
    "with torch.no_grad():\n",
    "    y0, z0 = model.init_state(batch_size=x_val.size(0), device=device)\n",
    "    y1, z1, logits, _ = model.forward_step(x_val.to(device), y=y0, z=z0, n=N, T=T, k_last_ops=K_LAST_OPS)\n",
    "pred = logits.argmax(dim=-1).cpu()\n",
    "_ = plot_tsp_compare_from_tokens(x_val[0], y_val[0], pred[0], cfg_data.grid_size, annotate=False, title=\"GT vs Pred\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits: [B, L, V] with V = cfg.n_cities\n",
    "with torch.no_grad():\n",
    "    preds = logits.argmax(dim=-1)  # [B, L]\n",
    "\n",
    "_ = plot_tsp_compare_from_tokens(\n",
    "    x_val[0], y_val[0], preds[0], cfg.grid_size,\n",
    "    annotate=False,\n",
    "    title=\"Ground truth vs model\"\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_trm = TRMConfig(\n",
    "    input_vocab_size=INPUT_TOKENS,\n",
    "    output_vocab_size=OUTPUT_TOKENS,\n",
    "    seq_len=SEQ_LEN,\n",
    "    d_model=128,           # or 256 if you like\n",
    "    n_layers=2,\n",
    "    use_attention=True,    # for TSP, attention often helps; Mixer can also work since L <= D\n",
    "    n_heads=8,\n",
    "    dropout=0.0,\n",
    "    mlp_ratio=4.0,\n",
    "    token_mlp_ratio=2.0,\n",
    "    n=6,\n",
    "    T=3,\n",
    "    stabilize_input_sums=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.0\n"
     ]
    }
   ],
   "source": [
    "import exploretinyrm as m\n",
    "print(m.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from exploretinyrm.utils import compute_tensor_summary  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "from exploretinyrm.trm import TRM, TRMConfig\n",
    "\n",
    "def set_seed(seed: int = 123):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, json, random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tsp_data_gen import (\n",
    "    write_dataset,\n",
    "    demo_once,\n",
    "    dataset_stats,\n",
    "    show_examples,\n",
    "    pairwise_euclidean,\n",
    "    tour_length\n",
    ")\n",
    "\n",
    "output_dir = Path(\"data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_path = str(output_dir / \"tsp_train_small.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo TSP with n=10, seed=42\n",
      "City coordinates (index: x y):\n",
      "   0: 0.3745 0.9507\n",
      "   1: 0.7320 0.5987\n",
      "   2: 0.1560 0.1560\n",
      "   3: 0.0581 0.8662\n",
      "   4: 0.6011 0.7081\n",
      "   5: 0.0206 0.9699\n",
      "   6: 0.8324 0.2123\n",
      "   7: 0.1818 0.1834\n",
      "   8: 0.3042 0.5248\n",
      "   9: 0.4319 0.2912\n",
      "\n",
      "Cheapest Insertion trajectory:\n",
      "  step  0: insert city 3 at position 3; len 2.6143 -> 2.6164 (delta +0.0021)\n",
      "    partial tour: [5, 6, 2]\n",
      "  step  1: insert city 7 at position 2; len 2.6164 -> 2.6265 (delta +0.0101)\n",
      "    partial tour: [5, 6, 2, 3]\n",
      "  step  2: insert city 9 at position 2; len 2.6265 -> 2.6558 (delta +0.0293)\n",
      "    partial tour: [5, 6, 7, 2, 3]\n",
      "  step  3: insert city 8 at position 1; len 2.6558 -> 2.6869 (delta +0.0311)\n",
      "    partial tour: [5, 6, 9, 7, 2, 3]\n",
      "  step  4: insert city 1 at position 2; len 2.6869 -> 2.9065 (delta +0.2196)\n",
      "    partial tour: [5, 8, 6, 9, 7, 2, 3]\n",
      "  step  5: insert city 4 at position 2; len 2.9065 -> 2.9919 (delta +0.0854)\n",
      "    partial tour: [5, 8, 1, 6, 9, 7, 2, 3]\n",
      "  step  6: insert city 0 at position 1; len 2.9919 -> 3.2503 (delta +0.2583)\n",
      "    partial tour: [5, 8, 4, 1, 6, 9, 7, 2, 3]\n",
      "\n",
      "2-Opt Best-Improvement trajectory:\n",
      "  start (degraded) length 5.6227\n",
      "  step  0: 2-opt swap on indices (i=6, j=7); len 5.6227 -> 4.2508 (delta -1.3718)\n",
      "    tour before: [0, 9, 8, 1, 6, 2, 3, 7, 5, 4]\n",
      "  step  1: 2-opt swap on indices (i=1, j=8); len 4.2508 -> 3.7563 (delta -0.4945)\n",
      "    tour before: [0, 9, 8, 1, 6, 2, 7, 3, 5, 4]\n",
      "  step  2: 2-opt swap on indices (i=5, j=8); len 3.7563 -> 3.4821 (delta -0.2743)\n",
      "    tour before: [0, 5, 3, 7, 2, 6, 1, 8, 9, 4]\n",
      "  step  3: 2-opt swap on indices (i=7, j=8); len 3.4821 -> 3.2852 (delta -0.1969)\n",
      "    tour before: [0, 5, 3, 7, 2, 9, 8, 1, 6, 4]\n",
      "  step  4: 2-opt swap on indices (i=3, j=6); len 3.2852 -> 3.0498 (delta -0.2354)\n",
      "    tour before: [0, 5, 3, 7, 2, 9, 8, 6, 1, 4]\n",
      "  step  5: 2-opt swap on indices (i=4, j=6); len 3.0498 -> 2.9032 (delta -0.1466)\n",
      "    tour before: [0, 5, 3, 8, 9, 2, 7, 6, 1, 4]\n",
      "  step  6: 2-opt swap on indices (i=4, j=5); len 2.9032 -> 2.9031 (delta -0.0001)\n",
      "    tour before: [0, 5, 3, 8, 7, 2, 9, 6, 1, 4]\n",
      "  step  7: STOP at length 2.9031\n"
     ]
    }
   ],
   "source": [
    "demo_once(n=10, seed=42, noise_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_problems = 800       # total problems to sample\n",
    "n_min, n_max = 6, 12           # instance size range\n",
    "prob_constructive = 0.8         # mix of constructive vs improvement samples\n",
    "insert_steps_per_problem = 6    # number of mid-trajectory insert snapshots per problem\n",
    "two_opt_steps_per_problem = 4   # number of mid-trajectory 2-opt snapshots per problem\n",
    "step_sampling = \"mid\"           # \"uniform\" | \"early\" | \"mid\" | \"late\" | \"all\"\n",
    "noise_min, noise_max = 2, 4     # random 2-opt noise before improvement trajectory\n",
    "include_stop_probability = 0.20 # occasionally include a STOP example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: data/tsp_train_small.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "write_dataset(\n",
    "    out_path=output_path,\n",
    "    num_problems=number_of_problems,\n",
    "    n_min=n_min,\n",
    "    n_max=n_max,\n",
    "    seed=1234,\n",
    "    p_constructive=prob_constructive,\n",
    "    ins_per=insert_steps_per_problem,\n",
    "    opt_per=two_opt_steps_per_problem,\n",
    "    step_sample=step_sampling,\n",
    "    noise_min=noise_min,\n",
    "    noise_max=noise_max,\n",
    "    include_stop_prob=include_stop_probability\n",
    ")\n",
    "\n",
    "print(\"Wrote:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing 5 examples from 2089 lines:\n",
      "\n",
      "----------------------------\n",
      "line 214: mode=insert n=10 step=3/7 teacher=cheapest_insertion\n",
      "tour_partial: [0, 1, 6, 7, 5, 3]\n",
      "action: insert city 9 at position 3\n",
      "cost_before -> cost_after: 2.3462 -> 2.3522\n",
      "\n",
      "----------------------------\n",
      "line 357: mode=insert n=12 step=5/9 teacher=cheapest_insertion\n",
      "tour_partial: [5, 0, 1, 6, 4, 8, 10, 7]\n",
      "action: insert city 3 at position 6\n",
      "cost_before -> cost_after: 2.6975 -> 2.7662\n",
      "\n",
      "----------------------------\n",
      "line 1091: mode=insert n=12 step=5/9 teacher=cheapest_insertion\n",
      "tour_partial: [0, 2, 6, 4, 8, 7, 5, 10]\n",
      "action: insert city 3 at position 2\n",
      "cost_before -> cost_after: 2.5198 -> 2.5776\n",
      "\n",
      "----------------------------\n",
      "line 1096: mode=insert n=11 step=2/8 teacher=cheapest_insertion\n",
      "tour_partial: [7, 6, 10, 0, 9]\n",
      "action: insert city 2 at position 1\n",
      "cost_before -> cost_after: 2.5911 -> 2.5964\n",
      "\n",
      "----------------------------\n",
      "line 1668: mode=two_opt n=11 step=1/4 teacher=two_opt_best\n",
      "action: 2-opt (i=4, j=7)\n",
      "tour_full: [0, 3, 2, 5, 10, 1, 9, 4, 6, 8, 7]\n",
      "cost_before -> cost_after: 3.1210 -> 2.9751\n"
     ]
    }
   ],
   "source": [
    "show_examples(output_path, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines read: 2089\n",
      "Mode counts: {'insert': 1089, 'two_opt': 1000}\n",
      "n: min=6 max=12 mean=9.43\n",
      "step_index/total_steps: mean=0.331 (0=early, ~0.5=mid, 1=late)\n",
      "mean delta (cost_after - cost_before): -0.1170\n"
     ]
    }
   ],
   "source": [
    "dataset_stats(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_insert_example(example: dict, figsize=(5,5)):\n",
    "    assert example[\"mode\"] == \"insert\"\n",
    "    coords = np.array(example[\"coords\"], dtype=np.float32)\n",
    "    tour_partial = example[\"tour_partial\"]\n",
    "    action_city = example[\"action\"][\"city\"]\n",
    "    insert_position = example[\"action\"][\"position\"]\n",
    "    m = len(tour_partial)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(coords[:, 0], coords[:, 1], s=40)\n",
    "    for idx, (x, y) in enumerate(coords):\n",
    "        plt.text(x, y, str(idx), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # Draw partial tour cycle\n",
    "    for k in range(m):\n",
    "        a = tour_partial[k]\n",
    "        b = tour_partial[(k + 1) % m]\n",
    "        plt.plot([coords[a, 0], coords[b, 0]], [coords[a, 1], coords[b, 1]])\n",
    "\n",
    "    # Highlight the insertion edge that will be split\n",
    "    u = tour_partial[(insert_position - 1) % m]\n",
    "    v = tour_partial[(insert_position) % m]\n",
    "    plt.plot([coords[u, 0], coords[v, 0]], [coords[u, 1], coords[v, 1]], linestyle=\"--\", linewidth=2)\n",
    "\n",
    "    # Highlight the city to insert\n",
    "    plt.scatter([coords[action_city, 0]], [coords[action_city, 1]], s=110, marker=\"s\")\n",
    "\n",
    "    title = f\"Insert step: city {action_city} at position {insert_position}\\n\" \\\n",
    "            f\"cost {example['cost_before']:.4f} -> {example['cost_after']:.4f}\"\n",
    "    plt.title(title)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_two_opt_example(example: dict, figsize=(5,5)):\n",
    "    assert example[\"mode\"] == \"two_opt\"\n",
    "    coords = np.array(example[\"coords\"], dtype=np.float32)\n",
    "    tour_full = example[\"tour_full\"]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(coords[:, 0], coords[:, 1], s=40)\n",
    "    for idx, (x, y) in enumerate(coords):\n",
    "        plt.text(x, y, str(idx), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # Draw entire tour\n",
    "    n = len(tour_full)\n",
    "    for k in range(n):\n",
    "        a = tour_full[k]\n",
    "        b = tour_full[(k + 1) % n]\n",
    "        plt.plot([coords[a, 0], coords[b, 0]], [coords[a, 1], coords[b, 1]])\n",
    "\n",
    "    action = example[\"action\"]\n",
    "    if not action.get(\"stop\", False):\n",
    "        i = action[\"i\"]\n",
    "        j = action[\"j\"]\n",
    "        a = tour_full[i - 1]; b = tour_full[i]\n",
    "        c = tour_full[j];     d = tour_full[(j + 1) % n]\n",
    "        # Emphasize the two edges that will be replaced by the 2-opt swap\n",
    "        plt.plot([coords[a, 0], coords[b, 0]], [coords[a, 1], coords[b, 1]], linewidth=3)\n",
    "        plt.plot([coords[c, 0], coords[d, 0]], [coords[c, 1], coords[d, 1]], linewidth=3)\n",
    "        title = f\"2-opt step: (i={i}, j={j})  cost {example['cost_before']:.4f} -> {example['cost_after']:.4f}\"\n",
    "    else:\n",
    "        title = f\"2-opt STOP at cost {example['cost_before']:.4f}\"\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAHDCAYAAACnJFQ8AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAgjxJREFUeJzt3Xd4VFX6wPHvnZnMpHeSkJAQeofQpYOiiEpxLYgNUBGlyMrubxUVUSyo6yooCIqNRQRWFLAgiigiTXrvNaEkkN4zycz5/TFkIBIgCUluMnk/zzMPkzu3vLlM5p1z73vO0ZRSCiGEEMKFGPQOQAghhChvktyEEEK4HEluQgghXI4kNyGEEC5HkpsQQgiXI8lNCCGEy5HkJoQQwuVIchNCCOFyJLkJIYRwOZLcRLn53//+R2BgIJmZmc5l0dHRDB8+XL+grmL16tVomsbq1asr7ZgrVqzA29ub8+fPV9oxhaiJXCK5bd68mbFjx9KiRQu8vLyIiori3nvv5dChQ7rFlJ2dzUsvvVSuH5zLly/npZdeKrf9lSebzcbkyZMZN24c3t7eFX681157DU3TaNmyZYUfqySGDx+OpmmXPZo2bVpkvVtvvZWGDRsydepUnSItvX379vHSSy9x4sSJEq2/Zs0aBg4cSGRkJO7u7oSFhXHrrbeybt26Em2/ZMkS+vXrR3h4OBaLhTp16nD33XezZ8+eYtf/9ttvadeuHe7u7kRFRTF58mQKCgqKrPP5558X+/+jaRrx8fFXjOXo0aO4u7ujaRpbtmwpUfwVLScnh0cffZSWLVvi5+eHt7c3bdq0Yfr06eTn55doH6+99hoDBw4kNDQUTdOu+bmyaNEiunTpgpeXF/7+/nTt2pVff/31svUSEhIYNWoUERERuLu7Ex0dzaOPPlpknZdeeqnY/wd3d/cSn4OSMJXr3nTy5ptvsm7dOu655x5at25NfHw8M2bMoF27dmzcuFGXD8Ds7GxefvllAHr37l0u+1y+fDkzZ86skgnuu+++4+DBgzz++ONFlh88eBCDoXy/Q506dYrXX38dLy+v69pPz549ycnJwWw2l0tcFouFjz/+uMgyPz+/y9YbNWoU//znP3n55Zfx8fEpl2NXpH379vHyyy/Tu3dvoqOjr7n+oUOHMBgMPPHEE4SFhZGSksIXX3xBz549+eGHH7j11luvuv3u3bsJCAhg/PjxBAcHEx8fz6effkqnTp3YsGEDbdq0ca77448/MnjwYHr37s3777/P7t27efXVVzl37hyzZs26bN9TpkyhXr16RZb5+/tfMZann34ak8lEXl7eNX/vypKTk8PevXu57bbbiI6OxmAwsH79ep5++mn+/PNPvvzyy2vu44UXXiAsLIy2bdvy008/XXXdl156iSlTpnD33XczfPhw8vPz2bNnD6dPny6yXlxcHN26dQPgiSeeICIigjNnzrBp06Zi9ztr1qwiX4SNRuM14y4V5QLWrVun8vLyiiw7dOiQslgs6oEHHtAlpvPnzytATZ48udz2OWbMGFVV/8sGDhyounfvXinHGjJkiLrxxhtVr169VIsWLSrlmNcybNgw5eXlVaJ1ExISlNFoVJ988kkFR1U+vvrqKwWo3377rcz7yMrKUqGhoapfv35l2j4+Pl6ZTCY1atSoIsubN2+u2rRpo/Lz853Lnn/+eaVpmtq/f79z2WeffaYAtXnz5hIfc8WKFcpsNqsXXnih1Nv+1fnz59WZM2fKvH1JjB07VgHq7Nmz11z3+PHjzriu9jm1YcMGpWmaeuedd665z/79+6t69eqpxMTEq643efJkBajz589fc5/XwyUuS3bt2vWyb9+NGjWiRYsW7N+/v8T7+eqrr2jfvj0eHh4EBwfz4IMPXvbtZPjw4Xh7e3Ps2DH69euHl5cX4eHhTJkyBXVhgoUTJ05Qq1YtAF5++WVns/tqLa78/HxefvllGjVqhLu7O0FBQXTv3p2VK1c6jztz5kyAIk35Qna7nWnTptGiRQvc3d0JDQ1l1KhRpKSkFDlOdHQ0d9xxBz///DMxMTG4u7vTvHlzvvnmm8tiOnr0KEePHr3mecvNzWXFihX07dv3stfK+57bmjVrWLx4MdOmTbvufVXEPTebzUZ6evpV1wkJCaF169YsW7aszMdJTU3l6aefJjo62nnp7uGHHyYxMdG5zrlz53j00UcJDQ3F3d2dNm3aMHfu3Mv2tXDhQtq3b4+Pjw++vr60atWK6dOnA47Leffccw8Affr0cb7vSnvOPD09qVWrFqmpqWX6fUNCQvD09Cyy/b59+9i3bx+PP/44JtPFi1CjR49GKcXixYuL3VdGRgY2m+2qx8vPz2f8+PGMHz+eBg0alCnmS+3Zs4eoqCgGDRrEt99+e9ll0/JQ2KouyTkuSQscYNq0aYSFhTF+/HiUUkXup1/qwIED/Pjjj/zf//0fQUFB5ObmXvMSqVKK9PR05+dmeXOJ5FYcpRQJCQkEBweXaP3PP/+ce++9F6PRyNSpUxk5ciTffPMN3bt3v+zNYrPZuPXWWwkNDeWtt96iffv2TJ48mcmTJwNQq1Yt5yWRO++8k3nz5jFv3jz+9re/XfH4L730Ei+//DJ9+vRhxowZPP/880RFRbFt2zbAcSnr5ptvBnDub968ec7tR40axf/93//RrVs3pk+fzogRI5g/fz79+vW77E12+PBhhgwZQv/+/Zk6dSomk4l77rnHmUgL3XTTTdx0003XPHdbt27FarXSrl27a65rt9tJTEws0eOvcdtsNsaNG8djjz1Gq1atrnmsssjOzi5RbH/90lC4ra+vL35+fgQGBjJmzJgrfhi0b9+e9evXlynGzMxMevTowfvvv88tt9zC9OnTeeKJJzhw4ACnTp0CHJeuevfuzbx583jggQf497//jZ+fH8OHD3cmLoCVK1cydOhQAgICePPNN3njjTfo3bu38/5Yz549eeqppwB47rnnnO+7Zs2aXTPO9PR0EhMTOXDgAM899xx79uwp0fupUGpqKufPn2f37t089thjpKenF9l++/btAHTo0KHIduHh4dSpU8f5+qX69OmDr68vnp6eDBw4kMOHDxd77GnTppGSksILL7xQ4nivJiYmhkmTJrF7924GDRpEVFQUEydOvOLxS8JqtZKYmEhcXBxLlizh7bffpm7dujRs2LBcYgZYtWoVHTt25L333qNWrVr4+PhQu3ZtZsyYUWS9X375BYDQ0FBuuukmPDw88PDwoH///le8V1u/fn38/Pzw8fHhwQcfJCEhodziBqroNa5yMG/ePAWU6NKP1WpVISEhqmXLlionJ8e5/Pvvv1eAevHFF53Lhg0bpgA1btw45zK73a5uv/12ZTabnU3t0l6WbNOmjbr99tuvus6VLkv+8ccfClDz588vsnzFihWXLa9bt64C1Ndff+1clpaWpmrXrq3atm1bZPu6deuqunXrXjP2jz/+WAFq9+7dl71Wt25dNWzYMOfPx48fV0CJHn+9DDZjxgzl5+enzp07p5RS131Z8rfffrvsOIWXTK71+Ot5efbZZ9UzzzyjFi1apBYsWOB8n3Tr1q3IJbNCr7/+ugJUQkJCqeN+8cUXFaC++eaby16z2+1KKaWmTZumAPXFF184X7NarapLly7K29tbpaenK6WUGj9+vPL19VUFBQVXPF5ZL0v269fPeb7MZrMaNWpUkb+va2nSpIlze29vb/XCCy8om83mfP3f//63AlRsbOxl23bs2FHdcMMNzp8XLVqkhg8frubOnauWLFmiXnjhBeXp6amCg4Mv2/7s2bPKx8dHffjhh0qpsl3SvBK73a5+/fVX9eCDDyoPDw8FqJ49e6q5c+eq7OzsUu1rwYIFRd6THTp0ULt27SrVPq72OZWcnKwAFRQUpLy9vdW///1vtWjRInXrrbcqQM2ePdu57lNPPeVc99Zbb1WLFi1S//73v5W3t7dq0KCBysrKcq47bdo0NXbsWDV//ny1ePFiNX78eGUymVSjRo1UWlpaqeK/GpdMbvv371e+vr6qS5cuV/2jLbR+/XoFqA8++OCy15o2barat2/v/LnwQ+vgwYNF1vvxxx8VoBYsWKCUKn1y69Wrl4qOjlaHDh264jpXSm5PPfWU80P//PnzRR7e3t7qsccec65bt25dFR4e7vwQLPTMM8+U+Hr9X7355psKUKdOnbrstb8mt5ycHLVy5coSPZKTk53bJSYmqsDAQPX22287l1VEcjt69GiJYlu7du019//aa68VeU9catasWQpQe/fuLXXcLVq0UG3atLnqOrfccosKCwsrkgyUuviB+N133ymlHMncaDSqH3/88Yr7Kmty2759u/r555/VJ598onr27KlGjBihMjIySrz9+vXr1YoVK9QHH3ygOnbsqP7xj38oq9XqfH3KlClX/ILQo0ePa56jP/74Q2madtl9vIcffli1adPGee7KM7ldKi0tTc2ePVt17txZAcrPz0898cQTRd73VxMfH69WrlypvvrqK/XEE0+oLl26qA0bNpQqhqt9TsXGxjoT58KFC53LbTabat68uapTp45z2SOPPKIA1aJFiyLvucL325w5c64ax/z58xWgpk6dWqr4r8blktvZs2dV/fr1VWRkpDp9+nSR11JTU9XZs2edj6SkJKXUxf+AVatWXba/wYMHq+DgYOfPw4YNUwaD4bJv40ePHi3yn1Pa5Pb7778rf39/BaiWLVuqf/7zn2rnzp1F1rlScuvfv/9VWxkDBw50rlu3bl3Vs2fPy/bxySefKKDUfxxKXUxucXFxl7321+RWVk888YRq2LBhkcKhikhu5Sk7O1sZDAb16KOPXvbaBx98oAC1b9++Uu/X3d39moVSTZo0UT169Lhs+Y4dOxSgZsyYoZRyFLc0a9ZMASoiIkKNGDHiskRXHgUleXl5qkWLFuquu+4q0/bJyckqNDRU/eMf/3AuK03L7UpuuOEG1aBBA+fPhQUUv/76q3NZSZNbQUFBkc+Xs2fPXlboVpycnBz1wgsvKE3TFKC2b99+zW2K89prrylvb+9SfUG92udU4Wtubm6XNRJefvllBaiTJ08qpS5+Nr388stF1isoKFAmk0mNGDHimrGEhYWpm266qcSxX4tLdAUolJaWRv/+/UlNTeWPP/4gPDy8yOvjx48vckO9V69eldqB92p69uzJ0aNHWbZsGT///DMff/wx7777LrNnz+axxx676rZ2u52QkBDmz59f7OuFxS0VJSgoCICUlBTq1Klz1XVtNluJOzAHBgZiNps5fPgwH330EdOmTePMmTPO1wtvWp84cQJfX18CAwPL/ktckJmZecX7ZJcyGo3XPK8eHh4EBQWRnJx82WuF9+xKek+4ooSEhLBjxw5++uknfvzxR3788Uc+++wzHn744WKLT8rKbDYzcOBA3njjDXJycvDw8CjV9gEBAdx4443Mnz+ft99+G4DatWsDcPbsWSIjI4usf/bsWTp16nTN/UZGRnLw4EHnz//617/o0aMH9erVc94rKizSOXv2LLGxsURFRRW7r7i4uMu6Gfz2229X7Aq0efNmPv30UxYuXEhqaiqdO3fm0UcfLdH9zOLcfffdPP/88yxbtoxRo0aVaR+XCgwMxN3dHX9//8vK9ENCQgDH+zgqKsr5WRsaGlpkPaPRSFBQULH3qP8qMjKy2L+VsnKZ5Jabm8uAAQM4dOgQv/zyC82bN79snX/96188+OCDzp8DAgIAqFu3LuDok3XjjTcW2ebgwYPO1wvZ7XaOHTtG48aNncsKO4wXViFdWslYUoGBgYwYMYIRI0aQmZlJz549eemll5zJ7Ur7bNCgAb/88gvdunUr0YfGkSNHUEoV2d9f4y+Nwo7Kx48fv2ahR3EfAFdS+MFw+vRp7HY7Tz31lLO44VL16tVj/Pjx5VJB+fbbbzv7J15N3bp1r9mpOSMjg8TExGKT4PHjxwkODi7TF48GDRpcsUPzpfHt2rULu91epJ/hgQMHnK8XMpvNDBgwgAEDBmC32xk9ejQffvghkyZNomHDhmV6LxcnJycHpRQZGRmlTm6F26elpTl/jomJAWDLli1FEtmZM2c4derUZX0ui3Ps2LEi/wexsbGcPHmy2PfowIED8fPzu2I1YlhY2GVFWZf2yQNHBeu8efP47LPP2Lt3L0FBQQwfPtzZKft65OTkABQ5R9fDYDAQExPD5s2bsVqtRSrSC79kFp679u3bA1xWXV5Y9HKt97lSihMnTtC2bdtyiR1cJLnZbDaGDBnChg0bWLZsGV26dCl2vebNmxeb9Dp06EBISAizZ8/mkUcewWKxAI4Oovv37+fFF1+8bJsZM2bw3nvvAY7/mBkzZuDm5uas5vL09ARKVpYLkJSU5GwBAXh7e9OwYUPi4uKcywo7LaemphbpeHrvvffywQcf8Morr/D6668X2W9BQQGZmZlF1j9z5gxLlixxVm+mp6fz3//+l5iYGMLCwpzrFXYDuFYpdPv27TGbzWzZsoWBAwdedd3iPgCupPCDoWXLlixZsuSy11944QUyMjKYPn16uZRrAzz88MN07979mutd+uFc2IL8a4fsV155BaVUsZ2Wt27desX36bXcddddTJkyhSVLlnDnnXcWea3wS8ttt93Gzz//zKJFixg6dCjgeC+8//77eHt706tXL+Dy953BYKB169YAzo7Ll77vSuLcuXPOb/aFUlNT+frrr4mMjCzyWmxsLNnZ2UVGcilu+xMnTrBq1aoilZEtWrSgadOmfPTRR4waNcrZupg1axaapnH33Xc71z1//vxlH7DLly9n69atRb4wffTRR2RnZxdZ79dff+X999/n7bffvmzEmUu5u7sX2x0GHF/qxo0bxw8//IDNZuOmm25i0qRJ3HnnnaUeRCAxMZGgoKDLvnQUDiBw6TlKS0vj7Nmz1K5du9gBBa5lyJAhbNy4kblz5zJy5EjA8X6fP38+zZs3d7bYevfu7bx69NxzzzlHG/n888+x2WzOSm8o/v9i1qxZnD9//pod/Eul3C5w6mj8+PEKUAMGDFDz5s277FEShdfVO3furKZNm6YmTpyoPD09VXR0tEpJSXGuN2zYMOXu7q4aNWqkHn74YTVz5kx1xx13KEA999xzRfbZvHlzFRYWpmbOnKkWLFhQbDVhoZCQEHXvvfeqN998U82ZM0eNGjVKaZpWpCrzf//7nwLUQw89pL744osihQqjRo1SgOrfv79699131YwZM9T48eNVeHi4+uqrr5zr1a1bVzVu3Fj5+/urZ599Vr377ruqVatWymAwqBUrVhSJqaTVkkopdccdd6guXbpctry87rkV50r33AorHq91j6i87rkdP35c+fv7qyeffFJNnz5dTZ8+Xd12220KULfeeutlRR2Fnbg//vjjMh0vIyNDNW/eXBmNRjVy5Eg1e/Zs9frrr6sbbrhB7dixQynluN/XrFkzZTab1T/+8Q/1/vvvq169eilATZs2zbmvwYMHq549e6qXXnpJffzxx2rSpEnK399fxcTEOOM+e/asMhqN6oYbblCff/65WrBgwVWrPNu1a6cGDhyoXnvtNTVnzhw1adIkVadOHWUwGIq8F5VSzpguFRISooYOHarefPNN9dFHH6n/+7//U4GBgcrd3V2tW7euyLrfffed0jRN3Xjjjeqjjz5STz31lDIYDGrkyJFF1mvYsKG655571Jtvvqlmz56tHn/8cWUymVRkZKSKj4+/6vkuj4KS3377TUVGRqpJkyY5O1CX1bvvvquaNGminnnmGfXhhx+qt99+W918883Oz8DiYv/ss8+KLP/vf/+rXnnlFTVx4kQFqD59+qhXXnlFvfLKK+rEiRPO9bKzs1WLFi2Um5ub+uc//6nee+891bFjR2U0GtXy5cuL7HPu3LkKUB07dlTvvfee+uc//6nc3NxUjx49ityz8/DwUMOHD1f/+c9/1MyZM9XQoUOVpmkqJiamSFXl9XKJ5Fb4B3KlR0ktWrRItW3bVlksFhUYGKgeeOCByyoAC0eiOHr0qLrllluUp6enCg0NVZMnT77sQ2z9+vWqffv2ymw2X7O45NVXX1WdOnVS/v7+ysPDQzVt2lS99tprRarDCgoK1Lhx41StWrWcN58v9dFHH6n27dsrDw8P5ePjo1q1aqX+9a9/FRkZoW7duur2229XP/30k2rdurWyWCyqadOml33oFK5b0uT2zTffKE3TLru5r0dy+8c//nHZCBXFKa/klpKSoh588EHVsGFD5enpqSwWi2rRooV6/fXXi/z/FZo1a5by9PR0luOXRVJSkho7dqyKiIhQZrNZ1alTRw0bNqzI6BAJCQlqxIgRKjg4WJnNZtWqVavLPuQWL16sbrnlFhUSEqLMZrOKiopSo0aNuqwoYc6cOap+/frKaDRe85zNmDFDde/eXQUHByuTyaRq1aqlBgwYoNasWXPZusUlt8mTJ6sOHTqogIAAZTKZVHh4uLrvvvuuWOa+ZMkSFRMToywWi6pTp4564YUXLjvvzz//vIqJiVF+fn7Kzc1NRUVFqSeffPKaiU2p8klu2dnZl30+lNXmzZvVPffco6KiopTFYlFeXl6qXbt26p133rms0O1Kye1qn5l//b9NSEhQw4YNU4GBgcpisajOnTtf9kW40IIFC1SbNm2UxWJRoaGhauzYsZe9zx977DHVvHlz5ePjo9zc3FTDhg3VM888c11/D8XRlKqg7uEuavjw4SxevLhERQdVUXR0NC1btuT7778v1/3abDaaN2/OvffeyyuvvFKu+y6tTp06UbduXb766qurrrdq1Sr69u3LH3/8UaJLkeWlbdu29O7dm3fffbfSjilETeOyI5SIymU0GpkyZQozZ87UNfGnp6ezc+dOpkyZcs11z549C1RuxeKKFSs4fPgwEydOrLRjClETuURBiagahgwZwpAhQ3SNwdfX95ojuGdlZTF//nymT59OnTp1ilS9VrRbb7212rb6hahOpOUmapzz588zbtw4PDw8+Prrr8t9Sh4hhP7knpsQQgiXI19ZhRBCuBxJbkIIIVxOtSgosdvtnDlzBh8fn3IbCkgIIUT1oi4M3xYeHn7Ne+XVIrmdOXPmsoFRhRBC1ExxcXHXHKS9WiS3wjH74uLi8PX11TkaIYQQekhPTycyMvKycVyLUy2SW+GlSF9fX0luQghRw5Xk9pQUlAghhHA5ktyEEEK4HEluQghRRhkZGfz973+nbt26eHh40LVrVzZv3qx3WAJJbkIIUWaPPfYYK1euZN68eezevZtbbrmFvn37XjYjtah81WL4rfT0dPz8/EhLS5OCEiFElZCTk4OPjw/Lli3j9ttvdy5v3749/fv359VXX9UxOtdUmlwgLTchhCiDgoICbDYb7u7uRZZ7eHiwdu1anaKqHt544w00TePvf/97hR1DkpsQQpSBj48PXbp04ZVXXuHMmTPYbDa++OILNmzY4JwrUFxu8+bNfPjhh7Ru3bpCjyPJTQghymjevHkopYiIiMBisfDee+8xdOhQmUbpCjIzM3nggQeYM2cOAQEBFXos+R8QQogyatCgAb///juZmZnExcWxadMm8vPzqV+/vt6hVUljxozh9ttvp2/fvhV+rGoxQokQQlRlXl5eeHl5kZKSwk8//cRbb72ld0hVzsKFC9m2bVuldZWQ5CaEEGX0008/cS49h6N5vhw5eoTV896lfsPGjBgxQu/QqpS4uDjGjx/PypUrLyvAqSiS3IQQogzybXZm/byLHz59h4KMRIzuPng16Yaxx0O8+O1+pgxuiZtR7vwAbN26lXPnztGuXTvnMpvNxpo1a5gxYwZ5eXkYjcZyPaYkNyGEKIMXl+5hp7k5EaM+vuy1hVviAJh6V8VWBFYXN910EyvXbmLV/nOkZlvx9zSzfOaLtG7ZnGeeeabcExtIchNCiFKLTcpm4eY4FOBLHrcaClin7ORioAADNgws3XyMx7pGEh3iWyEf3tVFvs3O6z8fZ+Hmcxg0DU2zoBScTrJiTlE0ada8Qo4ryU0IIUpp2Y7TGDQNm1L0N2bg4Xacm4tZb96HOwDHFC1ubm64ublhMpmu+vxar19rXZPJVKW6Iry4dA8Ltzi+CNiUgkvGxDqYkMGLS/dUSAtXkpsQQpRSYmYemgYWBd2VB8ft3pyngFzNhhE7JhQmze5cXymF1WrFarVWSnxGo7FcEmVJ1jUajVecX+3SFu5fhd3/BgALN8cxuk9DIgM9y/UcSHITQohSCvZ2XFq7GzNt7b6EWkO5n0wKLlnHoCn+fmNDHusWRUFBAfn5+c5/S/K8tK/bbDbnsW02W5GfK9qVkl98Rj43ueVScOFybZOInXhaMll5ujsHMutdOE8aS7efZtxNjco1JkluQghRSoNiIvho5SEewALAJ+QVSWwASmnc2T4KT8/ybZFcid1uL3OiLEtSvXTM/cJ1ilPnktuNbSK24+WVytH0us7kpmmOlnB5k+QmhBClFBXkySu1a+F7No9j2FhJ0Q92TYP7OkSW+6W2qzEYDFgsFiwWS4UfSymFzWa7ZiL8YccpVu45g4YNf/cUenilYrdrbE9s4dyXXSmCvcs/ZkluQghRSrYMKx2T8lHAx+ShaRomzfFBrZQjsU0Z3FLvMCuMpmmYTCZMJtNVO2V7hdRl5s7fUECfwEMAHEmrz9n8IOc6SsHgthHlHqMkNyGEKKWM3+JQVjtukT68dl87lu44Q2JmHrV8LAyKiajUFltVFhXkyX0dI1m4JY7WwXsB2Hn+YqutIlu4ktyEEKIUClJyyfzTMaWNX7+6uAd5lXsxhCuZMrglBnJp5utoue1NaolBo8JbuJLchBCiFNJ/iQWbwtLQH/eGFTttiytwMxr4e68Mdu0qwKpCubFlJ2r5uFd4C1eSmxBClFD+uWyytyUA4HtLXZ2jqT4SE38FoH7kLfRv3KpSjll1urELIUQVl/7zCVDg3jwIS5Sv3uFUC0opkpJWAxAc1KfSjivJTQghSsB6KoOcPUmggZ+02kosM3M/eXnxGAwe+PvfUGnHleQmhBAlkPbTCQA8Y0JwC/PSN5hqJDHpNwACA7thNFZ8H7xCktyEEOIaco+mknc4FYwavjdLq600EhMdya0yL0mCJDchhLgqpRTpF1ptXh3DMAVWzkzSrsBqTSI9fQcAQcG9K/XYktyEEOIqcvcnY43NQHMz4HtjlN7hVCtJSb8DCh/vFrhbwir12JLchBDiCpRdOSokAe+u4Rh9zfoGVM0U3m8LCq7cS5IgyU0IIa4oZ9d58uOz0dyN+PSqo3c41Yrdnk9S0hqg8u+3gSQ3IYQolrLZSVt5EgCfXnUweLrpHFH1kpq2BZstEze3QHx9y3+m7WuR5CaEEMXI2pyALSkXg7cb3l3Lf9R6V5fkrJLsjaZVfqqR5CauaNasWbRu3RpfX198fX3p0qULP/74o95hCVHhVL6N9FWxAPj2icRgMV5jC/FXF++33ajL8SW5iSuqU6cOb7zxBlu3bmXLli3ceOONDBo0iL179+odmhAVKnP9WewZVoz+Frw619Y7nGonO/sE2dnH0DQTQYHddYlBBk4WVzRgwIAiP7/22mvMmjWLjRs30qJFiytsJUT1Zs8tIOP3OAB8+9ZFM0kboLQKW23+/h0xmXx0iUGSmygRm83GV199RVZWFl26dNE7HCEqTMaaU9izCzCFeODZLkTvcKqli/fb9LkkCZLcxDXs3r2bLl26kJubi7e3N0uWLKF58+Z6hyVEhbBlWslcexoA35uj0QyazhFVPwUFmaSkbgIgWIf+bYWkvS2uqkmTJuzYsYM///yTJ598kmHDhrFv3z69wxKiQmT8Foey2nGr441HyyC9w6mWkpPXoVQ+Hh7ReHrW0y0OabmJqzKbzTRs2BCA9u3bs3nzZqZPn86HH36oc2RClK+C1FwyN54FwK9fNJomrbaySExyTEwarFOVZCFpuYlSsdvt5OXl6R1GpVqzZg0DBgwgPDwcTdNYunSp3iGJCpD+SyzYFJb6flga+usdTrWklP2SiUl76xqLJDdxRRMnTmTNmjWs37GfiXO+p/vdj7F69WpuHnCX3qFVqqysLNq0acPMmTP1DkVUkPzz2WRvTQDAV1ptZZaRsQerNRGj0Rt//466xiKXJcUVxSckMODuoaQnn8Ng8cISEk3ova/wwmYDe9jFlMEtcTO6/vej/v37079/f73DEBUo/eeToMC9WSCWur56h1NtFc7dFhjYHYNB30Gmy/TJNHPmTKKjo3F3d6dz585s2rTpqutPmzaNJk2a4OHhQWRkJE8//TS5ubllClhUnrDb/07gox/R4qkviRw3n5Ahr2GJjkEBC7fE8eLSPXqHKMR1s57OJGd3ImiOe22i7C7eb9OvSrJQqVtuixYtYsKECcyePZvOnTszbdo0+vXrx8GDBwkJubxPyJdffsmzzz7Lp59+SteuXTl06BDDhw9H0zTeeeedcvklRPmLTcpm4eY4vKzZfLX8Rc54BXEgoC4HAqM4EFCXY37hLNwcx+g+DYkM9NQ7XCHKLO3CRKSebWrhFualbzDVWF7eOTIy9gAaQTrfb4MyJLd33nmHkSNHMmLECABmz57NDz/8wKeffsqzzz572frr16+nW7du3H///QBER0czdOhQ/vzzz+sMXVSkZTtOY9A0mqQ4xtcLz0oiPCuJG09tA8BqMBHrF8nepDV4D+qNd8d2uIWG6hmyEKWWdyyNvEMpYNDwvbmu3uFUa4WFJL6+rbGYg/UNhlImN6vVytatW5k4caJzmcFgoG/fvmzYsKHYbbp27coXX3zBpk2b6NSpE8eOHWP58uU89NBDVzxOXl5ekYq89PT00oQpykFiZh6aBk2TTxb7utleQMOU4/DHceL/+BYAq2cgeWENsddthqFJC9ybNccrxAcvPwte/hY8/cwYa8A9OlE9KKWcrTavjqGYgjz0DaiaS0y8cElSh7nbilOq5JaYmIjNZiP0L9/QQ0NDOXDgQLHb3H///SQmJtK9e3eUUhQUFPDEE0/w3HPPXfE4U6dO5eWXXy5NaKKcBXtbUAoapp0u8Tbm7GTMxzbBsU3wG9g1A4nedTjqW49032jS/epBcG28AtydCc/Lz3zh3ws/+1vw8HaTkSFEhcs9mIL1ZDqYDPjeGKV3ONWa3Z5Hcso6QJ9Zt4tT4dWSq1ev5vXXX+eDDz6gc+fOHDlyhPHjx/PKK68wadKkYreZOHEiEyZMcP6cnp5OZGRkRYcqLjEoJoJ3Vh7ilU7DiE6Pp2nKSZomx9I05SSRmedLtA+DsuObEYtvRiyc/h0Aq5s3aReS3VnfeqT71MVmci+6nUHD08+Mp58F7wsJ0NOZAC8mQ4unqVJKtjMzM1m7dTe/HTgHwH9/2oR7QCjN60UQFSUfitWRsivSL7TavLuGY/Sz6BtQNZeSsgmbLRuLORQf76oxqHqpkltwcDBGo5GEhIQiyxMSEggLCyt2m0mTJvHQQw/x2GOPAdCqVSuysrJ4/PHHef755zEYLr9MZbFYsFjkzaanqCBP7usYycItcRzzj+CYfwTL63UFwNuaTdPUWO52T6Vr3llydu3CnpFRov2a8zOplbSbWkm7AVBoWIMiyQqsT4pXXc6bI8n2CCUzJY/MlDzOXWVfRjfDxZZfYfL7SwL08rfgdh1zceXb7Dz+n0UseOkx57Ils6eyZPZUWvYeyLZfltSI7hCuJmf3efLPZqFZjPj0qqN3ONVeYZVkUHDvKtNHsFTJzWw20759e1atWsXgwYMBx4gVq1atYuzYscVuk52dfVkCMxodHzZKqTKELCrLlMEtAVi4OQ6DpqFpYFeKLLMnjW6/mbsu9HNTdjvW48fJ2bGTnJ2OR97hw2C3X/MYGgpLUiyWpFgCgQaA5u2NsXFzVHQzrOGNyQqsR2aBO9mpeWSm5pGVlkdeVgG2fDvpibmkJ169W4nZ3Vg0Afo7WoVefha8Axz3Ar38LBiLmdrkxaV72JAbRsM3nkXZvCjIaAXK8WeTqTlen3pX61KfW6EfZbM7+rUBPj3rYPRy0zmi6k0p5ezfVlXut0EZLktOmDCBYcOG0aFDBzp16sS0adPIyspyVk8+/PDDREREMHXqVMAxJ9g777xD27ZtnZclJ02axIABA5xJTlRNbkYDU+9qzeg+DVm6/TSJmXnU8rEwKCaiSPm/ZjBgadAAS4MG+N/1NwBsmVnk7tnjTHY5O3ZgS04u0XFVZiYF2zbBtk0YAV8gODoajzZt8Ihpg0ebNhjrNSAny+5Idql5ZKdZybqQ+Bz/WslMzaMgz4Y114Y1PpuU+OyrHtfd261IAiwwG9i/8QzRpjwSa/0MRiv2gh/IT+mMNaUryuYl3SGqoaytCRQk5WLwcsO7e7je4VR72dlHyc2Nw2AwExDQVe9wnEqd3IYMGcL58+d58cUXiY+PJyYmhhUrVjiLTGJjY4u01F544QU0TeOFF17g9OnT1KpViwEDBvDaa6+V328hKlRkoCfjbmpUqm2M3l543dAZrxs6A45vd/mnT5OzfYcz4eXu3w8FBSXan/XECawnTpC2bBkAmocHHi1a4BHThtA2joTnFnL5fVlrboEj2V1IeBef55GVanX8m5aHvUCRm5lPbmY+SacyndvfjJldYetJNFoBMJgyMQf/Sn5qRxRg0DSWbj9d6vMj9KHybWT84uje4tMnEoNFBmm6XhcnJu2MyVR1+glqqhpcG0xPT8fPz4+0tDR8fWVoHFdhz80ld9/+i627nTspOHu2zPszhdfGMybG0cJr0wZL8+YYzNceAkgpRW5W/sVkdyEBrt4Rz5n4dDa0fIMs90Tn+vnpLck9/aDjmAaNBzpH8fKglmWOW1SejDWnSFt+HKOfhbB/dkBzk/ul12vrtvtJTf2Txo0nE1nn4Qo9VmlygXxtEboxuLvj2a4tnu3aOpflJyRcuIx5oXW3Zw+qhLMQFJw5S/qZs6Qv/xEAzc0NS/NmzmTn0SYGt4jwy254a5qGh7cZD28zwXW8ncs3uhfw/YYNuF+S2ADyky9eerErRbC3FD9VB/bcAjJWxwHg2zdKEls5yM9PIy1tC1C17reBJDdRxbiFhuJ2yy343nILACo/n9yDh8jZefFyZv7J2BLtS+Xnk7tzF7k7d5HCPACMwcEXk11MGzxatsTgWfz9skExEXxwYF2RZbbc2thyLk7AqBQMbhtRll9VVLKMP05jzy7AVMsDz3Yymk55SE7+A6VseHk1wsOjanXXkuQmqjTNzQ2Pli3waNkCHngAgIKUlIv37XbuJGfnLuxZWSXany0xkcxVq8hctcqxwGDA0qQJHm1a49HGcUnTHF0XzWDAZkrA5H2oyPaOVpuj5adpcF+HSCkmqQZsmVYy/3AMSOB7c100Y9UoV6/uqmKVZCFJbqLaMQUE4NO7Nz69ewOgbDbyjh4tkvDyjhx1NKuuxW4nb/9+8vbvJ3XhIgAMfn54tG7N9qA02ljsHKmtkeWhoQo8UZltMWiOXd/XIdLZXUJUbRmrT6GsNtzCvfBoqf+4h65AKRtJyY7BGYIkuQlR/jSjEffGjXFv3JiAe+4BwJaRQe7u3UXu39lSU0u0P3taGll//EEz4PkLy04FQVJ0INa6x7A3a8GN/bsQVcunQn4fUb4KUvPI3HgGcExpI0O7lY+09B3k56dgMvni59dO73AuI8lNuCSjjw9eXbvi1dVR/KGUIj821tnnLmfHTnIPHgSbrUT7q5MEdZJOwNaPAMiZ5snJVq2K9L0zBQVV1K8jrkPGr7FQoDDX88XSOEDvcFxG4SXJoKBeGAxVL5VUvYiEqACapmGuWxdz3br4DRwIgD0nh9y9ey+27nbsoOB8ycbNtGdnk/3nn2RfMnWTW506F5JdDB4xbXBv0gStBF0RRMXJP59N1pZ44EKrrYoMDeUKkpKq7v02kOQmajCDhweeHTrg2aED4GjdFcTHs/XXhaz5aQ6NTivqx4O5ZI078k+dIv/UKdJ/+AEAzWzGvUWLoq27sDD5gK1E6StPgh3cmwZiifbTOxyXkZt7hszMA4CBoKCeeodTLEluQlygaRputWvzechB1t3kGBrOVKDomRPFi773kbtzl6MrwqlTJdqfslrJ2b6dnO3bnctMISFFkp17ixYYPGQesYpgPZNJzi5HH0XfW2Qi0vKUeGFiUj+/tri5Vc1LvZLchLjE8bTjrDtzsW9bgUmjV7/HCGr0t4vLEhPJ2bXr4kDRu3ehsnNKtP+Cc+fIWLmSjJUrHQtMJtybNCmS8NyioqR1Vw4Kp7TxaFMLc7j31VcWpXJxYtIbdY7kyiS5CXGJBQcWFPnZz+zHbfVuK7LMFByMz4034nOj4w9bFRQ4uiJcuG+Xs3Mn1mPHSnbAggJy9+4ld+9eUr78EgCjv3/R1l3r1hi95cO5NPJOpJF7MAUMjn5tovzYbDmkpKwHILiKTExaHEluQlyQYc1g2ZFlRZbd3fhu3P8ymepfaRdaX+5NmhAw5F4AbGlp5OzaXWTcTHt6eonisKWmkvn772T+/vuFA2hYGjbAvU0b59iZ5gYN0IqZC1E47p2mrTgBgFeHMNyC5bJveUpJ2Yjdnoe7JRwvr8Z6h3NFktyEuGDZkWVkF1ycFsegGRjSZEiZ9mX088O7R3e8e3QHcMx5d+LkhUTn6IqQd+hQiea8QynyDh8h7/AR0hZ/7YjN2xuP1q1wd46b2QZTQNW891HZcg+lYD2RDiYNn5tkpvTydnFi0hur9OVzSW5CAHZlv+yS5E1RN1Hbu3a57F8zGLDUr4elfj387xzsOGZWFjl79had8y4pqWTxZmaStX4DWes3OJe51Y0qMki0e5PGaG41ayJOZVfOe23eXcIx+cmg1uWp6MSkvfUN5hokuQkBrD29ltiMogMy39/0/go9psHLC6/OnfDq3AkonPPuTJFBonP37ivxnHf5J2PJPxlL+rffAaC5u+PeskXRWRFCQyrs96kKcvYkkn8mC81ixKd31RrI1xVkZh0kL+8sBoM7AQFd9A7nqiS5CQF8uf/LIj83CWhC+9D2lRqDpmmY60RgrhOB3+23A2DPyyNv/36yd1xMeAVnSjbnncrNJWfLVnK2bHUuM9WufUmya4N7i+YYLNWndXP69GmeeeYZfvzxR7Kzs2nYsCGfffYZHTp0QNkU6T+fBMCnRwRGr5rVaq0MSReqJAMDu2E0Xv1etN4kuYka76/l/wD3N7u/StxPMFgsF0Y8iXEuy084R86uCzMi7NhJzp49qNzcEu2v4OxZMs6eJWPFCscCkwn3Zs0cx7hQoekWEVElfve/SklJoVu3bvTp04cff/yRWrVqcfjwYQIu3GvM3pZAQWIOBk8T3t1lGqKKUDjrdlAVvyQJktyEKFH5f1XiFhqC280343vzzYBj3rq8w4eLjJtpPXmyZDsrKCB3925yd+8mZd6FOe+Cgoq07jxatcTg5VVRv06Jvfnmm0RGRvLZZ585l9Wr55hbT+XbSf/lQqutTyQGd/loK29WazJpaY4BCarqkFuXkneAqNHKWv5flWhubrg3b4578+YEDB0KOOa8y9216+K4mbt2Yc/MLNH+bElJZP76K5m/Oi5BYTBgadToYrJrG4M5OrrSuyJ8++239OvXj3vuuYfff/+diIgIRo8ezciRI8n88yy2NCtGPzPeN4RXalw1RVLyGkDh7d0Md/fyKbSqSJLcRI1WnuX/VYkpIADvXr3w7tULuNAV4dixIlMA5R0+XPI57w4eJO/gQVL/9z8ADL6+eLRufbGzeatWGP39K/A3gmPHjjFr1iwmTJjAc889x+bNm3nqqacwaUb6nW0GgM9NUWhu0v+vIlwclaTqt9pAkpuowSq6/L8q0QwGLA0bYmnYEP+77gLAlpl5+Zx3KSkl2p89PZ2stWvJWrvWucxcr16RkVUsjRqhmcrvI8Zut9OhQwdef/11ANq2bcuePXuY9c4Mbh44HVOwB17tQ8vteOIiuz2f5OQ1AAQHV90hty4lyU3UWHqU/1clRm9vvLp0wauLo6RbKUV+XFyRZJd74ECJuyJYjx/Hevw4aUuXAqB5euDR8pI571q3xlSrVpnjrV27Ns2bNy+yrEn9xvzvlOMLiu/NUWhGabVVhLS0bRQUZODmFoivb2u9wykRSW6ixqoK5f9ViaZpmKOiMEdF4TdgAAD23Fxy9+0jZ/sOZ8FKwblzJdqfys4he9Mmsjdtci5zi4go2rpr1gxDCee869atGwcPHiyybO/qbdTxCcWtthcercqeOMXVOUclCeqFphl1jqZkJLmJGqkql/9XJQZ3dzzbtcOzXTvnsvz4+IszIuzcSe7evai8vBLtL//0afJPnyZ9+XLgYjFMYbLzaNMGU3h4sf8PTz/9NF27duWZF17G0rgbh7dsYdnyBbzZ75/49otGM8j/XUVJTFwNVJ/7bSDJTdRQ1a38vypxCwvD7dYwfG/tBzjmrcs9eKjIINH5sbHX2IuDys93blPIVKtWkWTn3qIFBk9PYtq1Z/A/3+HdD/9DQcprhPjV5qUbx9GgxY1M2RPHlEb+uMllyXKXnX2S7OwjaJqJwMAeeodTYpLcRI3jCuX/VYlmNuPRqiUerVrCgw8AUJCURM6FyV1zdu4kd9cu7NnZ19iTQ8H582Ss/IWMlb84FhiNWJo0Zpd3HXxtQXQc8hJ41WK+5oMJjdFksXvrKdA0pt5VPe4HVSdJFzpu+/t1wM3NV+doSk6Sm6hx/lr+b9SMLlH+X5WYgoLwubEPPjc6LmMpm428I0eLjJtpPXK0ZDuz2cjbt58m7KfJhUX5bl5YA+txMiCa/Y26o4wmFm6OY3SfhkQGelbML1VDFc66HVSF524rjiQ3UaMUV/5/Y9SNLln+X5VoRiPuTRrj3qQxAfdemPMuPZ2c3budE7zm7NyFPS2tRPtzy8/ClrAHz5Tj5Ddx9OUzaBpLt59m3E2NKuz3qGkKCrJISfkTqNqzbhdHkpuoUWp6+X9VYvT1xbtbN7y7dQMcXRGsJ04UuXeXd/AQ2GxX3MeewLpwofhE0yAxs2SFLaJkUlLWoZQVD48oPD3r6R1OqUhyEzWKlP9XXZqmYalXD0u9evgPHgyAPTub3L17+eXrVZz/cxtNk08QmJfh3OZAQF3nc7tSBHtXnxkOqoOLc7dV7YlJiyPJTdQYUv5f/Rg8PfHs2JGY+i3o9e/fUEoRkpNC0+RYmqacZFtIY+e6SsHgtjIbQHlRyu6cBaC6jEpyKUluosaQ8v/qKyrIk/s6RrJwSxznPAM55xnImjoxztc1De7rECnFJOUoI2MvVut5jEYv/P076h1OqUlyEzWClP9Xf1MGtwRg4eY4DJqGpjkuRSrlSGyFr4vyUdhqCwzsjsFQslFkqhJJbqJGkPL/6s/NaGDqXa0Z3achS7efJjEzj1o+FgbFREiLrQIkOe+3Va8uAIUkuQmXJ+X/riUy0FPK/StYXt550jN2AdVj1u3iyFg1wuVJ+b8QpZN0oeO2r09rLJbqOSC1JDfh8qT8X4jSKbzfVt1GJbmUJDfh0qT8X4jSsdvzSE52TEJbXe+3gSQ34eKk/F+I0klN3YLNloXZXAsfnxZ6h1NmktyEy5LyfyFKLzHRMTFpcFAfNK36pojqG7kQ1yDl/0KUjlLKOet2cDW+3waS3ISLkvJ/IUovO/s4OTmxaJqZgIBueodzXSS5CZck5f9ClF5hqy0goDMmk5fO0VwfSW7CJUn5vxCld3FUkt76BlIOJLkJl3Ms7ZiU/wtRSgUFGaSmbQEgqBp3ASgkyU24nAX7pfxfiNJKSv4DpQrw9GyIp2fda29QxUlyEy4lw5rBt0e/LbJMyv+FuDZnF4Dg3voGUk4kuQmXIuX/QpSeUjaSkn4HHLNuuwJJbsJlSPm/EGWTnr6L/PxkTCYf/Pza6R1OuZDkJlyGlP8LUTaFlyQDA3tiMLjpHE35kOQmXIaU/wtRNokXpripzgMl/5UkN+ESiiv/f6DZA1L+L8Q15OaeJTNzH6ARFNRL73DKjSS3KmzmzJlER0fj7u5O586d2bRpk94hVVl/Lf/3t/jTv15/naIRovoonLvNz68tZnOgztGUH0luVdSiRYuYMGECkydPZtu2bbRp04Z+/fpx7tw5vUOrcoor/7+r0V1S/i9ECSS54CVJkORWZb3zzjuMHDmSESNG0Lx5c2bPno2npyeffvqp3qFVmjVr1jBgwADCw8PRNI2lS5cWeV0pxYsvvkhkRCSbR2zm+FvHyYvPk/J/IUrIZsslOdlxOT8o2DW6ABSS5FYFWa1Wtm7dSt++fZ3LDAYDffv2ZcOGDTpGVrmysrJo06YNM2fOLPb1t956i/fee4+GjzakwYsNMFgMnPjPCXqG9ZTyfyFKICV1I3Z7LhZLGN5eTfQOp1yZ9A5AXC4xMRGbzUZoaGiR5aGhoRw4cECnqCpf//796d+/+PtmSimmTZvG/WPv549Gf+COO3VG1uHAUwcIOxZWyZEKUT0lFg6UHHyjyxVfSctNVEvHjx8nPj6ec3Uu3oM0ehoJahJE/L54HSMTonpQSpHknHXbtS5JgiS3Kik4OBij0UhCQkKR5QkJCYSFSasEID7ekcD25O0psrxxVOPLzpsQ4nJZWYfIzTuDwWAhIOAGvcMpd5LcqiCz2Uz79u1ZtWqVc5ndbmfVqlV06dJFx8iqNn+LP2FekvyFKInCS5IBAV0xGj10jqb8yT23Kurhx8fw9ydHEmcIo0WbdpxYs5isrCxGjBihd2hVgnegNwAFaQW4+TuGC7qr0V0sObeEmJgYHSMTonoo7N/mal0ACklyq2LybXZeXLqHhYeD8Ov9CD98Pp1lWSmYQ+pz37/eJzC4lt4hVgk7bTsx+ZnI2peFR10PjJqR22rfxrN/PsuTTz6pd3hCVGn5+SmkpW0DIDhYkpuoBC8u3cPCLXEowLvdHXi3u8P52pp0x+tT72qtX4CVKDMzkyNHjjh/Pn78OCt+38DG07mssP2XoFuCOPfdOcxhZnq17sW/Rv+L8PBwBg8erF/QQlQDSUlrADve3k1xdw/XO5wKIcmtColNymbhZkdiA4jSEuhu2MPXth7kYUYpWLg5jtF9GhIZ6KlrrJVhy5Yt9Olz8VvlhAkTAPBp24G643MJvi0Ye56dM5+dYVHeInp078GKFStwd5eRSYS4msJLkkEuekkSJLlVKct2nMagadiUI709bvyeB02r6GzYz/j8sQAYNI2l208z7qZGeoZaKXr37o26cC4mfr3L0aJV4BH5KXAITdMI/VsowbfFMDDo37xxdxt9AxaiGrDbCy5OTOqilyRBqiWrlMTMPAr7UQaRxt3GNQAssF3sg6JpjvVqEmeLVoHBfA6T96Eir1uTu7JoyynikrOvsAchRKG09O0UFKTj5haAn2+M3uFUGEluVUiwt4ULDRUeNq3EXctnp70+G+3NnOsU2BW+7q4xmWBJFbZoAdwCig4/Zi/wpCA9xtmiFUJcXWHH7aDAXmiaUedoKo4ktypkUEwEdqXwIJeHjT8D8GHBHUDRYXGW7jjNntNpOkSoD2eL1pCLm9/WIq/lp3YC5VYjW7RClIWzC4ALX5IESW5VSlSQJ/d1jORe4+8EaJmctIewwt6pyDqebkbiUnL42wfr+WTtcec9KVdW2KI1mM+hlNm5XCmN/BTHyAp2pQj2tugVohDVQk5OHFlZh9E0I4GBPfUOp0KVKbmVdhLN1NRUxowZQ+3atbFYLDRu3Jjly5eXKWBXN2VgU8Z7Olptn9hux2AwYtAcbbehHSNZ/a/e3Nw8FKvNzivf7+ORzzeT5OItlsIWrT03iqzDz1KQ0RgAZQ1CFfg7nisY3DZCxyiFqPouTkzaATc3X52jqVilrpYsnERz9uzZdO7cmWnTptGvXz8OHjxISEjIZetbrVZuvvlmQkJCWLx4MREREZw8eRJ/f//yiN/luB38jsD8s9jcA6nd9VEeyNGo5WNhUEyEs/z/o4fa88XGk7zyw35+O3ie/tP/4N0hMXRrGKxz9BWjsEXrqJY0UZDdEJPPIWy5jv45mgb3dYisEd0jhLgeSYWzAAT11jeQSlDq5HbpJJoAs2fP5ocffuDTTz/l2WefvWz9Tz/9lOTkZNavX4+bm6MQIjo6+vqidlVKwbr3ADB2fpwn+7QqdjVN03ioSzQd6wUy9svtHDmXyYOf/MmTvRrw9M2NcTO63tXmKYNbAo5+fprN8Y3TYMpEw5HYCl8XQhSvoCCL5JSNgGOKG1dXqk/Bskyi+e2339KlSxfGjBlDaGgoLVu25PXXX8dms11f5K7o+Bo4uwNMHtBp5DVXbxrmy3djuzO0UxRKwQerj3LvhxtcsiTezWhg6l2tWfOvPtzV2lE9GuSXx5p/9WHqXa1dMqELUZ5SUtajlBUP9yg8PRvoHU6FK9UnwtUm0SycguSvjh07xuLFi7HZbCxfvpxJkybxn//8h1dfffWKx8nLyyM9Pb3Io0ZY72i10fYB8CrZJUYPs5Gpf2vFzPvb4eNuYntsKrdN/4Pvdp6pwED1ExnoycjuMQAUaGlyKVKIEnKOShLc2+UmJi1OhX/dtdvthISE8NFHH9G+fXuGDBnC888/z+zZs6+4zdSpU/Hz83M+IiMjKzpM/cXvgSO/gGaALmNKvfntrWvz4/getK8bQEZeAeMWbOeZxbvIthZUQLD6CvFw3NvNzM8kpyBH52iEqPocE5OuBlxzYtLilCq5lWUSzdq1a9O4cWOMxoudBZs1a0Z8fDxWq7XYbSZOnEhaWprzERcXV5owq6f17zv+bTYQAuuXaRd1AjxZ9PgNjLuxIZoGi7bEccf7a9l7xrX6xHm5eeFhcsw/lZidqHM0QlR9mZn7yLMmYDR6EhDQ6dobuIBSJbeyTKLZrVs3jhw5gt1udy47dOgQtWvXxmw2F7uNxWLB19e3yMOlpZ2CPYsdz7s9dV27MhkN/OOWJsx/rDOhvhaOnc/izpnr+Xyd6/SJq1evHpsf3Mye4XuI8otC0zQ0TWPMmNK3eIWoCRIvjEoSGNANg6Fm9Act9WXJCRMmMGfOHObOncv+/ft58skni0yi+fDDDzNx4kTn+k8++STJycmMHz+eQ4cO8cMPP/D666/LB9GlNs4CewFE94CI9uWyy64NgvlxfE/6NgvBarPz0nf7GPnfLSRnFd9ark42b97MPV/cQ5NpTZi/aT4rV64E4J577tE5MiGqpoujktSMS5JQhq4AQ4YM4fz587z44ovEx8cTExPDihUrnEUmsbGxGAwXc2ZkZCQ//fQTTz/9NK1btyYiIoLx48fzzDPPlN9vUZ3lpMLWzx3Pu15fq+2vAr3MzHm4A3PXn+D15Qf4Zf85+k9fw7tDYujaoPr2iatVqxZRdaLYX7Afu5ed7+d/T4MGDejVq5feoQlR5eRZE0lP3wVAUA3o31aoTFPejB07lrFjxxb72urVqy9b1qVLFzZu3FiWQ7m+rZ+BNRNqNYNGN5f77jVNY3i3enSqF8TYBds4dj6LBz7+k7F9GjL+pkaYqmkJfS0Px4zkZ9LP8MUXXzBhwoQaUQEmRGklJa0GFD4+LbFYLh9ow1VVz082V1GQ57gkCY57bRX44dw83Jfvx3VnSIdIlIL3fz3CkI82ciqlevaJq+XpSG4bftpAamoqw4cP1zcgIaqoi1WSrj1Q8l9JctPTrv9BZgL4hEPLuyv8cJ5mE2/e3Zr3h7bFx2Ji68kU+k//g+W7z1b4sctbYctt83eb6d+/P+Hh4TpHJETVY7dbSUr+A6hZ99tAkpt+7PaL5f83PAmm4itHK8KANuEsH9+DtlH+ZOQWMHr+NiZ+s5sca/UZNSbYIxhropXT20/z2GOP6R2OEFVSaupmbLZMzOZgfHxq1hB1ktz0cvgnSDwIFl9oP7zSDx8Z6Mn/RnVhdO8GaBos2BTLgBlrORBfPUaDqeVRi5Q/UnDzdeP222/XOxwhqqTEpNWAo5BE02rWx33N+m2rkgsDJNN+OLjr04/PzWjgX7c25YtHOxPiY+HIuUwGzljHvA0nqnyfuNwcT1LXpuLX1Y/3Vx90yfE0hbhehf3basqoJJeS5KaHuM0Qux4Mbo5Lkjrr1jCYH8f3oE+TWlgL7ExatpdR87aSml31+sTl2+xM/HoXfZ/+mPykfAJ6BjBzzXZ6vvUbE7/eRb7Nfu2dCFEDZGcfJyfnBJrmRmBgN73DqXSS3PSwfrrj39b3gm/VKIQI8rbw6fCOTLqjOW5GjZ/3JdB/+h/8eSxJ79CKeHHpHhZuicO9Xnuaf9QDS5gFuzEdBSzcEseLS/foHaIQVULihbnbAvw7YTJ56xxN5ZPkVtmSjsL+7x3Pu47TN5a/0DSNR7vXY8nobtQL9uJsWi5D52zk3ZWHKKgCLaLYpGwWbo6j8IqpKiic181xn1Apx3xvcolSCEhMclySDAquWV0ACklyq2zr3wcUNOoHIc30jqZYLSP8+H5cd+5uXwe7gumrDnP/nD85narvCPzLdpzGcElfwChbFgAdzFucywyaxtLtpys9NiGqkoKCDFJTNwM1r39bIUlulSnzHOz40vH8OgdIrmheFhNv39OG6ffF4G0xselEMrdN/4MVe4qft68yJGbmFennHmVztNA0Y6ZzmaY51hOiJktKXotSBXh61sfTM1rvcHQhya0ybfoIbHmOwZHrVo8bvINiIvjhqe60qeNHWk4+T3yxlReW7iY3v/L7xAV7W7i0iHNoRjLvJZwnP+3iYNN2pQj2rhmjngtxJUkX7rfV1FYbSHKrPNYs2Pyx43nXih1qq7zVDfLiqye6MqqXY565LzbGMmjGOg4lZFRqHINiIrBfyG4BpNOn4Bx9snM4lNfGuY5SMLhtRKXGJURVopT9klm3JbmJirb9C8hJgYB60GyA3tGUmtlkYGL/Zvz3kU4Ee1s4mJDBgPfXMv/Pk5XWJy4qyJP7OkaiadDacByAY/Yw0vECHN8X7usYSWSgZ6XEI0RVlJ6+i/z8ZIxGb/z9Ougdjm4kuVUGWwFsmOF43nUsGIxXX78K69m4Fj+O70HPxrXIK7Dz/JI9jJ6/jbTs/Eo5/pTBLbmvQyRttKMA7FYNMGigAfd1iGTK4Jo1xJAQf+VstQX1xGBw0zka/ZRpyhtRSvuWQmoseAZBzAN6R3PdavlY+Hx4Rz5Ze5y3fjrAj3vi2RmXyvShbekYHVihx3YzGph6V2tyMtPhOJij2jOhQWMGxURIi00I5H5bIWm5VTSlYN2FTtudRoGbh77xlBODQWNkz/p8/WRXooM8OZOWy5APN/DeqsPY7BV8mVIpPM7vBKB/v9sZe2MjSWxCALl58WRk7gU0goJq9uS9ktwq2vHfIX4XmDygo+uNXt+6jj/fP9WDv7WNwK7gnZWHuH/ORs6mVWCfuPQzjqmCNAOEta644whRzRTO3ebrG4PZHKRvMDqT5FbRCgdIbvsgeLnmm83bYuKdITG8c28bvMxG/jyeTP/pf/Dz3grqE3dmm+PfWs3ALC02IQoV3m8LrsFVkoUkuVWk+N1wdJWjhdFljN7RVLi/tavD90/1oFWEH6nZ+Tw+byuTl+0p/z5xZ7Y7/o1oW777FaIas9nySE5eB8j9NpDkVrEKJyNtPggC6+kbSyWpF+zF1092ZWQPx+87d8NJBs9cx5Fz5dgn7vSFllt4u/LbpxDVXGrqRuz2HCyWMLy9q+bQfpVJkltFSY2DPV87nnet2kNtlTezycDztzfn8xEdCfY2cyA+gzveX8vCTbHX3ydOqUtabpLchCh0sQtAb7RqNEhERZHkVlE2zgJ7AUT3qLEfwr2bhLB8fA96NAomN9/Os9/sZuyC7aTllLxP3EsvvYSmaRcfBgNN3z4FRjOEtKjA6IWoPpRSzilugoNr3sSkxZHkVhFyUmDbXMfzbuP1jUVnIT7uzB3RiYn9m2IyaPyw6yy3Tf+DrSdTSryPFi1acPbsWcfj149Y+4gnhLUCk7kCIxei+sjKOkxu7ikMBguBAV31DqdKkORWEbZ8CtZMCGkODfvqHY3uDAaNUb0asPjJrkQFenI6NYd7P9zAzN+OlKhPnMlkIiwszPHIO06wp0HutwlxiaQLlyQDAm7AaHSNvrTXS5JbeSvIgz8/dDyvZgMkV7SYSH9+eKo7A9uEY7Mr/v3TQR78+E8S0nOvut3hw4cJDw+nfv36PPDS58Sm2WvspV4hiuO8JBkklyQLSXIrb7sWOToY+0ZAy7v0jqbK8XF3Y/p9Mbx9Txs8zUY2HEvi1mlrWLU/odj1O3fuzOeff86KFSuYNWMGx88k0uOzLDJ8G1dy5EJUTfn5qaSlOyqIg6QLgJMkt/Jkt1/stH3Dk3JP6Ao0TePu9nX4blx3WoT7kpKdz6Nzt/DSt3vJKyjaJ65///7cc889tG7dmn7to1l+vzupufC/Vdt1il6IqiUpaQ1K2fDyaoyHh0z3VEiSW3k6tAKSDoPFF9oN0zuaKq9BLW++Gd2VR7o5+sR9vv4Ed85cz9HzmcVvcGYb/u4ajWv7cuTYsUqMVIiq6+KoJHJJ8lKS3MpT4QDJHR4Bd199Y6kmLCYjLw5ozqfDOxDoZWbf2XTueG8t/9sSd3mfuNPbyLQqjibmUbt2bX0CFqIKsdsLSEpaA8ioJH8lU96Ul7hNELcRDG7Q+Qm9o6l2bmwayo/je/D0oh2sP5rEvxbvYu3hRNSf8+h5Uz/2pptpu24FM37KxmDyZOjQoXqHLITu0tN3UFCQisnkj69vjN7hVCmS3MpLYaut9RDwlVZFWYT6ujPv0c58uOYo//n5EN/uPEPKL1uZ9clc7DnphHva6B5lpN6Q53hnzVmmDA7CzSgXH0TNVXRiUvk4v5ScjfKQeAQO/OB43nWcvrFUc0aDxujeDbmhfhAPffwn3PEvAoDW2lG+tUwiWXnTLq8lC7fEATD1LpnyRtRciYm/AnJJsjjytbc8bHgfUND4Vghpqnc0LiHYy0KW9WLlZGuDo4Bkt70+oKEULNwcR1xytk4RCqGvnJzTZGUdQtOMBAX11DucKkeS2/XKPAc7Fjie1/ChtsrTsh2nMV7SAb6NdhSAXaq+c5lB01i6/XSlxyZEVVA4Komfbzvc3Pz1DaYKksuS1+vPD8GWBxEdIKqL3tG4jMTMPMfgLhcKJn+ydyS3wMwftlbOdTTNsZ4QNVFi0oVLkjIxabEkuV2PvEzY/LHjeTcZaqs8BXtbuLQnwC/29vxib19kHbtSBHtbKjkyIfRns2WTkrIBkFFJrkQuS16P7fMgNxUC60PTO/SOxqUMionAfo2535SCwW1lRAZR8ySnbMBut+LuXgcvr0Z6h1MlSXIrK1sBbPjA8bzLWDAY9Y3HxUQFeXJfx8grNoY1De7rGElkoGflBiZEFeCskgzuIxOTXoFcliyrfUshLRY8gyHmfr2jcUlTBrcEHFWRBk1D0xyXIpWC+zpEOl8XoiZRSpGUtBqQLgBXI8mtLJSCddMczzuPAjeZP6kiuBkNTL2rNaP7NGTp9tMkZuZRy8fCoJgIabGJGiszcz95efEYDB74+9+gdzhVliS3sji2GuJ3g5sndHxM72hcXmSgJ+NukvsKQsDFUUkCA7thNEpB1ZXIPbeyKBxqq+1D4BmobyxCiBrl4sSkcknyaiS5ldbZXXDsN9AM0GW03tEIIWoQqzWJ9PQdAAQF99Y1lqpOkltprX/f8W/zwRAQrWckQogaJinpd0Dh490Cd0uY3uFUaZLcSiM1FvZ87Xje7Sl9YxFC1DjOWQBkVJJrkuRWGhtngbJBvZ4Q3lbvaIQQNYjdni8Tk5aCJLeSykmBrXMdz2WAZCFEJUtN24LNlombWyC+vjLV07VIciupzZ9AfhaEtoQGN+kdjRCihklyVkn2RtPko/ta5AyVRH6uY/R/cExGKsPdCCEq2cX7bTfqHEn1IMmtJHYthKxz4FsHWt6ldzRCiBomO/sE2dnH0DQTQYHd9Q6nWpDkdi12O6yf4Xh+w5NgdNM3HiFEjVPYavP374jJ5KNzNNWDJLdrOfQjJB0Gix+0H6Z3NEKIGuji/Ta5JFlSktyupXCorY6PgEW+MQkhKldBQSYpqZsAmXW7NCS5XU3sRoj7E4xm6PyE3tEIIWqg5OR1KJWPh0c0np719A6n2pDkdjXr3nP823oI+MhQN0KIypeYVDgxqVySLA1JbleSeBgOLnc87zpO31iEEDWSUvZLJibtrWss1Y0ktytZ/z6goHF/qNVE72iEEDVQRsYerNZEjEZv/P076h1OtSLJrTgZCbBzgeO5DLUlhNBJ4dxtgYHdMRjMOkdTvUhyK86mD8FmhTodIUqmcRdC6OPi/TapkiwtSW5/lZcBmz92PO82XobaEkLoIi/vHBkZewCNILnfVmqS3P5q2zzITYPABtDkNr2jEULUUIWFJL6+rbGYg/UNphqS5HYpWz5s/MDxvOtYMBj1jUcIUWMlJl64JClzt5WJJLdL7V0CaXHgVQvaDNU7GiFEDWW355Gcsg6QWbfLSpJbIaUudtruNArcPPSNRwhRY6WkbMJmy8ZiDsXHu4Xe4VRLktwKHfsNEnaDmyd0fFTvaIQQNVhhlWRQcG80KWorE0luhQoHSG73MHgG6huLEKLGUko5+7fJ/bayk+QGcHYnHFsNmhFuGK13NEKIGiw7+yi5uXEYDGYCArrqHU61JckNLt5ra3EnBNTVNxYhRI12cWLSzphMXjpHU31JckuNdVRJAnR7St9YhBA1nvOSpMwCcF0kuW34AJQN6vWC2m30jkYIUYPl56eRlrYFkPtt16tMyW3mzJlER0fj7u5O586d2bRpU4m2W7hwIZqmMXjw4LIctvxlJ8O2uY7nMkCyEEJnycl/oJQNL69GeHhE6h1OtVbq5LZo0SImTJjA5MmT2bZtG23atKFfv36cO3fuqtudOHGCf/7zn/To0aPMwZa7LZ9AfjaEtoIGcglACKEvqZIsP6VObu+88w4jR45kxIgRNG/enNmzZ+Pp6cmnn356xW1sNhsPPPAAL7/8MvXr17+ugMtNfi78+aHjebenZIBkIYSulLKRlPw7AEGS3K5bqZKb1Wpl69at9O3b9+IODAb69u3Lhg0brrjdlClTCAkJ4dFHS9Y5Oi8vj/T09CKPcrdzAWSdB986jipJIYTQUVr6DvLzUzCZfPHza6d3ONVeqZJbYmIiNpuN0NDQIstDQ0OJj48vdpu1a9fyySefMGfOnBIfZ+rUqfj5+TkfkZHXvva8Zs0aBgwYQHh4OJqmsXTp0iKvf/PNN9xyyy0EBQWhaRo7vnrL8UKX0WB0K3FsQghREQovSQYF9cJgMOkcTfVXodWSGRkZPPTQQ8yZM4fg4JJP2TBx4kTS0tKcj7i4uGtuk5WVRZs2bZg5c+YVX+/evTtvvvmmY0FaHLj7OUYkEUIInSUlyf228lSqrwfBwcEYjUYSEhKKLE9ISCAsLOyy9Y8ePcqJEycYMGCAc5ndbncc2GTi4MGDNGjQ4LLtLBYLFoulNKHRv39/+vfvf8XXH3roIcBR2OLU4VGw+JTqOEIIUd5yc8+QmXkAMBAU1FPvcFxCqVpuZrOZ9u3bs2rVKucyu93OqlWr6NKly2XrN23alN27d7Njxw7nY+DAgfTp04cdO3aU6HJjuTu93fGvwQ06P1H5xxdCiL9IvDAxqZ9fW9zcAvQNxkWU+sLuhAkTGDZsGB06dKBTp05MmzaNrKwsRowYAcDDDz9MREQEU6dOxd3dnZYtWxbZ3t/fH+Cy5ZVm64Wqzsb9wCf06usKIUQluDgxqXRJKi+lTm5Dhgzh/PnzvPjii8THxxMTE8OKFSucRSaxsbEYDFV04JPzhxwDJAO0lslIhRD6s9lySElZD0CwTExabspUkjN27FjGjh1b7GurV6++6raff/55WQ5ZPta/d/F5oAyQLITQX0rKRuz2PNwt4Xh5NdY7HJdRRZtYFSAjHnYt0jsKIYQo4uLEpDfKxKTlyGU6U2RmZnLkyBHnz8ePH2fHjh3kGjzYfN5Ave1vUud0DidNDYGdHDx4EICwsLBiKz2FEKKiFZ2YtLe+wbgYl0luW7ZsoU+fi9erJ0yYAIBXy5uof8eTPLnvKwZ8mwXsBOC+++4DYPLkybz00kuVHa4QQpCZdZC8vLMYDO4EBFxecS7KzmWSW+/evVFKOX+e+PUuFm6JQym4x7CcJ9vCLW2a0Nf6b9AM3Nchkql3tdYxYiFETZd0oUoyMLAbRqO7ztG4Fpe85xablM3CzY7EZqKAR0w/AjDHdjsKA0rBws1xxCVn6xypEKImK5x1O0guSZY7l0xuy3acxnDhxuwdho1EaEmcV34ssXV3rmPQNJZuP61XiEKIGs5qTSYtzTGohAy5Vf5c5rLkpRIz8xwz2ChIxYv99ki+t3UhD7NzHU1zrCeEEHpISl4DKLy9m+HuXlvvcFyOSya3YG8LhbffVtvbstoagxu2IuvY7Ipg79KNXymEEOXl4qgk0mqrCC55WXJQTAT2S4pLQCP/L3lcAXe0lm9LQojKZ7fnk5y8BoDgYBlyqyK4ZHKLCvLkvo6R15xce+KS3aRkWSsnKCGEuCAtbRsFBRm4uQXi6ytV2xXBJZMbwJTBLbmvQyQaYNQ0TAYNgwYa0LNRMF5mIxuPJTNo5joOJ2ToHa4QogZxjkoS1AtNM+ocjWtyyXtuAG5GA1Pvas3oPg1Zuv00iZl51PKxMCgmgshATw7GZ/Do3M3EJmfztw/W8979benTJETvsIUQNUBi4mpA7rdVJE2pIjenqqT09HT8/PxIS0vD19e33PablJnHk/O3sel4MgYNnrutGY92ryfjuwkhKkx29kk2bLwRTTPRo/tm3NzK7zPN1ZUmF7jsZcmSCPK28MWjnRnSIRK7gld/2M8zX+/CWmDXOzQhhItKutBx29+vgyS2ClSjkxuA2WTgjbta8eIdzTFo8L8tp3jw4z9Jkj5wQogKUDjrdpDM3VahanxyA9A0jUe61+PT4R3xsZjYdCKZgTPWcSA+Xe/QhBAupKAgi5SUPwGZdbuiSXK7RO8mISwZ05W6QZ6cTs3hrg/W88u+BL3DEkK4iJSUdShlxcMjCk/PenqH49Ikuf1FwxAflo7uRtcGQWRZbYyct4XZvx+lGtTdCCGquItzt8nEpBVNklsxArzMzH2kEw90jkIpeOPHA/zjq53k5tuuvbEQQhRDKbtzFgAZlaTiSXK7AjejgdfubMUrg1pgNGh8s+0098/ZyLmMXL1DE0JUQxkZe7Faz2M0euHv31HvcFyeJLdreKhLNHNHdMLX3cS22FQGz1jH3jNpeoclhKhmClttgYHdMRjM11hbXC9JbiXQvVEwS8d0o36wF2fScrl71gZW7Dmrd1hCiGokyXm/TboAVAZJbiVUv5Y3S0Z3o0ejYHLybTzxxTZm/HpYCk2EENeUl3ee9IxdgMy6XVkkuZWCn6cbnw3vyPCu0QC8/fMhxi/cIYUmQoirSrrQcdvXpzUWSy19g6khJLmVkslo4KWBLXj9zlaYDBrf7jzDkA83kJAuhSZCiOIV3m+TUUkqjyS3Mrq/cxTzHu2Mv6cbO0+lMWjGOnafkkITIURRdnseyclrAbnfVpkkuV2HLg2CWDamGw1DvIlPz+WeD9fz/a4zeoclhKhCUlO3YLNlYTbXwsenhd7h1BiS3K5T3SAvvhndlT5NapGbb2fsl9t5d+Uh7HYpNBFCQGKiY2LS4KA+aJp85FYWOdPlwNfdjY+HdWRkD8dYcdNXHWbsgm3kWKXQRIiaTCnlnHU7WO63VSpJbuXEaNB4/vbmvHVXa9yMGst3x3PPh+s5m5ajd2hCCJ1kZx8nJycWTTMTENBN73BqFElu5ezejpF8OfIGAr3M7DmdzsAZ69gem6J3WEIIHRS22gICOmMyeekcTc0iya0CdIwOZNmYbjQJ9eF8Rh5DPtrIsh2n9Q5LCFHJLo5K0lvfQGogSW4VJDLQk69Hd6VvsxCsBXbGL9zBv386IIUmQrigqVOn0rFjR3x8fAgJCWHw4MHs3buN1LQtAARJF4BKJ8mtAnlbTHz4UAee7N0AgJm/HeWJL7aSlVegc2RCiPL0+++/M2bMGDZu3MjKlSvJz8+n3623kp1txdOzIZ6edfUOscbRVDUYHDE9PR0/Pz/S0tLw9fXVO5wy+WbbKZ79ejdWm52mYT58PKwDdQI89Q5LCFEBzp8/T0hICO+8W5s77hhHo4YT9Q7JJZQmF0jLrZL8rV0dFjx+A8HeFg7EZzB45jq2nkzWOywhRAVITXX8bfv4GAkOkolJ9SDJrRK1rxvAsrHdaF7bl8RMK0M/+pPFW0/pHZYQohzZ7XbGjXuMFi0tNGoUhJ9fO71DqpEkuVWyCH8PFj/ZhX4tQrHa7Pzzq51MXb4fmxSaCOESxowZw549e3nhhVACA3tiMLjpHVKNJMlNB55mE7MeaM9TNzYE4MM1x3j8v1vIyM3XOTIhxPUYO3Ys33//Pe/PaEutWiYZKFlHktx0YjBoTLilCe8NbYvFZGDVgXPcNWs9sUnZeocmhCglpRRjx45lyZIl/PjjQnx9TwAaQUG99A6txpLkprOBbcJZNKoLIT4WDiVkMmjmWv48lqR3WEKIUhgzZgxffPEF02Z/ym/7V5GcXMDppHqcPC+3G/QiXQGqiPi0XEb+dwu7T6dhMmi8Orgl93WK0jssIUQJaJpW7PKg2/7OyEdGMGVwS9yM0pa4XqXJBZLcqpAcq41/Lt7JD7vOAvBIt3o8d1tTTPJHIUSVNvHrXSzcEodJszK9z0Qsxnwmr3+GU5kRaBrc1yGSqXe11jvMak/6uVVTHmYjM4a2ZcLNjQH4dN1xHp27hXQpNBGiyopNymbh5jiUgnqhcbxn/D+O5UZzKjMcAKVg4eY44pLlfnplkuRWxWiaxlM3NeKDB9rh7mbg90PnuXPmOk4kZukdmhCiGMt2nMagadg9jRxr3oYdWntmMw64eKnSoGks3S6Dp1cmSW5V1G2tarP4ia6E+bpz9HwWg2auY/2RRL3DEkL8RWJmHnYfE9ZOtcg0+eGbn4LfoTNF1tE0x3qi8khyq8JaRvjx7dhuxET6k5aTz0OfbmLexpN6hyWEuESet4mcDsFgMaKlW8lbm8ux+KIDJduVItjbolOENZMktyouxNedhY/fwOCYcGx2xaSle3hx2R4KbHa9QxOixluTnMEiUx64GdBS8jBvTkSzXv63qRQMbhuhQ4Q1lyS3asDdzci7Q2L4v35NAPjvhpMM/2wzadlSaCKEXpafT+XBXcfIsSvC88GyLQmt4PLic02D+zpGEhkos4BUJklu1YSmaYzp05APH2qPp9nI2iOJDP5gHUfPZ+odmhA1zsKzSTy25wRWpbijlh9r+rRkaNs6aIBR0zAZNAyao6Tkvg6RTBncUu+Qaxzp51YN7TuTzsj/buF0ag4+7iZm3t+Ono1r6R2WEDXCnLjzTDriqHwcWjuQt5tEYrzQiTsuOZul20+TmJlHLR8Lg2IipMVWjqQTdw2QmJnHE/O2suVkCkaDxqTbmzGsa/QVR0oQQlwfpRRvn4jnPycSABgVWYuXGoTL31wlkk7cNUCwt4X5IztzV7s62OyKl77bx3NL9pAvhSZClDu7Urx45LQzsT1TL0wSWxUnya0as5iMvH1Pa567rSmaBgs2xfLQJ3+SkmXVOzQhXEaBXfH0gTjmnHL0M32tUQRPR4dJYqviJLlVc5qm8XjPBnz8cAe8LSY2Hktm0Mx1HE7I0Ds0Iaq9PLudx/eeYFF8MkYN3m8WxaN15P52dSDJzUXc1CyUb0Z3JTLQg9jkbO78YD2/HTind1hCVFtZBTYe2nWM5YlpWAwan7Soxz1hgXqHJUpIkpsLaRzqw7Ix3elUL5DMvAIenbuZj/84RjWoGRKiSknJL+DenUdZk5KJl9HA/Nb1ubWWn95hiVKQ5OZiAr3MfPFoZ+7rGIldwas/7Odfi3eRV2DTOzQhqoWEvHzu3H6ErenZBJiMfBXTgO4BPnqHJUpJkpsLMpsMTP1bK168ozkGDb7aeooHP/5TBm4V4hpO5uQxaPthDmTlEmo28U3bhrTz9dI7LFEGktxclKZpPNK9Hp+N6ISPu4nNJ1IYNGMdB+LT9Q5NiCrpYFYug7Yd4USOlSh3M9+2a0Qzbw+9wxJlJMnNxfVqXIslo7sRHeTJ6dQc7vpgPSv3JegdlhBVyo70bO7cfph4az5NvNz5tl0j6nrIKP7VmSS3GqBhiDdLx3Sja4Mgsqw2Hp+3hVmrj0qhiRDAupQM7t5xhOR8G219PFnStiFhFje9wxLXSZJbDeHvaWbuI5148IYolII3VxzgH//bSW6+FJqImuvnxDTu33WMTJud7v7efBXTgEA3k95hiXIgya0GcTMaeHVwK14Z1AKjQeOb7acZOmcj5zJy9Q5NiEr3dXwyI/YcJ8+uuDXYly9a18fbZNQ7LFFOJLnVQA91iWbuiE74upvYHpvK4Bnr2HM6Te+whKg0n51OZOz+WGwK7gkL4OMW9XA3ysehK5H/zRqqe6Nglo3tTv1aXpxJy+We2RtYsees3mEJUaGUUkw/kcDEQ6dQwKMRwUxvGoXJIONEuhpJbjVYvWAvlozuRo9GweTk23jii228v+qwFJoIl6SUYsrRM0w97vgS93TdUF5tFIFBBkB2SZLcajg/Dzc+G96R4V2jAfjPykM8tXCHFJoIl2JTin8ejGNW3HkAXm4YzjP1a8vI/i5MkpvAZDTw0sAWvH5nK0wGje92nmHIhxtISJdCE1H9We12nth7kvlnkzEA7zSNZFRkiN5hiQpWpuQ2c+ZMoqOjcXd3p3PnzmzatOmK686ZM4cePXoQEBBAQEAAffv2ver6Qj/3d45i3qOd8fd0Y+epNAbOWMuuU6l6hyVEmWXb7AzbfZzvzqfipml81CKa+2sH6R2WqASlTm6LFi1iwoQJTJ48mW3bttGmTRv69evHuXPFT6+yevVqhg4dym+//caGDRuIjIzklltu4fTp09cdvCh/XRoEsWxMNxqFeJOQnsc9szfw3c4zeoclRKml5Rdw386j/JacgYfBwLzW9bgjxF/vsEQl0VQpqwc6d+5Mx44dmTFjBgB2u53IyEjGjRvHs88+e83tbTYbAQEBzJgxg4cffrhEx0xPT8fPz4+0tDR8fX1LE64oo4zcfJ5asJ3fDjruUTx1UyP+flMjDFJVJqqB89Z8hu48xp7MHPxMRr5oXZ+OfjIAcnVXmlxQqpab1Wpl69at9O3b9+IODAb69u3Lhg0bSrSP7Oxs8vPzCQyUSf+qMh93Nz4e1pGRPeoB8N6qw4z5chvZ1gKdIxPi6k7lWhm87Qh7MnMIdnOM7C+JreYpVXJLTEzEZrMRGhpaZHloaCjx8fEl2sczzzxDeHh4kQT5V3l5eaSnpxd5iMpnNGg8f3tz3rq7NW5GjR/3xHPP7A2cTcvROzQhinUkO5dB2w5zNCePCIsb37ZrRAsZ2b9GqtRqyTfeeIOFCxeyZMkS3N3dr7je1KlT8fPzcz4iIyMrMUrxV/d2iOTLkTcQ6GVm75l0Bs5Yx/bYFL3DEqKI3RnZDNp2hNN5+TTytPBdu0bU95SR/WuqUiW34OBgjEYjCQlFp0xJSEggLCzsqtu+/fbbvPHGG/z888+0bt36qutOnDiRtLQ05yMuLq40YYoK0DE6kGVjutE0zIfzGXkM+WgjS7dLUZCoGjamZvK37UdIyi+gtbcHS9o2ItzdrHdYQkelSm5ms5n27duzatUq5zK73c6qVavo0qXLFbd76623eOWVV1ixYgUdOnS45nEsFgu+vr5FHkJ/kYGeLH6yK32bhWItsPP3RTt4a8UB7HYZ0UTo59ekdIbuPEqGzc4Nfl4sbtuQYLOM7F/Tlfqy5IQJE5gzZw5z585l//79PPnkk2RlZTFixAgAHn74YSZOnOhc/80332TSpEl8+umnREdHEx8fT3x8PJmZmeX3W4hK420x8dFD7XmydwMAPlh9lFFfbCUrTwpNROVbdi6FYbuPk2NX9A3yZUGbBvjKyP6CMiS3IUOG8Pbbb/Piiy8SExPDjh07WLFihbPIJDY2lrNnLw7AO2vWLKxWK3fffTe1a9d2Pt5+++3y+y1EpTIYNJ65tSnv3NsGs9HAyn0J3DVrPadSsvUOTdQgX5xJ4om9J8lXijtD/PmsZT08ZGR/cUGp+7npQfq5VV3bYlN4/L9bSczMI8jLzIcPtadDtHTzEBVrZuw5XjnqGFzg4fAgpjaug1HGiXR5FdbPTYi/ahcVwLKx3Whe25ekLCtD52zkqy1SACQqhlKK14+ecSa2cVEhvCmJTRRDkpu4bhH+Hix+sgu3tggj36b4v8W7eH35fmxSaCLKkV0pnj10ivdiHUP9PV+/Ns83CJeR/UWxJLmJcuFpNvHBA+146saGAHy05hgj/7uFjNx8nSMTriDfrhi7P5a5Z5LQgLca12Fc3dBrbidqLkluotwYDBoTbmnCe0PbYjEZ+PXAOe6atZ7YJCk0EWWXY7PzyJ7jfJOQgkmDWc3r8nBEsN5hiSpOkpsodwPbhPO/UV0I8bFwKCGTQTPXsvFYkt5hiWooo8DG/buOsjIpHXeDxuet6jM4NEDvsEQ1IMlNVIg2kf58O7Y7rSL8SMnO58GP/2TBpli9wxLVSJK1gLt3HGFDahY+RgML2zSgb5BUS4uSkeQmKkyYnzv/G9WFO1rXpsCumPjNbl7+bi8FNrveoYkq7myelcHbD7MzI4dANyNft23IDf7eeoclqhFJbqJCeZiNvD+0LRNubgzAZ+tO8MjcLaTlSKGJKN7x7DwGbjvC4ew8wi1uLGvbiNY+nnqHJaoZSW6iwmmaxlM3NeKDB9rh7mZgzaHz3PnBOo4nZukdmqhi9mXmMHD7YeJyrdTzMLOsXSMaeV15BhEhrkSSm6g0t7WqzeInulLbz51j57MYPHMd644k6h2WqCK2pGVx5/YjnLcW0MLbnW/bNSJSRvYXZSTJTVSqlhF+LBvTjZhIf9Jy8nn4003M23BC77CEzn5PzuCeHUdJK7DR0deLb2IaUsvspndYohqT5CYqXYivOwsfv4HBMeHY7IpJy/Yyaeke8qXQpEb64XwqD+06Ro7dTu8AHxbG1MfPTaasEddHkpvQhbubkXeHxPCvW5ugaTBv40mGf7aJ1Gyr3qGJSrTwbBIj95zAqhR31PJjbut6eBllyhpx/SS5Cd1omsbo3g358MH2eJqNrDuSxOCZ6zhyTub6qwnmxJ3n7wfisAP31w7kwxbRWAzykSTKh7yThO5uaRHG4ie6EuHvwYmkbO78YB1rDp3XOyxRQZRS/Pv4WSYdOQ3AqMha/KdJpIzsL8qVJDdRJTQP92XZ2G50qBtARm4Bwz/bxGfrjlMNphsUpWBXiklHTvOfEwkAPFMvjJdkZH9RASS5iSoj2NvC/JGdubt9HewKXv5uH88t2Y21QApNXEGBXfH3A7F8fMrR/eO1RhE8HR0miU1UCEluokqxmIz8++7WPH9bMzQNFmyK46FP/iQ5SwpNqrNcm52Re0/wv/gUjBq83yyKR+vU0jss4cIkuYkqR9M0RvaszyfDOuBtMfHn8WQGz1zHoYQMvUMTZZBVYOOh3cf4MTENi0Hjkxb1uCcsUO+whIuT5CaqrBubhvLN6K5EBnoQm5zN3z5Yz68HEvQOS5RCSn4B9+w8yh8pmXgZDcxvXZ9ba/npHZaoASS5iSqtcagPy8Z0p1O9QDLzCnh07hbmrDkmhSbVQEJePnduP8K29GwCTEa+imlA9wAfvcMSNYQkN1HlBXqZ+eLRztzXMRKl4LXl+/nX4l3kFdj0Dk1cwcmcPAZtP8yBrFxCzSaWtGtIO18vvcMSNYgkN1EtmE0Gpv6tFS/e0RyDBl9tPcUDc/4kMTNP79DEXxzMymXQtiOcyLFS193Mt+0a0dTLQ++wRA0jyU1UG5qm8Uj3enw2ohM+7ia2nExh0Ix17D+brndo4oLt6dncuf0w8dZ8mni5s6xdI+p6WPQOS9RAktxEtdOrcS2WjO5GdJAnp1NzuGvWen7eG693WDXeupQM7t5xhOR8G219PFnStiFhFhnZX+hDkpuolhqGeLN0TDe6Nggi22pj1Bdb+WD1ESk00cnPiWncv+sYWTY73f29+SqmAYEysr/QkSQ3UW35e5qZ+0gnHrwhCqXgrRUHmfC/neTmS6FJZVocn8yIPcfJsytuDfbli9b18TbJyP5CX5LcRLXmZjTw6uBWvDKoBUaDxpLtp7nvo42cy8jVO7Qa4dNT5xm7PxabgnvCAvi4RT3cjfKxIvQn70LhEh7qEs1/H+mEn4cbO+JSGTRjHXtOp+kdlstSSjH9RALPHXaM7P9oRDDTm0ZhMsg4kaJqkOQmXEa3hsEsHdON+rW8OJuWyz2zN/Dj7rN6h+VylFJMOXqGqccd53ZCdCivNorAIAMgiypEkptwKfWCvVgyuhs9GgWTk2/jyfnbeG/VYSk0KSc2pfjnwThmxTnm23u5YTj/qldbRvYXVY4kN+Fy/Dzc+Gx4R4Z3jQbgnZWHGLdguxSaXCer3c4Te08y/2wyBuCdppGMigzROywhiiXJTbgkk9HASwNb8PqdrTAZNL7fdZZ7P9xAQroUmpRFls3GsN3H+e58Km6axkctorm/dpDeYQlxRZLchEu7v3MUXzzWmQBPN3adSmPgjLXsjEvVO6xqJS2/gKE7j/FbcgYeBgPzWtfjjhB/vcMS4qokuQmXd0P9IJaN6U6jEG8S0vO498MNfLfzjN5hVQvnrfncteMom9Ky8DMZ+V9MA3oH+uodlhDXJMlN1AhRQZ58M7orfZrUIq/AzrgF23nn54PY7VJociWncq0M2naEPZk51DKbWNK2IR39ZGR/UT1IchM1ho+7Gx8P68jIHvUAeO/XI4z5chvZ1gKdI6t6jmTnMmjbYY7l5FHH3Y1lbRvR3FtG9hfVhyQ3UaMYDRrP396ct+5ujZtR48c98dwzewNnUnP0Dq3K2J2RzaBtRzidl08jTwvftm1EfU8Z2V9UL5LcRI10b4dIFoy8gSAvM3vPpDNwxjq2xaboHZbuNqZm8rftR0jKL6C1twdL2jYi3N2sd1hClJokN1FjdYgOZOmYbjQN8yExM4/7PtrIku2n9A5LN6uS0hm68ygZNjs3+HmxuG1Dgs0ysr+oniS5iRotMtCTxU92pW+zUKwFdp5etJM3VxyocYUmSxNSGLb7GDl2Rd8gXxa0aYCvjOwvqjFJbqLG87aY+Oih9jzZuwEAs1Yf5fF5W8nMqxmFJl+cSeLJfScpUHBniD+ftayHh4zsL6o5eQcLARgMGs/c2pR3h7TBbDLwy/4E7p61nlMp2XqHVqFmnEzgnwfjUMDD4UHMaF4XNxnZX7gASW5CXOLOtnVY+PgNBHtbOBCfwaAZ69h8IlnvsMqdUorXjp7h1WOOkf2figrhzcZ1MMoAyMJFSHIT4i/aRQXw7dhuNK/tS1KWlfvnbOSrLXF6h1Vu7ErxzKFTvB97DoDn69fmuQbhMrK/cCmS3IQoRri/B4uf7EL/lmHk2xT/t3gXr/2wD1s1LzTJtyvG7o/lv2eS0IC3GtdhXN1QvcMSotxJchPiCjzNJmbe346nbmwIwJw/jvPY3M1k5ObrHFnZ5NjsPLLnON8kpGDSYFbzujwcEax3WEJUCEluQlyFwaAx4ZYmvD+0LRaTgd8OnudvH6wnNql6FZpkFNi4f9dRVial427Q+LxVfQaHBugdlhAVRpKbECUwoE04/xvVhRAfC4fPZTJo5lo2HE3SO6wSSbIWcNeOI2xIzcLHaGBhmwb0DZKR/YVrk+QmRAm1ifTn27HdaV3Hj5TsfB765E8WbIrVO6yrOpNrZfD2w+zKyCHQzcjXbRtyg7+33mEJUeEkuQlRCmF+7ix6vAt3tK5NgV0x8ZvdvPTtXgpsdr1Du8zx7DwGbj/M4ew8wi2Okf1b+3jqHZYQlUKSmxCl5GE28v7Qtky4uTEAn68/wYjPN5OWU3UKTfZl5jBw+2FO5eZT38PCsnaNaOTlrndYQlQaSW5ClIGmaTx1UyNmPdAODzcjfxxO5M4P1nE8MUvv0NiSlsWd249w3lpAC293lrVrSKSM7C9qGEluQlyH/q1q89UTXajt586x81kMnrmOtYcTdYvn9+QM7tlxlLQCGx19vfgmpiG1zG66xSOEXiS5CXGdWkb4sWxsN2Ii/UnLyWfYZ5uYt+FEpcfxw/lUHtp1jBy7nd4BPiyMqY+fm0xZI2omSW5ClIMQH3cWPn4Dg2PCsdkVk5bt5YWlu8mvpEKThWeTGLnnBFaluKOWH3Nb18PLKFPWiJpLkpsQ5cTdzci7Q2L4161N0DT4YmMswz7dRGq2tUTb22w2Jk2aRL169fDw8KBBgwa88sorKHX1Ib8+ijvH3w/EYQfurx3Ihy2isRjkT1vUbPIXIEQ50jSN0b0b8uGD7fE0G1l/NInBM9dx5FzmNbd98803mTVrFjNmzGD//v28+eabvPXWW7z//vvFrq+U4q3jZ3nxyBkAnoisxX+aRMrI/kIgyU2ICnFLizC+frIrEf4enEjK5s4P1vH7ofNX3Wb9+vUMGjSI22+/nejoaO6++25uueUWNm3adNm6dqV44fBp3jmRAMCz9cKYLCP7C+EkyU2ICtKsti/LxnajQ90AMnILGPHZJj5de/yKlxm7du3KqlWrOHToEAA7d+5k7dq19O/fv8h6BXbF+AOxfHLaUZX5eqMI/h4dJolNiEtIKZUQFSjY28L8kZ15fskeFm89xZTv93EoIYMpg1piNhX9bvnss8+Snp5O06ZNMRqN2Gw2XnvtNR544AHnOrk2O0/uO8mPiWkYNZjWNIp7wgIr+9cSosqTlpsQFcxiMvLvu1vz/G3N0DRYuDmOBz/5k+SsooUm//vf/5g/fz5ffvkl27ZtY+7cubz99tvMnTsXgKwCGw/tPsaPiWlYDBqftKgniU2IK9DUtUqxqoD09HT8/PxIS0vD11dGMxfV128HzjFuwXYy8wqIDPTgk2EdaRzqA0BkZCTPPvssY8aMca7/6quv8sUXX7Bh9x4e2HWMbenZeBkNzG1Vj+4BPnr9GkLoojS5QFpuQlSiPk1D+GZ0VyIDPYhLzuFvH6zn1wOOopDs7GwMfynhNxqN5Nts3Ln9CNvSswkwGfkqpoEkNiGuQe65CVHJGof6sGxMd578Yit/Hk/m0blbmNi/KQMGDGDKK6+yM9mIpVZdss4e4ev3/oNH/0EcyMol1GxiUUwDmnp56P0rCFHlyWVJIXRiLbAz+ds9LNgUB0CkN+xc+hE5hzdgy07D6BOE5Zb+eD3yBHV9vPgqpgF1PSw6Ry2EfkqTCyS5CaEjpRSfrz/BlO/2cekfot3XDWv7YDAb0DLzuTffzPS/tdEtTiGqArnnJkQ1oWkaNzUNLZLYlAHym/o5EluqFfOmRL7ddIq45Gzd4hSiupHkJoTOlu047RwySxlAs4N5axLGuEzMWxLR8u0YNI2l20/rHKkQ1YcUlAihs8TMPDQNUI6H3dcx/5ppfxrahSadpjnWE0KUTJlabjNnziQ6Ohp3d3c6d+5c7Nh3l/rqq69o2rQp7u7utGrViuXLl5cpWCFcUbC3hcI735oCLTMfLT3fmdjAMZZksLcUkwhRUqVObosWLWLChAlMnjyZbdu20aZNG/r168e5c+eKXX/9+vUMHTqURx99lO3btzN48GAGDx7Mnj17rjt4IVzBoJgI7JfUdWl2+OsokUrB4LYRlRuYENVYqaslO3fuTMeOHZkxYwYAdrudyMhIxo0bx7PPPnvZ+kOGDCErK4vvv//eueyGG24gJiaG2bNnl+iYUi0pXN3Er3excEscxf01ahrc1yGSqXe1rvzAhKhCKqxa0mq1snXrVvr27XtxBwYDffv2ZcOGDcVus2HDhiLrA/Tr1++K6wPk5eWRnp5e5CGEK5syuCX3dYhEA4yahsmgYdAcLbj7OkQyZXBLvUMUolopVUFJYmIiNpuN0NDQIstDQ0M5cOBAsdvEx8cXu358fPwVjzN16lRefvnl0oQmRLXmZjQw9a7WjO7TkKXbT5OYmUctHwuDYiKIDPTUOzwhqp0qWS05ceJEJkyY4Pw5PT2dyMhIHSMSonJEBnoy7qZGeochRLVXquQWHByM0WgkISGhyPKEhATCwsKK3SYsLKxU6wNYLBYsFqkME0IIUTaluudmNptp3749q1atci6z2+2sWrWKLl26FLtNly5diqwPsHLlyiuuL4QQQlyvUl+WnDBhAsOGDaNDhw506tSJadOmkZWVxYgRIwB4+OGHiYiIYOrUqQCMHz+eXr168Z///Ifbb7+dhQsXsmXLFj766KPy/U2EEEKIC0qd3IYMGcL58+d58cUXiY+PJyYmhhUrVjiLRmJjY4vMSdW1a1e+/PJLXnjhBZ577jkaNWrE0qVLadlSqr+EEEJUDJkVQAghRLUgswIIIYSo0SS5CSGEcDmS3IQQQrgcSW5CCCFcTpUcoeSvCmteZIxJIYSouQpzQEnqIKtFcsvIyACQIbiEEEKQkZGBn5/fVdepFl0B7HY7Z86cwcfHB03760xXNUfhGJtxcXHSJaIM5PxdPzmH10/OYdkppcjIyCA8PLxIf+riVIuWm8FgoE6dOnqHUWX4+vrKH8V1kPN3/eQcXj85h2VzrRZbISkoEUII4XIkuQkhhHA5ktyqEYvFwuTJk2U6oDKS83f95BxePzmHlaNaFJQIIYQQpSEtNyGEEC5HkpsQQgiXI8lNCCGEy5HkJoQQwuVIcqtiZs6cSXR0NO7u7nTu3JlNmzZdcd05c+bQo0cPAgICCAgIoG/fvlddvyYozfm71MKFC9E0jcGDB1dsgNVAac9hamoqY8aMoXbt2lgsFho3bszy5csrKdqqp7Tnb9q0aTRp0gQPDw8iIyN5+umnyc3NraRoXZgSVcbChQuV2WxWn376qdq7d68aOXKk8vf3VwkJCcWuf//996uZM2eq7du3q/3796vhw4crPz8/derUqUqOvGoo7fkrdPz4cRUREaF69OihBg0aVDnBVlGlPYd5eXmqQ4cO6rbbblNr165Vx48fV6tXr1Y7duyo5MirhtKev/nz5yuLxaLmz5+vjh8/rn766SdVu3Zt9fTTT1dy5K5HklsV0qlTJzVmzBjnzzabTYWHh6upU6eWaPuCggLl4+Oj5s6dW1EhVmllOX8FBQWqa9eu6uOPP1bDhg2r8cmttOdw1qxZqn79+spqtVZWiFVaac/fmDFj1I033lhk2YQJE1S3bt0qNM6aQC5LVhFWq5WtW7fSt29f5zKDwUDfvn3ZsGFDifaRnZ1Nfn4+gYGBFRVmlVXW8zdlyhRCQkJ49NFHKyPMKq0s5/Dbb7+lS5cujBkzhtDQUFq2bMnrr7+OzWarrLCrjLKcv65du7J161bnpctjx46xfPlybrvttkqJ2ZVVi4GTa4LExERsNhuhoaFFloeGhnLgwIES7eOZZ54hPDy8yB9XTVGW87d27Vo++eQTduzYUQkRVn1lOYfHjh3j119/5YEHHmD58uUcOXKE0aNHk5+fz+TJkysj7CqjLOfv/vvvJzExke7du6OUoqCggCeeeILnnnuuMkJ2adJycxFvvPEGCxcuZMmSJbi7u+sdTpWXkZHBQw89xJw5cwgODtY7nGrLbrcTEhLCRx99RPv27RkyZAjPP/88s2fP1ju0amH16tW8/vrrfPDBB2zbto1vvvmGH374gVdeeUXv0Ko9ablVEcHBwRiNRhISEoosT0hIICws7Krbvv3227zxxhv88ssvtG7duiLDrLJKe/6OHj3KiRMnGDBggHOZ3W4HwGQycfDgQRo0aFCxQVcxZXkP1q5dGzc3N4xGo3NZs2bNiI+Px2q1YjabKzTmqqQs52/SpEk89NBDPPbYYwC0atWKrKwsHn/8cZ5//vlrzlkmrkzOXBVhNptp3749q1atci6z2+2sWrWKLl26XHG7t956i1deeYUVK1bQoUOHygi1Sirt+WvatCm7d+9mx44dzsfAgQPp06cPO3bsqJGzvpflPditWzeOHDni/GIAcOjQIWrXrl2jEhuU7fxlZ2dflsAKvygoGfb3+uhd0SIuWrhwobJYLOrzzz9X+/btU48//rjy9/dX8fHxSimlHnroIfXss88613/jjTeU2WxWixcvVmfPnnU+MjIy9PoVdFXa8/dXUi1Z+nMYGxurfHx81NixY9XBgwfV999/r0JCQtSrr76q16+gq9Kev8mTJysfHx+1YMECdezYMfXzzz+rBg0aqHvvvVevX8FlSHKrYt5//30VFRWlzGaz6tSpk9q4caPztV69eqlhw4Y5f65bt64CLntMnjy58gOvIkpz/v5KkptDac/h+vXrVefOnZXFYlH169dXr732miooKKjkqKuO0py//Px89dJLL6kGDRood3d3FRkZqUaPHq1SUlIqP3AXI1PeCCGEcDlyz00IIYTLkeQmhBDC5UhyE0II4XIkuQkhhHA5ktyEEEK4HEluQgghXI4kNyGEEC5HkpsQQgiXI8lNCCGEy5HkJoQQwuVIchNCCOFyJLkJIYRwOf8P1kzXB7VscPsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with open(output_path, \"r\") as f:\n",
    "    lines = [json.loads(line) for _, line in zip(range(5000), f)]  # adjust cap as needed\n",
    "\n",
    "insert_example = next(ex for ex in lines if ex[\"mode\"] == \"insert\")\n",
    "two_opt_example = next(ex for ex in lines if ex[\"mode\"] == \"two_opt\" and not ex[\"action\"].get(\"stop\", False))\n",
    "\n",
    "\n",
    "plot_insert_example(insert_example)\n",
    "\n",
    "plot_two_opt_example(two_opt_example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrity check pass rate over sample: 20 / 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def verify_insert(example: dict) -> bool:\n",
    "    assert example[\"mode\"] == \"insert\"\n",
    "    coords = np.array(example[\"coords\"], dtype=np.float32)\n",
    "    dist = pairwise_euclidean(coords)\n",
    "    tour_before = example[\"tour_partial\"]\n",
    "    city = example[\"action\"][\"city\"]\n",
    "    position = example[\"action\"][\"position\"]\n",
    "    tour_after = tour_before[:position] + [city] + tour_before[position:]\n",
    "    computed_before = tour_length(dist, tour_before)\n",
    "    computed_after = tour_length(dist, tour_after)\n",
    "    ok = (abs(computed_before - example[\"cost_before\"]) < 1e-5) and (abs(computed_after - example[\"cost_after\"]) < 1e-5)\n",
    "    return bool(ok)\n",
    "\n",
    "\n",
    "def verify_two_opt(example: dict) -> bool:\n",
    "    assert example[\"mode\"] == \"two_opt\"\n",
    "    coords = np.array(example[\"coords\"], dtype=np.float32)\n",
    "    dist = pairwise_euclidean(coords)\n",
    "    tour_before = example[\"tour_full\"]\n",
    "    computed_before = tour_length(dist, tour_before)\n",
    "    if example[\"action\"].get(\"stop\", False):\n",
    "        # At STOP we only check that the stored cost matches the tour_before cost\n",
    "        return abs(computed_before - example[\"cost_before\"]) < 1e-5\n",
    "    i = example[\"action\"][\"i\"]\n",
    "    j = example[\"action\"][\"j\"]\n",
    "    # Apply the 2-opt swap and recompute\n",
    "    def two_opt_swap_py(tour, i, j):\n",
    "        return tour[:i] + list(reversed(tour[i:j+1])) + tour[j+1:]\n",
    "    tour_after = two_opt_swap_py(tour_before, i, j)\n",
    "    computed_after = tour_length(dist, tour_after)\n",
    "    ok1 = abs(computed_before - example[\"cost_before\"]) < 1e-5\n",
    "    ok2 = abs(computed_after - example[\"cost_after\"]) < 1e-5\n",
    "    return bool(ok1 and ok2)\n",
    "\n",
    "\n",
    "# Run a quick check over a handful of samples\n",
    "sampled = random.sample(lines, k=min(20, len(lines)))\n",
    "results = []\n",
    "for ex in sampled:\n",
    "    if ex[\"mode\"] == \"insert\":\n",
    "        results.append(verify_insert(ex))\n",
    "    else:\n",
    "        results.append(verify_two_opt(ex))\n",
    "\n",
    "print(\"Integrity check pass rate over sample:\", sum(results), \"/\", len(results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verify that the recorded insert is truly the cheapest among all city-position pairs\n",
    "def verify_insert_is_cheapest(example):\n",
    "    assert example[\"mode\"] == \"insert\"\n",
    "    coords = np.array(example[\"coords\"], dtype=np.float32)\n",
    "    dist = pairwise_euclidean(coords)\n",
    "    tour = example[\"tour_partial\"]\n",
    "    chosen_city = example[\"action\"][\"city\"]\n",
    "    chosen_pos  = example[\"action\"][\"position\"]\n",
    "    n = coords.shape[0]\n",
    "    visited = set(tour)\n",
    "\n",
    "    def delta(u, c, v): return float(dist[u, c] + dist[c, v] - dist[u, v])\n",
    "\n",
    "    best = float(\"inf\"); best_pair = None\n",
    "    m = len(tour)\n",
    "    for c in range(n):\n",
    "        if c in visited:\n",
    "            continue\n",
    "        for pos in range(m):\n",
    "            u, v = tour[pos], tour[(pos + 1) % m]\n",
    "            d = delta(u, c, v)\n",
    "            if d < best:\n",
    "                best = d; best_pair = (c, pos + 1)\n",
    "    return (chosen_city, chosen_pos) == best_pair\n",
    "\n",
    "\n",
    "# Verify that the recorded 2-opt move is the best improvement available\n",
    "def verify_two_opt_is_best(example):\n",
    "    assert example[\"mode\"] == \"two_opt\"\n",
    "    coords = np.array(example[\"coords\"], dtype=np.float32)\n",
    "    dist = pairwise_euclidean(coords)\n",
    "    tour = example[\"tour_full\"]\n",
    "    n = len(tour)\n",
    "\n",
    "    best_delta = 0.0; best_pair = (None, None)\n",
    "    for i in range(1, n - 2):\n",
    "        a, b = tour[i - 1], tour[i]\n",
    "        for j in range(i + 1, n - 1):\n",
    "            c, d = tour[j], tour[(j + 1) % n]\n",
    "            if b == c or a == d:\n",
    "                continue\n",
    "            delta = float(dist[a, c]) + float(dist[b, d]) - float(dist[a, b]) - float(dist[c, d])\n",
    "            if delta < best_delta - 1e-12:\n",
    "                best_delta = delta; best_pair = (i, j)\n",
    "\n",
    "    if example[\"action\"].get(\"stop\", False):\n",
    "        # No improving move should exist\n",
    "        return best_pair == (None, None)\n",
    "    else:\n",
    "        return (example[\"action\"][\"i\"], example[\"action\"][\"j\"]) == best_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert example cheapest?  True\n",
      "Two-opt example best?     True\n"
     ]
    }
   ],
   "source": [
    "from tsp_data_gen import pairwise_euclidean  # make sure this import works\n",
    "\n",
    "DATA_PATH = \"data/tsp_train_small.jsonl\"  # adjust if needed\n",
    "\n",
    "# Load a few lines\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    rows = [json.loads(line) for _, line in zip(range(5000), f)]\n",
    "\n",
    "insert_ex = next(ex for ex in rows if ex[\"mode\"] == \"insert\")\n",
    "twoopt_ex = next(ex for ex in rows if ex[\"mode\"] == \"two_opt\" and not ex[\"action\"].get(\"stop\", False))\n",
    "\n",
    "print(\"Insert example cheapest? \", verify_insert_is_cheapest(insert_ex))\n",
    "print(\"Two-opt example best?    \", verify_two_opt_is_best(twoopt_ex))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert:  152/152 passed\n",
      "Two-opt: 148/148 passed\n",
      "Overall: 300/300 passed\n"
     ]
    }
   ],
   "source": [
    "rng = random.Random(0)\n",
    "sample_size = min(300, len(rows))  # tune up or down\n",
    "indices = sorted(rng.sample(range(len(rows)), k=sample_size))\n",
    "\n",
    "insert_pass = insert_total = 0\n",
    "twoopt_pass = twoopt_total = 0\n",
    "\n",
    "for idx in indices:\n",
    "    ex = rows[idx]\n",
    "    if ex[\"mode\"] == \"insert\":\n",
    "        insert_total += 1\n",
    "        insert_pass  += int(verify_insert_is_cheapest(ex))\n",
    "    else:\n",
    "        twoopt_total += 1\n",
    "        twoopt_pass  += int(verify_two_opt_is_best(ex))\n",
    "\n",
    "print(f\"Insert:  {insert_pass}/{insert_total} passed\")\n",
    "print(f\"Two-opt: {twoopt_pass}/{twoopt_total} passed\")\n",
    "print(f\"Overall: {(insert_pass + twoopt_pass)}/{(insert_total + twoopt_total)} passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Insert:  1089/1089 passed\n",
      "Two-opt: 1000/1000 passed\n",
      "Overall: 2089/2089 passed\n"
     ]
    }
   ],
   "source": [
    "def explain_insert_failure(example):\n",
    "    coords = np.array(example[\"coords\"], dtype=np.float32)\n",
    "    dist = pairwise_euclidean(coords)\n",
    "    tour = example[\"tour_partial\"]\n",
    "    chosen_city = example[\"action\"][\"city\"]\n",
    "    chosen_pos  = example[\"action\"][\"position\"]\n",
    "    n = coords.shape[0]\n",
    "    visited = set(tour)\n",
    "    m = len(tour)\n",
    "\n",
    "    def delta(u, c, v): return float(dist[u, c] + dist[c, v] - dist[u, v])\n",
    "\n",
    "    best = float(\"inf\"); best_pair = None; chosen_delta = None\n",
    "    for c in range(n):\n",
    "        if c in visited:\n",
    "            continue\n",
    "        for pos in range(m):\n",
    "            u, v = tour[pos], tour[(pos + 1) % m]\n",
    "            d = delta(u, c, v)\n",
    "            if c == chosen_city and (pos + 1) == chosen_pos:\n",
    "                chosen_delta = d\n",
    "            if d < best:\n",
    "                best = d; best_pair = (c, pos + 1)\n",
    "    print(\"Insert failure:\")\n",
    "    print(\"  chosen (city, pos):\", (chosen_city, chosen_pos), \"delta:\", chosen_delta)\n",
    "    print(\"  best   (city, pos):\", best_pair, \"delta:\", best)\n",
    "\n",
    "def explain_two_opt_failure(example):\n",
    "    coords = np.array(example[\"coords\"], dtype=np.float32)\n",
    "    dist = pairwise_euclidean(coords)\n",
    "    tour = example[\"tour_full\"]\n",
    "    n = len(tour)\n",
    "\n",
    "    best_delta = 0.0; best_pair = (None, None)\n",
    "    for i in range(1, n - 2):\n",
    "        a, b = tour[i - 1], tour[i]\n",
    "        for j in range(i + 1, n - 1):\n",
    "            c, d = tour[j], tour[(j + 1) % n]\n",
    "            if b == c or a == d:\n",
    "                continue\n",
    "            delta = float(dist[a, c]) + float(dist[b, d]) - float(dist[a, b]) - float(dist[c, d])\n",
    "            if delta < best_delta - 1e-12:\n",
    "                best_delta = delta; best_pair = (i, j)\n",
    "\n",
    "    if example[\"action\"].get(\"stop\", False):\n",
    "        print(\"Two-opt STOP failure: should have no improving move.\")\n",
    "        print(\"  best available delta:\", best_delta, \"pair:\", best_pair)\n",
    "    else:\n",
    "        i0, j0 = example[\"action\"][\"i\"], example[\"action\"][\"j\"]\n",
    "        print(\"Two-opt failure:\")\n",
    "        print(\"  chosen (i, j):\", (i0, j0))\n",
    "        print(\"  best   (i, j):\", best_pair, \"best delta:\", best_delta)\n",
    "\n",
    "# Stream the file and check\n",
    "insert_pass = insert_total = 0\n",
    "twoopt_pass = twoopt_total = 0\n",
    "fail_show = 0\n",
    "max_fail_show = 3\n",
    "\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    for line in f:\n",
    "        ex = json.loads(line)\n",
    "        if ex[\"mode\"] == \"insert\":\n",
    "            insert_total += 1\n",
    "            ok = verify_insert_is_cheapest(ex)\n",
    "            insert_pass += int(ok)\n",
    "            if not ok and fail_show < max_fail_show:\n",
    "                explain_insert_failure(ex); fail_show += 1\n",
    "        else:\n",
    "            twoopt_total += 1\n",
    "            ok = verify_two_opt_is_best(ex)\n",
    "            twoopt_pass += int(ok)\n",
    "            if not ok and fail_show < max_fail_show:\n",
    "                explain_two_opt_failure(ex); fail_show += 1\n",
    "\n",
    "print(f\"\\nInsert:  {insert_pass}/{insert_total} passed\")\n",
    "print(f\"Two-opt: {twoopt_pass}/{twoopt_total} passed\")\n",
    "print(f\"Overall: {(insert_pass + twoopt_pass)}/{(insert_total + twoopt_total)} passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No failures found in the loaded rows.\n"
     ]
    }
   ],
   "source": [
    "# collect first failing examples for each mode\n",
    "first_bad_insert = None\n",
    "first_bad_twoopt = None\n",
    "\n",
    "for ex in rows:\n",
    "    if ex[\"mode\"] == \"insert\" and not verify_insert_is_cheapest(ex) and first_bad_insert is None:\n",
    "        first_bad_insert = ex\n",
    "    if ex[\"mode\"] == \"two_opt\" and not verify_two_opt_is_best(ex) and first_bad_twoopt is None:\n",
    "        first_bad_twoopt = ex\n",
    "    if first_bad_insert and first_bad_twoopt:\n",
    "        break\n",
    "\n",
    "if first_bad_insert:\n",
    "    plot_insert_example(first_bad_insert)\n",
    "if first_bad_twoopt:\n",
    "    plot_two_opt_example(first_bad_twoopt)\n",
    "\n",
    "if not first_bad_insert and not first_bad_twoopt:\n",
    "    print(\"No failures found in the loaded rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = \"data/tsp_train_small.jsonl\"   \n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: insert=1089  two_opt=1000\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl_split(path):\n",
    "    insert_list = []\n",
    "    two_opt_list = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            if ex[\"mode\"] == \"insert\":\n",
    "                tp = np.array(ex[\"tour_partial\"], dtype=np.int64)  # [m]\n",
    "                m = int(tp.shape[0])\n",
    "                pos_raw = int(ex[\"action\"][\"position\"])            # JSONL stores 1..m\n",
    "                pos_edge = (pos_raw - 1) % m                       # map to 0..m-1\n",
    "\n",
    "                insert_list.append({\n",
    "                    \"coords\": np.array(ex[\"coords\"], dtype=np.float32),  # [n,2]\n",
    "                    \"tour_partial\": tp,                                  # [m]\n",
    "                    \"target_city\": int(ex[\"action\"][\"city\"]),\n",
    "                    \"target_position\": int(pos_edge)                     # 0..m-1 for classifier\n",
    "                })\n",
    "            else:\n",
    "                act = ex[\"action\"]\n",
    "                stop = bool(act.get(\"stop\", False))\n",
    "                target_i = -1 if stop else int(act[\"i\"])\n",
    "                target_j = -1 if stop else int(act[\"j\"])\n",
    "                two_opt_list.append({\n",
    "                    \"coords\": np.array(ex[\"coords\"], dtype=np.float32),     # [n,2]\n",
    "                    \"tour_full\": np.array(ex[\"tour_full\"], dtype=np.int64), # [n]\n",
    "                    \"stop\": stop,\n",
    "                    \"target_i\": target_i,\n",
    "                    \"target_j\": target_j\n",
    "                })\n",
    "    return insert_list, two_opt_list\n",
    "\n",
    "insert_examples, two_opt_examples = load_jsonl_split(DATA_PATH)\n",
    "print(f\"Loaded: insert={len(insert_examples)}  two_opt={len(two_opt_examples)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsertDataset(Dataset):\n",
    "    def __init__(self, items): self.items = items\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i): return self.items[i]\n",
    "\n",
    "def collate_insert(batch):\n",
    "    batch_size = len(batch)\n",
    "    n_max = max(item[\"coords\"].shape[0] for item in batch)\n",
    "    m_max = max(item[\"tour_partial\"].shape[0] for item in batch)\n",
    "\n",
    "    coords = torch.zeros(batch_size, n_max, 2, dtype=torch.float32)\n",
    "    visited_mask = torch.zeros(batch_size, n_max, dtype=torch.float32)\n",
    "    partial_tour = torch.full((batch_size, m_max), -1, dtype=torch.long)\n",
    "    partial_length = torch.zeros(batch_size, dtype=torch.long)\n",
    "    target_city = torch.zeros(batch_size, dtype=torch.long)\n",
    "    target_position = torch.zeros(batch_size, dtype=torch.long)\n",
    "\n",
    "    for b, item in enumerate(batch):\n",
    "        n = item[\"coords\"].shape[0]\n",
    "        m = item[\"tour_partial\"].shape[0]\n",
    "        coords[b, :n] = torch.from_numpy(item[\"coords\"])\n",
    "        partial_tour[b, :m] = torch.from_numpy(item[\"tour_partial\"])\n",
    "        partial_length[b] = m\n",
    "        target_city[b] = item[\"target_city\"]\n",
    "        target_position[b] = item[\"target_position\"]\n",
    "        # build visited mask from the partial tour\n",
    "        for v in item[\"tour_partial\"]:\n",
    "            visited_mask[b, int(v)] = 1.0\n",
    "\n",
    "    return {\n",
    "        \"coords\": coords,                         # [B, Nmax, 2]\n",
    "        \"visited_mask\": visited_mask,             # [B, Nmax] 1=visited\n",
    "        \"partial_tour\": partial_tour,             # [B, Mmax] with -1 pad\n",
    "        \"partial_length\": partial_length,         # [B]\n",
    "        \"target_city\": target_city,               # [B]\n",
    "        \"target_position\": target_position        # [B]\n",
    "    }\n",
    "\n",
    "class TwoOptDataset(Dataset):\n",
    "    def __init__(self, items): self.items = items\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i): return self.items[i]\n",
    "\n",
    "def enumerate_two_opt_pairs(tour):\n",
    "    # valid 2-opt pairs under standard adjacency constraints\n",
    "    n = len(tour)\n",
    "    pairs = []\n",
    "    for i in range(1, n - 2):\n",
    "        a, b = tour[i - 1], tour[i]\n",
    "        for j in range(i + 1, n - 1):\n",
    "            c, d = tour[j], tour[(j + 1) % n]\n",
    "            if b == c or a == d:\n",
    "                continue\n",
    "            pairs.append((i, j))\n",
    "    return pairs\n",
    "\n",
    "def collate_two_opt_pairclass(batch):\n",
    "    # Build a per-sample catalogue of candidate pairs and target index, with a STOP class\n",
    "    batch_size = len(batch)\n",
    "    n_max = max(item[\"coords\"].shape[0] for item in batch)\n",
    "\n",
    "    coords = torch.zeros(batch_size, n_max, 2, dtype=torch.float32)\n",
    "    tour_full = torch.full((batch_size, n_max), -1, dtype=torch.long)\n",
    "    n_nodes = torch.zeros(batch_size, dtype=torch.long)\n",
    "\n",
    "    candidate_lists = []\n",
    "    target_indices = []\n",
    "    for b, item in enumerate(batch):\n",
    "        n = item[\"coords\"].shape[0]\n",
    "        coords[b, :n] = torch.from_numpy(item[\"coords\"])\n",
    "        tour_full[b, :n] = torch.from_numpy(item[\"tour_full\"])\n",
    "        n_nodes[b] = n\n",
    "\n",
    "        pairs = enumerate_two_opt_pairs(item[\"tour_full\"].tolist())\n",
    "        pair_to_index = {p: k for k, p in enumerate(pairs)}\n",
    "        if item[\"stop\"]:\n",
    "            target_idx = len(pairs)  # STOP class at the end\n",
    "        else:\n",
    "            target_idx = pair_to_index[(item[\"target_i\"], item[\"target_j\"])]\n",
    "        candidate_lists.append(pairs)\n",
    "        target_indices.append(target_idx)\n",
    "\n",
    "    # Pad the candidate pair lists and build masks\n",
    "    L = max(len(pairs) + 1 for pairs in candidate_lists)  # +1 for STOP\n",
    "    pair_indices = torch.full((batch_size, L, 2), -1, dtype=torch.long)  # holds (i,j)\n",
    "    pair_mask = torch.zeros(batch_size, L, dtype=torch.bool)              # True where valid\n",
    "    for b, pairs in enumerate(candidate_lists):\n",
    "        K = len(pairs)\n",
    "        if K > 0:\n",
    "            pair_indices[b, :K] = torch.tensor(pairs, dtype=torch.long)\n",
    "        # mark K pairs plus one STOP slot as valid classes\n",
    "        pair_mask[b, :K+1] = True\n",
    "\n",
    "    target_pair_index = torch.tensor(target_indices, dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"coords\": coords,                   # [B, Nmax, 2]\n",
    "        \"tour_full\": tour_full,             # [B, Nmax] with -1 pad\n",
    "        \"n_nodes\": n_nodes,                 # [B]\n",
    "        \"pair_indices\": pair_indices,       # [B, L, 2], entries -1 are padding\n",
    "        \"pair_mask\": pair_mask,             # [B, L], True for valid classes (K pairs + STOP)\n",
    "        \"target_pair_index\": target_pair_index  # [B], in [0..K] where K is STOP\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Insert batch\n",
      "  coords             (64, 12, 2) torch.float32\n",
      "  visited_mask       (64, 12) torch.float32\n",
      "  partial_tour       (64, 8) torch.int64\n",
      "  partial_length     (64,) torch.int64\n",
      "  target_city        (64,) torch.int64\n",
      "  target_position    (64,) torch.int64\n",
      "\n",
      "Two-opt batch\n",
      "  coords             (64, 12, 2) torch.float32\n",
      "  tour_full          (64, 12) torch.int64\n",
      "  n_nodes            (64,) torch.int64\n",
      "  pair_indices       (64, 46, 2) torch.int64\n",
      "  pair_mask          (64, 46) torch.bool\n",
      "  target_pair_index  (64,) torch.int64\n"
     ]
    }
   ],
   "source": [
    "insert_loader = DataLoader(\n",
    "    InsertDataset(insert_examples),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_insert,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "two_opt_loader = DataLoader(\n",
    "    TwoOptDataset(two_opt_examples),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_two_opt_pairclass,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "insert_batch = next(iter(insert_loader))\n",
    "two_opt_batch = next(iter(two_opt_loader))\n",
    "\n",
    "def brief_shapes(name, batch):\n",
    "    print(f\"\\n{name}\")\n",
    "    for k, v in batch.items():\n",
    "        if torch.is_tensor(v):\n",
    "            print(f\"  {k:18s} {tuple(v.shape)} {v.dtype}\")\n",
    "        else:\n",
    "            print(f\"  {k:18s} {type(v)}\")\n",
    "\n",
    "brief_shapes(\"Insert batch\", insert_batch)\n",
    "brief_shapes(\"Two-opt batch\", two_opt_batch)\n",
    "\n",
    "# A couple of sanity asserts you can keep\n",
    "B, Nmax = insert_batch[\"coords\"].shape[:2]\n",
    "assert (insert_batch[\"target_city\"] < Nmax).all()\n",
    "assert (insert_batch[\"target_position\"] < insert_batch[\"partial_length\"]).all()\n",
    "\n",
    "B2, Lmax = two_opt_batch[\"pair_mask\"].shape\n",
    "assert (two_opt_batch[\"target_pair_index\"] < Lmax).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: data/tsp_ready.pt\n",
      "Reloaded counts: 1089 1000\n"
     ]
    }
   ],
   "source": [
    "# save (unchanged)\n",
    "checkpoint_path = \"data/tsp_ready.pt\"\n",
    "payload = {\n",
    "    \"insert_examples\": insert_examples,\n",
    "    \"two_opt_examples\": two_opt_examples,\n",
    "    \"meta\": {\n",
    "        \"source\": DATA_PATH,\n",
    "        \"counts\": {\"insert\": len(insert_examples), \"two_opt\": len(two_opt_examples)}\n",
    "    }\n",
    "}\n",
    "torch.save(payload, checkpoint_path)\n",
    "print(\"Saved checkpoint:\", checkpoint_path)\n",
    "\n",
    "# load with weights_only=False (explicitly allow general pickled objects)\n",
    "reloaded = torch.load(checkpoint_path, weights_only=False)\n",
    "print(\"Reloaded counts:\", len(reloaded[\"insert_examples\"]), len(reloaded[\"two_opt_examples\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Minimal wrappers around tsp_data_gen.write_dataset\n",
    "import os\n",
    "from tsp_data_gen import write_dataset\n",
    "\n",
    "def make_dataset_pairs_only(\n",
    "    out_path: str,\n",
    "    num_problems: int = 4000,\n",
    "    n_min: int = 6,\n",
    "    n_max: int = 12,\n",
    "    seed: int = 1234,\n",
    "    opt_per: int = 6,\n",
    "    step_sample: str = \"mid\",\n",
    "    noise_min: int = 2,\n",
    "    noise_max: int = 4,\n",
    "    include_stop_prob: float = 0.2,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create a JSONL with only 2-opt steps (pairs track).\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    write_dataset(\n",
    "        out_path=out_path,\n",
    "        num_problems=num_problems,\n",
    "        n_min=n_min,\n",
    "        n_max=n_max,\n",
    "        seed=seed,\n",
    "        p_constructive=0.0,\n",
    "        ins_per=0,\n",
    "        opt_per=opt_per,\n",
    "        step_sample=step_sample,\n",
    "        noise_min=noise_min,\n",
    "        noise_max=noise_max,\n",
    "        include_stop_prob=include_stop_prob,\n",
    "    )\n",
    "\n",
    "def make_dataset_mixed(\n",
    "    out_path: str,\n",
    "    num_problems: int = 4000,\n",
    "    n_min: int = 6,\n",
    "    n_max: int = 12,\n",
    "    seed: int = 1234,\n",
    "    p_constructive: float = 0.7,\n",
    "    ins_per: int = 6,\n",
    "    opt_per: int = 6,\n",
    "    step_sample: str = \"mid\",\n",
    "    noise_min: int = 2,\n",
    "    noise_max: int = 4,\n",
    "    include_stop_prob: float = 0.2,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create a JSONL with both insert and 2-opt steps (mixed track).\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    write_dataset(\n",
    "        out_path=out_path,\n",
    "        num_problems=num_problems,\n",
    "        n_min=n_min,\n",
    "        n_max=n_max,\n",
    "        seed=seed,\n",
    "        p_constructive=p_constructive,\n",
    "        ins_per=ins_per,\n",
    "        opt_per=opt_per,\n",
    "        step_sample=step_sample,\n",
    "        noise_min=noise_min,\n",
    "        noise_max=noise_max,\n",
    "        include_stop_prob=include_stop_prob,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def load_jsonl_split(path: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Returns (insert_examples, two_opt_examples).\n",
    "    For insert_examples, maps action.position (1..m) to edge index 0..m-1.\n",
    "    Each example dict stores numpy arrays for coords and tours, and ints for labels.\n",
    "    \"\"\"\n",
    "    insert_list: List[Dict[str, Any]] = []\n",
    "    two_opt_list: List[Dict[str, Any]] = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            if ex[\"mode\"] == \"insert\":\n",
    "                tour_partial = np.array(ex[\"tour_partial\"], dtype=np.int64)\n",
    "                m = int(tour_partial.shape[0])\n",
    "                pos_raw = int(ex[\"action\"][\"position\"])   # 1..m in JSON\n",
    "                pos_edge = (pos_raw - 1) % m              # 0..m-1 for classifier\n",
    "                insert_list.append({\n",
    "                    \"coords\": np.array(ex[\"coords\"], dtype=np.float32),  # [n,2]\n",
    "                    \"tour_partial\": tour_partial,                        # [m]\n",
    "                    \"target_city\": int(ex[\"action\"][\"city\"]),\n",
    "                    \"target_position\": int(pos_edge),\n",
    "                })\n",
    "            elif ex[\"mode\"] == \"two_opt\":\n",
    "                act = ex[\"action\"]\n",
    "                stop = bool(act.get(\"stop\", False))\n",
    "                two_opt_list.append({\n",
    "                    \"coords\": np.array(ex[\"coords\"], dtype=np.float32),   # [n,2]\n",
    "                    \"tour_full\": np.array(ex[\"tour_full\"], dtype=np.int64),  # [n]\n",
    "                    \"stop\": stop,\n",
    "                    \"target_i\": -1 if stop else int(act[\"i\"]),\n",
    "                    \"target_j\": -1 if stop else int(act[\"j\"]),\n",
    "                })\n",
    "    return insert_list, two_opt_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InsertDataset(Dataset):\n",
    "    def __init__(self, items): self.items = items\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i): return self.items[i]\n",
    "\n",
    "def collate_insert(batch):\n",
    "    B = len(batch)\n",
    "    n_max = max(x[\"coords\"].shape[0] for x in batch)\n",
    "    m_max = max(x[\"tour_partial\"].shape[0] for x in batch)\n",
    "\n",
    "    coords = torch.zeros(B, n_max, 2, dtype=torch.float32)\n",
    "    visited_mask = torch.zeros(B, n_max, dtype=torch.float32)\n",
    "    partial_tour = torch.full((B, m_max), -1, dtype=torch.long)\n",
    "    partial_length = torch.zeros(B, dtype=torch.long)\n",
    "    target_city = torch.zeros(B, dtype=torch.long)\n",
    "    target_position = torch.zeros(B, dtype=torch.long)\n",
    "\n",
    "    for b, x in enumerate(batch):\n",
    "        n = x[\"coords\"].shape[0]\n",
    "        m = x[\"tour_partial\"].shape[0]\n",
    "        coords[b, :n] = torch.from_numpy(x[\"coords\"])\n",
    "        partial_tour[b, :m] = torch.from_numpy(x[\"tour_partial\"])\n",
    "        partial_length[b] = m\n",
    "        target_city[b] = x[\"target_city\"]\n",
    "        target_position[b] = x[\"target_position\"]\n",
    "        for v in x[\"tour_partial\"]:\n",
    "            visited_mask[b, int(v)] = 1.0\n",
    "\n",
    "    return {\n",
    "        \"coords\": coords,                   # [B, Nmax, 2]\n",
    "        \"visited_mask\": visited_mask,       # [B, Nmax]\n",
    "        \"partial_tour\": partial_tour,       # [B, Mmax]\n",
    "        \"partial_length\": partial_length,   # [B]\n",
    "        \"target_city\": target_city,         # [B]\n",
    "        \"target_position\": target_position  # [B] in [0..m-1]\n",
    "    }\n",
    "\n",
    "class TwoOptDataset(Dataset):\n",
    "    def __init__(self, items): self.items = items\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i): return self.items[i]\n",
    "\n",
    "def enumerate_two_opt_pairs(tour):\n",
    "    \"\"\"\n",
    "    Return valid 2-opt candidate pairs (i, j) with standard adjacency rules.\n",
    "    \"\"\"\n",
    "    n = len(tour)\n",
    "    out = []\n",
    "    for i in range(1, n - 2):\n",
    "        a, b = tour[i - 1], tour[i]\n",
    "        for j in range(i + 1, n - 1):\n",
    "            c, d = tour[j], tour[(j + 1) % n]\n",
    "            if b == c or a == d:\n",
    "                continue\n",
    "            out.append((i, j))\n",
    "    return out\n",
    "\n",
    "def collate_two_opt_pairclass(batch):\n",
    "    \"\"\"\n",
    "    Build a per-sample catalog of candidate pairs plus a STOP class.\n",
    "    Returns tensors and a mask so your model can produce [B, L] logits and use CE.\n",
    "    \"\"\"\n",
    "    B = len(batch)\n",
    "    n_max = max(x[\"coords\"].shape[0] for x in batch)\n",
    "\n",
    "    coords = torch.zeros(B, n_max, 2, dtype=torch.float32)\n",
    "    tour_full = torch.full((B, n_max), -1, dtype=torch.long)\n",
    "\n",
    "    candidate_lists = []\n",
    "    targets = []\n",
    "    for b, x in enumerate(batch):\n",
    "        n = x[\"coords\"].shape[0]\n",
    "        coords[b, :n] = torch.from_numpy(x[\"coords\"])\n",
    "        tour_full[b, :n] = torch.from_numpy(x[\"tour_full\"])\n",
    "        pairs = enumerate_two_opt_pairs(x[\"tour_full\"].tolist())\n",
    "        pair_to_index = {p: k for k, p in enumerate(pairs)}\n",
    "        if x[\"stop\"]:\n",
    "            targets.append(len(pairs))  # STOP index\n",
    "        else:\n",
    "            targets.append(pair_to_index[(x[\"target_i\"], x[\"target_j\"])])\n",
    "        candidate_lists.append(pairs)\n",
    "\n",
    "    L = max(len(pairs) + 1 for pairs in candidate_lists)  # +1 STOP\n",
    "    pair_indices = torch.full((B, L, 2), -1, dtype=torch.long)\n",
    "    pair_mask = torch.zeros(B, L, dtype=torch.bool)\n",
    "    for b, pairs in enumerate(candidate_lists):\n",
    "        k = len(pairs)\n",
    "        if k > 0:\n",
    "            pair_indices[b, :k] = torch.tensor(pairs, dtype=torch.long)\n",
    "        pair_mask[b, :k+1] = True\n",
    "\n",
    "    target_pair_index = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"coords\": coords,                 # [B, Nmax, 2]\n",
    "        \"tour_full\": tour_full,           # [B, Nmax]\n",
    "        \"pair_indices\": pair_indices,     # [B, L, 2]\n",
    "        \"pair_mask\": pair_mask,           # [B, L] True for valid classes\n",
    "        \"target_pair_index\": target_pair_index  # [B] in [0..K], K=STOP\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def split_list(items, val_frac: float = 0.1, seed: int = 0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    idx = np.arange(len(items))\n",
    "    rng.shuffle(idx)\n",
    "    cut = int((1.0 - val_frac) * len(items))\n",
    "    train_idx = idx[:cut].tolist()\n",
    "    val_idx = idx[cut:].tolist()\n",
    "    return [items[i] for i in train_idx], [items[i] for i in val_idx]\n",
    "\n",
    "def build_insert_loaders(insert_examples, batch_size: int = 64, val_frac: float = 0.1, seed: int = 0) -> Tuple[DataLoader, DataLoader]:\n",
    "    train_items, val_items = split_list(insert_examples, val_frac=val_frac, seed=seed)\n",
    "    train_loader = DataLoader(InsertDataset(train_items), batch_size=batch_size, shuffle=True,\n",
    "                              collate_fn=collate_insert, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "    val_loader = DataLoader(InsertDataset(val_items), batch_size=batch_size, shuffle=False,\n",
    "                            collate_fn=collate_insert, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def build_two_opt_loaders(two_opt_examples, batch_size: int = 64, val_frac: float = 0.1, seed: int = 0) -> Tuple[DataLoader, DataLoader]:\n",
    "    train_items, val_items = split_list(two_opt_examples, val_frac=val_frac, seed=seed)\n",
    "    train_loader = DataLoader(TwoOptDataset(train_items), batch_size=batch_size, shuffle=True,\n",
    "                              collate_fn=collate_two_opt_pairclass, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "    val_loader = DataLoader(TwoOptDataset(val_items), batch_size=batch_size, shuffle=False,\n",
    "                            collate_fn=collate_two_opt_pairclass, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def save_tsp_checkpoint(path: str, insert_examples, two_opt_examples, source_path: str = None):\n",
    "    payload = {\n",
    "        \"insert_examples\": insert_examples,\n",
    "        \"two_opt_examples\": two_opt_examples,\n",
    "        \"meta\": {\n",
    "            \"source\": source_path,\n",
    "            \"counts\": {\"insert\": len(insert_examples), \"two_opt\": len(two_opt_examples)},\n",
    "        },\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    torch.save(payload, path)\n",
    "\n",
    "def load_tsp_checkpoint(path: str):\n",
    "    # weights_only=False is required for general Python objects in PyTorch 2.6+\n",
    "    return torch.load(path, weights_only=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_pairs_acc(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for batch in loader:\n",
    "        coords = batch[\"coords\"].to(device)\n",
    "        tour_full = batch[\"tour_full\"].to(device)\n",
    "        pair_indices = batch[\"pair_indices\"].to(device)\n",
    "        pair_mask = batch[\"pair_mask\"].to(device)\n",
    "        target = batch[\"target_pair_index\"].to(device)\n",
    "\n",
    "        logits = model(coords, tour_full, pair_indices)  # [B, L]\n",
    "        logits = logits.masked_fill(~pair_mask, -1e9)\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += target.numel()\n",
    "    return correct / max(1, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSP TRM Data: Quick User Guide (no function definitions)\n",
    "\n",
    "This minimal workflow supports:\n",
    "- Pairs track (recommended to start): 2-opt pairs only.\n",
    "- Mixed track (optional): insert + 2-opt.\n",
    "\n",
    "Assumption: you already pasted the helper cells (dataset creation wrappers, JSONL loader, datasets + collates, dataloader builders, checkpoint helpers). This guide only calls those helpers.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Create data\n",
    "\n",
    "### 1.1 Pairs track (2-opt only)\n",
    "```python\n",
    "make_dataset_pairs_only(\n",
    "    out_path=\"data/train_pairs.jsonl\",\n",
    "    num_problems=1200,\n",
    "    n_min=6, n_max=12,\n",
    "    seed=1234,\n",
    "    opt_per=6,\n",
    "    step_sample=\"mid\",\n",
    "    noise_min=2, noise_max=4,\n",
    "    include_stop_prob=0.2\n",
    ")\n",
    "```\n",
    "\n",
    "### 1.2 Mixed track (optional)\n",
    "```python\n",
    "make_dataset_mixed(\n",
    "    out_path=\"data/train_mixed.jsonl\",\n",
    "    num_problems=2000,\n",
    "    n_min=6, n_max=12,\n",
    "    seed=1234,\n",
    "    p_constructive=0.7,\n",
    "    ins_per=6,\n",
    "    opt_per=6,\n",
    "    step_sample=\"mid\",\n",
    "    noise_min=2, noise_max=4,\n",
    "    include_stop_prob=0.2\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Load data from JSONL\n",
    "```python\n",
    "# Use the file produced above; here we start with pairs-only\n",
    "insert_examples, two_opt_examples = load_jsonl_split(\"data/train_pairs.jsonl\")\n",
    "print(\"Loaded examples:\", len(insert_examples), len(two_opt_examples))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Build loaders\n",
    "\n",
    "### 3.1 Pairs track\n",
    "```python\n",
    "train_loader, val_loader = build_two_opt_loaders(two_opt_examples, batch_size=64, val_frac=0.1, seed=0)\n",
    "batch = next(iter(train_loader))\n",
    "print({k: tuple(v.shape) for k, v in batch.items()})\n",
    "# Expect keys:\n",
    "#   coords [B, Nmax, 2], tour_full [B, Nmax],\n",
    "#   pair_indices [B, L, 2], pair_mask [B, L], target_pair_index [B]\n",
    "```\n",
    "\n",
    "### 3.2 Mixed track (optional)\n",
    "```python\n",
    "train_ins, val_ins = build_insert_loaders(insert_examples, batch_size=64, val_frac=0.1, seed=0)\n",
    "b_ins = next(iter(train_ins))\n",
    "print({k: tuple(v.shape) for k, v in b_ins.items()})\n",
    "# Expect keys:\n",
    "#   coords [B, Nmax, 2], visited_mask [B, Nmax],\n",
    "#   partial_tour [B, Mmax], partial_length [B],\n",
    "#   target_city [B], target_position [B]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Train loop skeletons (no function definitions)\n",
    "\n",
    "### 4.1 Pairs track train loop\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = YourTwoOptModel(...)  # must output [B, L] logits from (coords, tour_full, pair_indices)\n",
    "model.to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for batch in train_loader:\n",
    "        coords = batch[\"coords\"].to(device)\n",
    "        tour_full = batch[\"tour_full\"].to(device)\n",
    "        pair_indices = batch[\"pair_indices\"].to(device)\n",
    "        pair_mask = batch[\"pair_mask\"].to(device)\n",
    "        target = batch[\"target_pair_index\"].to(device)\n",
    "\n",
    "        logits = model(coords, tour_full, pair_indices)   # [B, L]\n",
    "        logits = logits.masked_fill(~pair_mask, -1e9)     # mask invalid classes\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        total += float(loss.item())\n",
    "    print(\"epoch\", epoch+1, \"train_loss\", total / max(1, len(train_loader)))\n",
    "```\n",
    "\n",
    "### 4.2 Pairs track evaluation (inline, no defs)\n",
    "```python\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        coords = batch[\"coords\"].to(device)\n",
    "        tour_full = batch[\"tour_full\"].to(device)\n",
    "        pair_indices = batch[\"pair_indices\"].to(device)\n",
    "        pair_mask = batch[\"pair_mask\"].to(device)\n",
    "        target = batch[\"target_pair_index\"].to(device)\n",
    "\n",
    "        logits = model(coords, tour_full, pair_indices)\n",
    "        logits = logits.masked_fill(~pair_mask, -1e9)\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += target.numel()\n",
    "val_acc = correct / max(1, total)\n",
    "print(\"val_acc\", round(val_acc, 3))\n",
    "```\n",
    "\n",
    "### 4.3 Mixed track insert head (optional)\n",
    "```python\n",
    "# Your insert head must return:\n",
    "#   city_logits [B, Nmax] and pos_logits [B, Mmax]\n",
    "coords = b_ins[\"coords\"].to(device)\n",
    "visited_mask = b_ins[\"visited_mask\"].to(device)\n",
    "partial_tour = b_ins[\"partial_tour\"].to(device)\n",
    "partial_length = b_ins[\"partial_length\"].to(device)\n",
    "t_city = b_ins[\"target_city\"].to(device)\n",
    "t_pos = b_ins[\"target_position\"].to(device)\n",
    "\n",
    "city_logits, pos_logits = your_insert_model(coords, visited_mask, partial_tour, partial_length)\n",
    "city_logits = city_logits.masked_fill(visited_mask.bool(), -1e9)\n",
    "\n",
    "B, Mmax = pos_logits.shape\n",
    "pos_mask = torch.arange(Mmax, device=device).unsqueeze(0) >= partial_length.unsqueeze(1)\n",
    "pos_logits = pos_logits.masked_fill(pos_mask, -1e9)\n",
    "\n",
    "loss_insert = F.cross_entropy(city_logits, t_city) + F.cross_entropy(pos_logits, t_pos)\n",
    "print(\"insert_loss_sample\", float(loss_insert))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Save or reload a ready checkpoint\n",
    "```python\n",
    "save_tsp_checkpoint(\"data/tsp_ready.pt\", insert_examples, two_opt_examples, source_path=\"data/train_pairs.jsonl\")\n",
    "ckpt = load_tsp_checkpoint(\"data/tsp_ready.pt\")\n",
    "print(\"counts\", ckpt[\"meta\"][\"counts\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Make easier or harder data later\n",
    "\n",
    "Easier:\n",
    "```python\n",
    "make_dataset_pairs_only(\"data/easy_pairs.jsonl\", num_problems=800, n_min=6, n_max=10, opt_per=4, noise_min=1, noise_max=3)\n",
    "```\n",
    "\n",
    "Harder:\n",
    "```python\n",
    "make_dataset_pairs_only(\"data/hard_pairs.jsonl\", num_problems=4000, n_min=10, n_max=16, opt_per=10, noise_min=4, noise_max=8, step_sample=\"uniform\")\n",
    "```\n",
    "\n",
    "That is the complete minimal path: generate, load, build loaders, train on pairs, evaluate, checkpoint. Optional insert supervision can be added later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: data/train_pairs_easy.jsonl and data/val_pairs_easy.jsonl\n",
      "Lines read: 1516\n",
      "Mode counts: {'insert': 0, 'two_opt': 1516}\n",
      "n: min=6 max=12 mean=9.09\n",
      "step_index/total_steps: mean=0.308 (0=early, ~0.5=mid, 1=late)\n",
      "mean delta (cost_after - cost_before): -0.2786\n",
      "Lines read: 368\n",
      "Mode counts: {'insert': 0, 'two_opt': 368}\n",
      "n: min=6 max=12 mean=8.94\n",
      "step_index/total_steps: mean=0.300 (0=early, ~0.5=mid, 1=late)\n",
      "mean delta (cost_after - cost_before): -0.2555\n",
      "Train counts -> insert: 0 two_opt: 1516\n",
      "Val   counts -> insert: 0 two_opt: 368\n",
      "Saved checkpoint: data/tsp_ready_easy.pt\n",
      "Reloaded counts: {'train_insert': 0, 'train_two_opt': 1516, 'val_insert': 0, 'val_two_opt': 368}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_path = \"data/train_pairs_easy.jsonl\"\n",
    "val_path   = \"data/val_pairs_easy.jsonl\"\n",
    "\n",
    "make_dataset_pairs_only(\n",
    "    out_path=train_path,\n",
    "    num_problems=1200,\n",
    "    n_min=6, n_max=12,\n",
    "    seed=1234,\n",
    "    opt_per=6,\n",
    "    step_sample=\"mid\",\n",
    "    noise_min=2, noise_max=4,\n",
    "    include_stop_prob=0.2\n",
    ")\n",
    "\n",
    "make_dataset_pairs_only(\n",
    "    out_path=val_path,\n",
    "    num_problems=300,\n",
    "    n_min=6, n_max=12,\n",
    "    seed=5678,\n",
    "    opt_per=6,\n",
    "    step_sample=\"mid\",\n",
    "    noise_min=2, noise_max=4,\n",
    "    include_stop_prob=0.2\n",
    ")\n",
    "\n",
    "print(\"Wrote:\", train_path, \"and\", val_path)\n",
    "\n",
    "# Quick sanity stats (optional but recommended)\n",
    "dataset_stats(train_path)\n",
    "dataset_stats(val_path)\n",
    "\n",
    "# Load into memory lists (ready for building DataLoaders later)\n",
    "insert_train, two_opt_train = load_jsonl_split(train_path)\n",
    "insert_val,   two_opt_val   = load_jsonl_split(val_path)\n",
    "\n",
    "print(\"Train counts -> insert:\", len(insert_train), \"two_opt:\", len(two_opt_train))\n",
    "print(\"Val   counts -> insert:\", len(insert_val),   \"two_opt:\", len(two_opt_val))\n",
    "\n",
    "# Save a ready checkpoint you can reload without reparsing JSONL later\n",
    "ckpt_path = \"data/tsp_ready_easy.pt\"\n",
    "payload = {\n",
    "    \"train\": {\"insert_examples\": insert_train, \"two_opt_examples\": two_opt_train},\n",
    "    \"val\":   {\"insert_examples\": insert_val,   \"two_opt_examples\": two_opt_val},\n",
    "    \"meta\": {\n",
    "        \"source_train\": train_path,\n",
    "        \"source_val\": val_path,\n",
    "        \"counts\": {\n",
    "            \"train_insert\": len(insert_train),\n",
    "            \"train_two_opt\": len(two_opt_train),\n",
    "            \"val_insert\": len(insert_val),\n",
    "            \"val_two_opt\": len(two_opt_val)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(payload, ckpt_path)\n",
    "print(\"Saved checkpoint:\", ckpt_path)\n",
    "\n",
    "# Reload once to confirm (PyTorch 2.6+ needs weights_only=False for general objects)\n",
    "reloaded = torch.load(ckpt_path, weights_only=False)\n",
    "print(\"Reloaded counts:\", reloaded[\"meta\"][\"counts\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train two_opt: 1516 Val two_opt: 368\n",
      "{'coords': (64, 12, 2), 'tour_full': (64, 12), 'pair_indices': (64, 46, 2), 'pair_mask': (64, 46), 'target_pair_index': (64,)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load into memory\n",
    "insert_train, two_opt_train = load_jsonl_split(\"data/train_pairs_easy.jsonl\")\n",
    "insert_val,   two_opt_val   = load_jsonl_split(\"data/val_pairs_easy.jsonl\")\n",
    "\n",
    "print(\"Train two_opt:\", len(two_opt_train), \"Val two_opt:\", len(two_opt_val))\n",
    "\n",
    "# Create DataLoaders (pairs track)\n",
    "train_loader = DataLoader(\n",
    "    TwoOptDataset(two_opt_train),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_two_opt_pairclass,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TwoOptDataset(two_opt_val),\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_two_opt_pairclass,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# Preview one batch to confirm shapes\n",
    "batch = next(iter(train_loader))\n",
    "print({k: tuple(v.shape) for k, v in batch.items()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
